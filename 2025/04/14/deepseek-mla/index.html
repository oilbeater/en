<!DOCTYPE html>
<html lang="en" color-mode="light">

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Oilbeater" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://oilbeater.com/images/oilbeater.png" />
  <meta name="twitter:site" content="Oilbeater&#39;s Study Room" />
  <meta name="twitter:creator" content="Oilbeater">
  <meta name="twitter:title" content="DeepSeek MLA -- The Attention Mechanism Born for Cost Optimization | Oilbeater&#39;s Study Room" />
  <meta name="twitter:description" content="" />

  <meta property="og:site_name" content="Oilbeater&#39;s Study Room">
  <meta property="og:title" content="DeepSeek MLA -- The Attention Mechanism Born for Cost Optimization | Oilbeater&#39;s Study Room" />
  <meta property="og:type" content="article" />
  <meta property="og:locale" content="en" />
  <meta property="og:description" content="" />

  <meta name="google-adsense-account" content="ca-pub-9141929305076734">
  <!-- Open Graph Description 简短摘要-->
  
  <!-- 用于搜索引擎的文章摘要 -->
  
  
  
  <title>
    
      DeepSeek MLA -- The Attention Mechanism Born for Cost Optimization 
      
      
      |
    
     Oilbeater&#39;s Study Room
  </title>

  
    <link rel="apple-touch-icon" href="/images/oilbeater.png">
    <link rel="icon" href="/images/oilbeater.png">
  

  <!-- Raleway-Font -->
  <link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

  <!-- hexo site css -->
  <link rel="stylesheet" href="/css/main.css" />
  <link rel="stylesheet" href="//at.alicdn.com/t/font_1886449_67xjft27j1l.css" />
  <!-- 代码块风格 -->
  

  <!-- jquery3.3.1 -->
  
    <script defer type="text/javascript" src="/plugins/jquery.min.js"></script>
  

  <!-- fancybox -->
  
    <link href="/plugins/jquery.fancybox.min.css" rel="stylesheet">
    <script defer type="text/javascript" src="/plugins/jquery.fancybox.min.js"></script>
  
  
<script src="/en/js/fancybox.js"></script>


  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-31254339-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-31254339-1');
    </script>
  

  
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

  <script>
    var html = document.documentElement
    const colorMode = localStorage.getItem('color-mode')
    if (colorMode) {
      document.documentElement.setAttribute('color-mode', colorMode)
    }
  </script>
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9141929305076734"
  crossorigin="anonymous"></script>
<meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/en/atom.xml" title="Oilbeater's Study Room" type="application/atom+xml">
</head>


  <body>
    <div id="app">
      <div class="header">
  <div class="avatar">
    <a href="/en">
      <!-- 头像取消懒加载，添加no-lazy -->
      
        <img src="/images/oilbeater.png" alt="">
      
    </a>
    <div class="nickname"><a href="/en">Oilbeater</a></div>
  </div>
  <div class="navbar">
    <ul>
      
        <li class="nav-item" data-path="/en/">
          <a href="/en/">Home</a>
        </li>
      
        <li class="nav-item" data-path="/en/archives/">
          <a href="/en/archives/">Archives</a>
        </li>
      
        <li class="nav-item" data-path="/en/tags/">
          <a href="/en/tags/">Tags</a>
        </li>
      
        <li class="nav-item" data-path="/en/about/">
          <a href="/en/about/">About</a>
        </li>
      
    </ul>
  </div>
</div>


<script src="/en/js/activeNav.js"></script>



      <div class="flex-container">
        <!-- 文章详情页，展示文章具体内容，url形式：https://yoursite/文章标题/ -->
<!-- 同时为「标签tag」，「朋友friend」，「分类categories」，「关于about」页面的承载页面，具体展示取决于page.type -->


  <!-- LaTex Display -->

  
    <script async type="text/javascript" src="/plugins/mathjax/tex-chtml.js"></script>
  
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    }
  </script>





  <!-- clipboard -->

  
    <script async type="text/javascript" src="/plugins/clipboard.min.js"></script>
  
  
<script src="/en/js/codeCopy.js"></script>







  

  

  

  
  <!-- 文章内容页 url形式：https://yoursite/文章标题/ -->
  <div class="container post-details" id="post-details">
    <div class="post-content">
      <div class="post-title">DeepSeek MLA -- The Attention Mechanism Born for Cost Optimization</div>
      <div class="post-attach">
        <span class="post-pubtime">
          <i class="iconfont icon-updatetime mr-10" title="Update time"></i>
          2025-12-01 05:35:24
        </span>
        
              <span class="post-tags">
                <i class="iconfont icon-tags mr-10" title="Tags"></i>
                
                <span class="span--tag mr-8">
                  <a href="/en/tags/DeepSeek/" title="DeepSeek">
                    #DeepSeek
                  </a>
                </span>
                
                <span class="span--tag mr-8">
                  <a href="/en/tags/LLM/" title="LLM">
                    #LLM
                  </a>
                </span>
                
                <span class="span--tag mr-8">
                  <a href="/en/tags/Paper/" title="Paper">
                    #Paper
                  </a>
                </span>
                
              </span>
          
      </div>
      <div class="markdown-body">
        <p>DeepSeek first gained fame because DeepSeek V2 achieved a cost of just 0.14 dollar per million tokens. At the same time, GPT-4 cost 30 dollar, and even the highly cost-effective GPT-3.5 was priced at 1.5 dollar. This breakthrough pricing sparked a price war in China, with many major tech companies slashing prices or even offering free models. However, unlike the logic of burning money for subsidies adopted by other companies, DeepSeek achieved an order-of-magnitude cost reduction through a series of technological innovations. This article introduces one of the most critical innovations behind this — MLA (Multi-Head Latent Attention).</p>
<p>The core mathematical trick of MLA isn’t complicated; the paper explains it in just a few sentences, leaving readers amazed at such an elegant solution. However, because it’s tightly coupled with the Transformer architecture, understanding it can be challenging. Here, I’ll simplify the explanation as much as possible so that even those unfamiliar with Transformers can grasp the brilliance of this method.</p>
<p>Of course, some linear algebra basics are required. If you remember that multiplying a 5×4 matrix by a 4×3 matrix results in a 5×3 matrix, you’re good to go.</p>
<p><img src="/en/../images/20250414233740.png"></p>
<h2 id="KVCache"><a href="#KVCache" class="headerlink" title="KVCache"></a>KVCache</h2><p>Where is the bottleneck in large model inference costs? The answer might surprise you — it’s GPU memory. GPUs have a large number of compute units, but inference tasks are linear, generating only one token at a time. To maximize throughput and fully utilize GPU resources, as many generation tasks as possible are run simultaneously. Each task consumes a significant amount of GPU memory during inference, so to run as many tasks as possible, the runtime memory footprint must be minimized. MLA reduces the runtime memory usage of the original attention mechanism to <strong>6.7%</strong>. That’s not a 6.7% reduction — it’s a <strong>93.3%</strong> reduction. To put it metaphorically, this isn’t a waist cut but an ankle cut. Ignoring the model’s own memory footprint, MLA can theoretically accommodate <strong>15 times</strong> more generation tasks under the same memory constraints.</p>
<p>Although the MLA paper doesn’t elaborate on the inspiration behind this, I believe they reverse-engineered it from the original KVCache, leading to a completely new attention mechanism. This brings us to the question: What is KVCache?</p>
<p>For each token generated, a large model needs to compute all previous tokens to determine the next one. However, tasks typically don’t end after generating one token; they continue until an end token is produced. This means the earlier tokens must be recomputed every time. KVCache addresses this by storing the intermediate results of each token’s computation, avoiding redundant calculations. Imagine each token being mapped to a 1000×1000 matrix. Is there a way to reduce the memory footprint of this matrix?</p>
<p><img src="/en/../images/20250414234534.png"></p>
<h2 id="MLA"><a href="#MLA" class="headerlink" title="MLA"></a>MLA</h2><p>Here’s where things get interesting. We can approximate a large matrix by multiplying two smaller matrices. Remember your linear algebra: a 1000×2 matrix multiplied by a 2×1000 matrix also yields a 1000×1000 matrix, but the two smaller matrices contain only 4000 elements — just <strong>0.4%</strong> of the original matrix’s size.</p>
<p>This is the core mathematical idea behind MLA. In DeepSeek V2, a token is originally mapped to a 1×16k vector. With MLA, it’s first compressed into a 1×512 vector via a compression matrix, then later decompressed into a 1×16k vector using a 512×16k decompression matrix. Here, both the compression and decompression matrices are learned during training and are fixed parts of the model, occupying constant memory. At runtime, each token’s memory footprint is reduced to just the 1×512 vector — only <strong>3%</strong> of the original size.</p>
<p>A full comparison is shown below. The original MHA needs to cache the full matrix, while MLA only caches the compressed vector and reconstructs the full matrix when needed.</p>
<p><img src="/en/../images/20250415000024.png"><br><img src="/en/../images/20250414235538.png"></p>
<p>Is it really this perfect? Let’s revisit the original purpose of KVCache: to avoid redundant intermediate computations for tokens. While MLA compresses KVCache, it still requires a decompression step, bringing the computational cost back.</p>
<p>Here’s where the story gets even more fascinating. In Transformer computations, the cached intermediate result is multiplied by a decompression matrix and then an output matrix to produce the final result. Roughly speaking, the computation can be expressed as: Cache × W<sup>decompress</sup> × W<sup>output</sup></p>
<p>Thanks to the associative property of matrix multiplication, we can first multiply the latter two matrices and fuse them into a single new matrix. Since W<sup>decompress</sup> and W<sup>output</sup> are fixed after training, this fusion can be precomputed with simple post-processing. The authors even describe this in the paper with the word <strong>“Fortunately”</strong>.</p>
<p>In other words, we initially compressed KVCache to save memory, but in actual inference, no decompression happens. Not only is memory usage drastically reduced, but the smaller matrices also decrease computational requirements.</p>
<h2 id="Model-Capability"><a href="#Model-Capability" class="headerlink" title="Model Capability"></a>Model Capability</h2><p>However, one question remains unanswered: MLA essentially approximates a large matrix with two smaller ones, but not all large matrices can be perfectly decomposed this way. The actual search space of MLA is smaller than MHA’s, so theoretically, MLA’s model capability should be weaker. Yet, according to DeepSeek’s paper, MLA slightly outperforms MHA in evaluations.</p>
<p><img src="/en/../images/20250415003909.png"></p>
<p>This is harder to explain. I suspect that while MLA’s search space is reduced, the probability of finding a better solution increases, allowing it to converge to a more optimal point than MHA. Additionally, although MLA’s optimization starts from MHA, the final result is a completely new attention mechanism, altering the model’s architecture. Perhaps DeepSeek has indeed discovered a more efficient attention mechanism.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Many performance optimizations are zero-sum games — trading GPU compute time for memory, or sacrificing model capability for cost reduction. MLA, however, achieves a drastic reduction in memory usage while also lowering computational demands and improving model performance. It’s almost unbelievable.</p>
<p>Another takeaway is that, having lived through China’s mobile internet era, we often assume price wars mean losing money for market share. But we forget that technological innovation should be the greatest lever of all.</p>
<blockquote>
<p>This blog only covers the core idea of MLA. In practice, there are many implementation details, such as: How is rotary positional encoding handled? The fusion of K and V decompression matrices differs slightly — one applies the associative property directly, while the other requires transposition first. I highly recommend reading DeepSeek V2’s original paper. With this article as a foundation, it should be much easier to understand.</p>
<p>Some images in this blog are sourced from <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1BYXRYWEMj/">DeepSeek-v2 MLA Explanation Video</a>. I also recommend watching this video.</p>
</blockquote>

      </div>
      
        <div class="prev-or-next">
          <div class="post-foot-next">
            
              <a href="/en/2025/04/06/chaos-llama4/" target="_self">
                <i class="iconfont icon-chevronleft"></i>
                <span>Prev</span>
              </a>
            
          </div>
          <div class="post-attach">
            <span class="post-pubtime">
              <i class="iconfont icon-updatetime mr-10" title="Update time"></i>
              2025-12-01 05:35:24
            </span>
            
                  <span class="post-tags">
                    <i class="iconfont icon-tags mr-10" title="Tags"></i>
                    
                    <span class="span--tag mr-8">
                      <a href="/en/tags/DeepSeek/" title="DeepSeek">
                        #DeepSeek
                      </a>
                    </span>
                    
                    <span class="span--tag mr-8">
                      <a href="/en/tags/LLM/" title="LLM">
                        #LLM
                      </a>
                    </span>
                    
                    <span class="span--tag mr-8">
                      <a href="/en/tags/Paper/" title="Paper">
                        #Paper
                      </a>
                    </span>
                    
                  </span>
              
          </div>
          <div class="post-foot-prev">
            
              <a href="/en/2025/04/23/supaas-openshift/" target="_self">
                <span>Next</span>
                <i class="iconfont icon-chevronright"></i>
              </a>
            
          </div>
        </div>
      
    </div>
    
  <div id="btn-catalog" class="btn-catalog">
    <i class="iconfont icon-catalog"></i>
  </div>
  <div class="post-catalog hidden" id="catalog">
    <div class="title">Contents</div>
    <div class="catalog-content">
      
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#KVCache"><span class="toc-text">KVCache</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MLA"><span class="toc-text">MLA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Capability"><span class="toc-text">Model Capability</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-text">Conclusion</span></a></li></ol>
      
    </div>
  </div>

  
<script src="/en/js/catalog.js"></script>




    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-9141929305076734"
     crossorigin="anonymous"></script>
<!-- 横向广告 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-9141929305076734"
     data-ad-slot="3076777751"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
    
      <div class="comments-container">
        






  <div id="gitalk-container"></div>

  <script>
    function loadGitalkSuc() {
      const gitalk = new Gitalk({
        clientID: 'fa956f38daf8cc1f8222',
        clientSecret: '8e37dcda7dc6d86518fc00b06f8d7008ef8542c6',
        repo: 'oilbeater.github.com',
        owner: 'oilbeater',
        admin: ['oilbeater'],
        id: location.pathname,
        distractionFreeMode: false
      })

      gitalk.render('gitalk-container')
    }
  </script>
  
    <link rel="stylesheet" href="/plugins/gitalk.css">
    <script type="text/javascript" src="/plugins/gitalk.min.js" onload="loadGitalkSuc(this)"></script>
  



      </div>
    
  </div>


        
<div class="footer">
  <div class="social">
    <ul>
      
        <li>
          
              <a title="github" target="_blank" rel="noopener" href="https://github.com/oilbeater">
                <i class="iconfont icon-github"></i>
              </a>
              
        </li>
        
        <li>
          
            <a title="email" href="mailto:mailto:liumengxinfly@gmail.com">
              <i class="iconfont icon-envelope"></i>
            </a>
            
        </li>
        
        <li>
          
              <a title="twitter" target="_blank" rel="noopener" href="https://twitter.com/liumengxinfly">
                <i class="iconfont icon-twitter"></i>
              </a>
              
        </li>
        
        <li>
          
              <a title="linkedin" target="_blank" rel="noopener" href="https://www.linkedin.com/in/oilbeater/">
                <i class="iconfont icon-linkedin"></i>
              </a>
              
        </li>
        
        <li>
          
              <a title="rss" href="/en/atom.xml">
                <i class="iconfont icon-rss"></i>
              </a>
              
        </li>
        
    </ul>
  </div>
  
    
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/oilbeater/oilbeater.github.com">Copyright © 2025 Oilbeater</a>
        
    </div>
  
    
    <div class="footer-more">
      
        <a target="_blank" rel="noopener" href="https://github.com/zchengsite/hexo-theme-oranges">Theme by Oranges | Powered by Hexo</a>
        
    </div>
  
  
    <div class="footer-views">
      
          sv：<span id="busuanzi_value_site_pv"></span>
        
      
          pv：<span id="busuanzi_value_page_pv"></span>
        
      
          uv：<span id="busuanzi_value_site_uv"></span>
        
      
    </div>
  
</div>

      </div>
      <div style="position: fixed;right: 2.2rem;top: 2.2rem;font-weight: bold;font-size: 1.6rem;">
        <a href="https://oilbeater.com/">中</a>
      </div>
      <div class="tools-bar">
        <div class="back-to-top tools-bar-item hidden">
  <a href="javascript: void(0)">
    <i class="iconfont icon-chevronup"></i>
  </a>
</div>


<script src="/en/js/backtotop.js"></script>



        
  <div class="search-icon tools-bar-item" id="search-icon">
    <a href="javascript: void(0)">
      <i class="iconfont icon-search"></i>
    </a>
  </div>

  <div class="search-overlay hidden">
    <div class="search-content" tabindex="0">
      <div class="search-title">
        <span class="search-icon-input">
          <a href="javascript: void(0)">
            <i class="iconfont icon-search"></i>
          </a>
        </span>
        
          <input type="text" class="search-input" id="search-input" placeholder="Search...">
        
        <span class="search-close-icon" id="search-close-icon">
          <a href="javascript: void(0)">
            <i class="iconfont icon-close"></i>
          </a>
        </span>
      </div>
      <div class="search-result" id="search-result"></div>
    </div>
  </div>

  <script type="text/javascript">
    var inputArea = document.querySelector("#search-input")
    var searchOverlayArea = document.querySelector(".search-overlay")

    inputArea.onclick = function() {
      getSearchFile()
      this.onclick = null
    }

    inputArea.onkeydown = function() {
      if(event.keyCode == 13)
        return false
    }

    function openOrHideSearchContent() {
      let isHidden = searchOverlayArea.classList.contains('hidden')
      if (isHidden) {
        searchOverlayArea.classList.remove('hidden')
        document.body.classList.add('hidden')
        // inputArea.focus()
      } else {
        searchOverlayArea.classList.add('hidden')
        document.body.classList.remove('hidden')
      }
    }

    function blurSearchContent(e) {
      if (e.target === searchOverlayArea) {
        openOrHideSearchContent()
      }
    }

    document.querySelector("#search-icon").addEventListener("click", openOrHideSearchContent, false)
    document.querySelector("#search-close-icon").addEventListener("click", openOrHideSearchContent, false)
    searchOverlayArea.addEventListener("click", blurSearchContent, false)

    var searchFunc = function (path, search_id, content_id) {
      'use strict';
      var $input = document.getElementById(search_id);
      var $resultContent = document.getElementById(content_id);
      $resultContent.innerHTML = "<ul><span class='local-search-empty'>First search, index file loading, please wait...<span></ul>";
      $.ajax({
        // 0x01. load xml file
        url: path,
        dataType: "xml",
        success: function (xmlResponse) {
          // 0x02. parse xml file
          var datas = $("entry", xmlResponse).map(function () {
            return {
              title: $("title", this).text(),
              content: $("content", this).text(),
              url: $("url", this).text()
            };
          }).get();
          $resultContent.innerHTML = "";

          $input.addEventListener('input', function () {
            // 0x03. parse query to keywords list
            var str = '<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length <= 0) {
              return;
            }
            // 0x04. perform local searching
            datas.forEach(function (data) {
              var isMatch = true;
              var content_index = [];
              if (!data.title || data.title.trim() === '') {
                data.title = "Untitled";
              }
              var orig_data_title = data.title.trim();
              var data_title = orig_data_title.toLowerCase();
              var orig_data_content = data.content.trim().replace(/<[^>]+>/g, "");
              var data_content = orig_data_content.toLowerCase();
              var data_url = data.url;
              var index_title = -1;
              var index_content = -1;
              var first_occur = -1;
              // only match artiles with not empty contents
              if (data_content !== '') {
                keywords.forEach(function (keyword, i) {
                  index_title = data_title.indexOf(keyword);
                  index_content = data_content.indexOf(keyword);

                  if (index_title < 0 && index_content < 0) {
                    isMatch = false;
                  } else {
                    if (index_content < 0) {
                      index_content = 0;
                    }
                    if (i == 0) {
                      first_occur = index_content;
                    }
                    // content_index.push({index_content:index_content, keyword_len:keyword_len});
                  }
                });
              } else {
                isMatch = false;
              }
              // 0x05. show search results
              if (isMatch) {
                str += "<li><a href='" + data_url + "' class='search-result-title'>" + orig_data_title + "</a>";
                var content = orig_data_content;
                if (first_occur >= 0) {
                  // cut out 100 characters
                  var start = first_occur - 20;
                  var end = first_occur + 80;

                  if (start < 0) {
                    start = 0;
                  }

                  if (start == 0) {
                    end = 100;
                  }

                  if (end > content.length) {
                    end = content.length;
                  }

                  var match_content = content.substr(start, end);

                  // highlight all keywords
                  keywords.forEach(function (keyword) {
                    var regS = new RegExp(keyword, "gi");
                    match_content = match_content.replace(regS, "<span class=\"search-keyword\">" + keyword + "</span>");
                  });

                  str += "<p class=\"search-result-abstract\">" + match_content + "...</p>"
                }
                str += "</li>";
              }
            });
            str += "</ul>";
            if (str.indexOf('<li>') === -1) {
              return $resultContent.innerHTML = "<ul><span class='local-search-empty'>No result<span></ul>";
            }
            $resultContent.innerHTML = str;
          });
        },
        error: function(xhr, status, error) {
          $resultContent.innerHTML = ""
          if (xhr.status === 404) {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>The search.xml file was not found, please refer to：<a href='https://github.com/zchengsite/hexo-theme-oranges#configuration' target='_black'>configuration</a><span></ul>";
          } else {
            $resultContent.innerHTML = "<ul><span class='local-search-empty'>The request failed, Try to refresh the page or try again later.<span></ul>";
          }
        }
      });
      $(document).on('click', '#search-close-icon', function() {
        $('#search-input').val('');
        $('#search-result').html('');
      });
    }

    var getSearchFile = function() {
        var path = "/search.xml";
        searchFunc(path, 'search-input', 'search-result');
    }
  </script>




        
  <div class="tools-bar-item theme-icon" id="switch-color-scheme">
    <a href="javascript: void(0)">
      <i id="theme-icon" class="iconfont icon-moon"></i>
    </a>
  </div>

  
<script src="/en/js/colorscheme.js"></script>





        
  
    <div class="share-icon tools-bar-item">
      <a href="javascript: void(0)" id="share-icon">
        <i class="iconfont iconshare"></i>
      </a>
      <div class="share-content hidden">
        
          <a class="share-item" href="https://twitter.com/intent/tweet?text=' + DeepSeek%20MLA%20--%20The%20Attention%20Mechanism%20Born%20for%20Cost%20Optimization + '&url=' + http%3A%2F%2Foilbeater.com%2Fen%2F2025%2F04%2F14%2Fdeepseek-mla%2F + '" target="_blank" title="Twitter">
            <i class="iconfont icon-twitter"></i>
          </a>
        
        
          <a class="share-item" href="https://www.facebook.com/sharer.php?u=http://oilbeater.com/en/2025/04/14/deepseek-mla/" target="_blank" title="Facebook">
            <i class="iconfont icon-facebooksquare"></i>
          </a>
        
      </div>
    </div>
  
  
<script src="/en/js/shares.js"></script>



      </div>
    </div>
  </body>
</html>
