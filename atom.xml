<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Oilbeater&#39;s Study Room</title>
  <icon>http://oilbeater.com/en/icon.png</icon>
  
  <link href="http://oilbeater.com/en/atom.xml" rel="self"/>
  
  <link href="http://oilbeater.com/en/"/>
  <updated>2025-04-23T11:05:07.138Z</updated>
  <id>http://oilbeater.com/en/</id>
  
  <author>
    <name>Oilbeater</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>How to Surpass OpenShift</title>
    <link href="http://oilbeater.com/en/2025/04/23/supaas-openshift/"/>
    <id>http://oilbeater.com/en/2025/04/23/supaas-openshift/</id>
    <published>2025-04-23T10:40:43.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>OpenShift, as a benchmark product in the container platform space and a model of open-source commercialization, has always been a target for others to chase or surpass. However, if you merely imitate OpenShift’s product, the only scenario where you might catch up is if OpenShift stops developing. After a year, you might finally catch up, only to be phased out for the same reasons.</p><p>So, is there a way to catch up with or even surpass OpenShift? I believe the key lies in examining OpenShift’s business model choices and technical roadmap, identifying the inevitable shortcomings that arise from these choices, and then differentiating yourself. Only then is surpassing OpenShift possible.</p><h2 id="OpenShift-Is-Not-Open"><a href="#OpenShift-Is-Not-Open" class="headerlink" title="OpenShift Is Not Open"></a>OpenShift Is Not Open</h2><p>The “Open” in OpenShift might be the same as the “Open” in OpenAI. If you’ve ever tried to deploy OpenShift without going through RedHat’s sales team, you’ll know exactly what I mean.</p><p>All of OpenShift’s components are indeed open-source, but if you’re purely a community user, you’ll face obstacles at every step. You might struggle to find the corresponding component for a feature, locate its source code, or even find documentation on how to compile and use it. These issues are commonplace for components exclusively used by OpenShift. It seems OpenShift’s “community” is only open to customers and partners.</p><p>For non-OpenShift-specific components, OpenShift’s strategy is to heavily invest once a choice is made, aiming to gain dominance in the corresponding component’s community. As a result, you’ll notice that OpenShift contributors are entirely absent from some well-known community projects, while others that have been steadily developing for years suddenly see an influx of OpenShift contributors.</p><p>These are the trade-offs OpenShift has made between commercialization and open-source—there’s no right or wrong here. However, this leaves room for more open projects. If a new product can lower the barrier to entry, gather broader feedback, and enable more contributors to participate in innovation, I believe its potential will surpass OpenShift’s.</p><h2 id="OpenShift’s-Technology-Isn’t-Cutting-Edge"><a href="#OpenShift’s-Technology-Isn’t-Cutting-Edge" class="headerlink" title="OpenShift’s Technology Isn’t Cutting-Edge"></a>OpenShift’s Technology Isn’t Cutting-Edge</h2><p>Due to the first point, OpenShift cannot widely adopt the latest advancements from the broader ecosystem. When a component in the ecosystem overlaps with OpenShift’s proprietary components, OpenShift’s internal developers have little incentive to switch to another community or an open-source component led by another company.</p><p>Take networking, for example—an area I’m familiar with. Early on, OpenShift implemented Route using HAProxy to enable external traffic to access services inside the cluster. At the time, when Ingress was still immature, Route was a far more advanced solution than what the community offered, and OpenShift’s approach was undoubtedly leading. However, as Ingress matured and various gateways in the ecosystem rapidly evolved, OpenShift, constrained by its early implementation and user practices, took a long time to support Ingress, which had already become a standardized feature in the community. Now, the Ingress specification has progressed to GatewayAPI, and many new AI Gateway scenarios are extending through GatewayAPI. Yet, OpenShift still doesn’t support GatewayAPI and is only recently planning to add support for Route, Ingress, and GatewayAPI on its existing HAProxy.</p><p>There are many similar cases within OpenShift. Early solutions might have been excellent, but because they were OpenShift-specific and closed, they eventually became obstacles to progress as the community advanced. Today, no one building an Ingress Gateway on Kubernetes would reference OpenShift’s implementation. In many niche areas, OpenShift is no longer the most advanced solution. To me, OpenShift now feels like a broad but mediocre and uninspiring platform.</p><p>The production of a container platform is similar to manufacturing a smartphone—it involves selecting components from hundreds of suppliers and assembling them into a final product. If you can remain open, choose the most cutting-edge components from the supply chain, or quickly assemble a product tailored to specific scenarios, your technical competitiveness should far surpass OpenShift’s.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>To truly catch up with or surpass OpenShift, the key isn’t to follow its existing features step by step but to be more open and more advanced. Only then can you break free from the follower’s path, establish a differentiated advantage over OpenShift, and become a leader in the next wave of technology.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;OpenShift, as a benchmark product in the container platform space and a model of open-source commercialization, has always been a target </summary>
      
    
    
    
    
    <category term="OpenShift" scheme="http://oilbeater.com/en/tags/OpenShift/"/>
    
    <category term="Product" scheme="http://oilbeater.com/en/tags/Product/"/>
    
  </entry>
  
  <entry>
    <title>DeepSeek MLA -- The Attention Mechanism Born for Cost Optimization</title>
    <link href="http://oilbeater.com/en/2025/04/14/deepseek-mla/"/>
    <id>http://oilbeater.com/en/2025/04/14/deepseek-mla/</id>
    <published>2025-04-14T17:10:03.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>DeepSeek first gained fame because DeepSeek V2 achieved a cost of just 0.14 dollar per million tokens. At the same time, GPT-4 cost 30 dollar, and even the highly cost-effective GPT-3.5 was priced at 1.5 dollar. This breakthrough pricing sparked a price war in China, with many major tech companies slashing prices or even offering free models. However, unlike the logic of burning money for subsidies adopted by other companies, DeepSeek achieved an order-of-magnitude cost reduction through a series of technological innovations. This article introduces one of the most critical innovations behind this — MLA (Multi-Head Latent Attention).</p><p>The core mathematical trick of MLA isn’t complicated; the paper explains it in just a few sentences, leaving readers amazed at such an elegant solution. However, because it’s tightly coupled with the Transformer architecture, understanding it can be challenging. Here, I’ll simplify the explanation as much as possible so that even those unfamiliar with Transformers can grasp the brilliance of this method.</p><p>Of course, some linear algebra basics are required. If you remember that multiplying a 5×4 matrix by a 4×3 matrix results in a 5×3 matrix, you’re good to go.</p><p><img src="/en/../images/20250414233740.png"></p><h2 id="KVCache"><a href="#KVCache" class="headerlink" title="KVCache"></a>KVCache</h2><p>Where is the bottleneck in large model inference costs? The answer might surprise you — it’s GPU memory. GPUs have a large number of compute units, but inference tasks are linear, generating only one token at a time. To maximize throughput and fully utilize GPU resources, as many generation tasks as possible are run simultaneously. Each task consumes a significant amount of GPU memory during inference, so to run as many tasks as possible, the runtime memory footprint must be minimized. MLA reduces the runtime memory usage of the original attention mechanism to <strong>6.7%</strong>. That’s not a 6.7% reduction — it’s a <strong>93.3%</strong> reduction. To put it metaphorically, this isn’t a waist cut but an ankle cut. Ignoring the model’s own memory footprint, MLA can theoretically accommodate <strong>15 times</strong> more generation tasks under the same memory constraints.</p><p>Although the MLA paper doesn’t elaborate on the inspiration behind this, I believe they reverse-engineered it from the original KVCache, leading to a completely new attention mechanism. This brings us to the question: What is KVCache?</p><p>For each token generated, a large model needs to compute all previous tokens to determine the next one. However, tasks typically don’t end after generating one token; they continue until an end token is produced. This means the earlier tokens must be recomputed every time. KVCache addresses this by storing the intermediate results of each token’s computation, avoiding redundant calculations. Imagine each token being mapped to a 1000×1000 matrix. Is there a way to reduce the memory footprint of this matrix?</p><p><img src="/en/../images/20250414234534.png"></p><h2 id="MLA"><a href="#MLA" class="headerlink" title="MLA"></a>MLA</h2><p>Here’s where things get interesting. We can approximate a large matrix by multiplying two smaller matrices. Remember your linear algebra: a 1000×2 matrix multiplied by a 2×1000 matrix also yields a 1000×1000 matrix, but the two smaller matrices contain only 4000 elements — just <strong>0.4%</strong> of the original matrix’s size.</p><p>This is the core mathematical idea behind MLA. In DeepSeek V2, a token is originally mapped to a 1×16k vector. With MLA, it’s first compressed into a 1×512 vector via a compression matrix, then later decompressed into a 1×16k vector using a 512×16k decompression matrix. Here, both the compression and decompression matrices are learned during training and are fixed parts of the model, occupying constant memory. At runtime, each token’s memory footprint is reduced to just the 1×512 vector — only <strong>3%</strong> of the original size.</p><p>A full comparison is shown below. The original MHA needs to cache the full matrix, while MLA only caches the compressed vector and reconstructs the full matrix when needed.</p><p><img src="/en/../images/20250415000024.png"><br><img src="/en/../images/20250414235538.png"></p><p>Is it really this perfect? Let’s revisit the original purpose of KVCache: to avoid redundant intermediate computations for tokens. While MLA compresses KVCache, it still requires a decompression step, bringing the computational cost back.</p><p>Here’s where the story gets even more fascinating. In Transformer computations, the cached intermediate result is multiplied by a decompression matrix and then an output matrix to produce the final result. Roughly speaking, the computation can be expressed as: Cache × W<sup>decompress</sup> × W<sup>output</sup></p><p>Thanks to the associative property of matrix multiplication, we can first multiply the latter two matrices and fuse them into a single new matrix. Since W<sup>decompress</sup> and W<sup>output</sup> are fixed after training, this fusion can be precomputed with simple post-processing. The authors even describe this in the paper with the word <strong>“Fortunately”</strong>.</p><p>In other words, we initially compressed KVCache to save memory, but in actual inference, no decompression happens. Not only is memory usage drastically reduced, but the smaller matrices also decrease computational requirements.</p><h2 id="Model-Capability"><a href="#Model-Capability" class="headerlink" title="Model Capability"></a>Model Capability</h2><p>However, one question remains unanswered: MLA essentially approximates a large matrix with two smaller ones, but not all large matrices can be perfectly decomposed this way. The actual search space of MLA is smaller than MHA’s, so theoretically, MLA’s model capability should be weaker. Yet, according to DeepSeek’s paper, MLA slightly outperforms MHA in evaluations.</p><p><img src="/en/../images/20250415003909.png"></p><p>This is harder to explain. I suspect that while MLA’s search space is reduced, the probability of finding a better solution increases, allowing it to converge to a more optimal point than MHA. Additionally, although MLA’s optimization starts from MHA, the final result is a completely new attention mechanism, altering the model’s architecture. Perhaps DeepSeek has indeed discovered a more efficient attention mechanism.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Many performance optimizations are zero-sum games — trading GPU compute time for memory, or sacrificing model capability for cost reduction. MLA, however, achieves a drastic reduction in memory usage while also lowering computational demands and improving model performance. It’s almost unbelievable.</p><p>Another takeaway is that, having lived through China’s mobile internet era, we often assume price wars mean losing money for market share. But we forget that technological innovation should be the greatest lever of all.</p><blockquote><p>This blog only covers the core idea of MLA. In practice, there are many implementation details, such as: How is rotary positional encoding handled? The fusion of K and V decompression matrices differs slightly — one applies the associative property directly, while the other requires transposition first. I highly recommend reading DeepSeek V2’s original paper. With this article as a foundation, it should be much easier to understand.</p><p>Some images in this blog are sourced from <a href="https://www.bilibili.com/video/BV1BYXRYWEMj/">DeepSeek-v2 MLA Explanation Video</a>. I also recommend watching this video.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;DeepSeek first gained fame because DeepSeek V2 achieved a cost of just 0.14 dollar per million tokens. At the same time, GPT-4 cost 30 do</summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/en/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/en/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/en/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Chaos in Llama 4</title>
    <link href="http://oilbeater.com/en/2025/04/06/chaos-llama4/"/>
    <id>http://oilbeater.com/en/2025/04/06/chaos-llama4/</id>
    <published>2025-04-06T15:57:23.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>A while ago, the departure of Meta AI’s head raised suspicions about issues with the progress of Llama 4. However, just two days later, Meta released Llama 4, seemingly to dispel rumors. Yet, after reviewing the basic information of the models that have been published, I feel that the Llama project is now in extreme disarray. Below is my analysis based on the available information, and I welcome any corrections.</p><h2 id="Model-Basic-Information"><a href="#Model-Basic-Information" class="headerlink" title="Model Basic Information"></a>Model Basic Information</h2><p>Llama has officially released a 109B and a 400B parameter model this time, along with information about an unreleased 2T parameter model. I have summarized the key architectural details below. All information is sourced from Llama’s blog and the huggingface model page:</p><table><thead><tr><th>Name</th><th>Parameters</th><th>Activated Parameters</th><th>Experts</th><th>Context</th><th>Corpus Size</th><th>GPU Time</th></tr></thead><tbody><tr><td>Scout</td><td>109B</td><td>17B</td><td>16</td><td>10M</td><td>40T</td><td>5.0M</td></tr><tr><td>Maverick</td><td>400B</td><td>17B</td><td>128+1</td><td>1M</td><td>22T</td><td>2.38M</td></tr><tr><td>Behemoth</td><td>2T</td><td>288B</td><td>16</td><td>-</td><td>-</td><td>-</td></tr></tbody></table><p>There’s no need to look at the model’s scoring performance anymore. The comparison of these models’ architectures reveals many issues.</p><h2 id="Strange-MoE-Architecture"><a href="#Strange-MoE-Architecture" class="headerlink" title="Strange MoE Architecture"></a>Strange MoE Architecture</h2><p>Llama 4 has fully transitioned from Dense models to MoE this time, but the bizarre part is that they have adopted two different MoE architectures for the three models. The largest Behemoth and the smallest Scout use the traditional MoE, with 16 experts—a number traditionally considered conventional. The middle Maverick, however, uses a new fine-grained expert plus shared expert model proposed by DeepSeek MoE, with a 128 experts plus 1 shared expert architecture.<br>Generally, models in the same generation use the same architecture, with adjustments made to the number of layers and the width of each layer. Having two models with significant differences in the same generation is odd. Moreover, even if there are changes, it shouldn’t be the largest and smallest models that remain consistent, while the middle-sized one is switched. It feels like the middle Maverick was hastily retrained under the impact of DeepSeek, but there wasn’t enough time to redo all three, so they were just released together.</p><h2 id="Strange-Cost-Investment"><a href="#Strange-Cost-Investment" class="headerlink" title="Strange Cost Investment"></a>Strange Cost Investment</h2><p>Typically, the larger the model’s parameter scale, the higher the cost required. On one hand, larger models can accommodate more knowledge and require more corpus for larger models; on the other hand, with the same corpus, larger models require more parameters to be trained, leading to higher GPU costs. Therefore, usually, the larger the model scale, the higher the required cost.<br>However, in Llama 4, both indicators are reversed. Maverick’s parameter scale is nearly four times that of Scout, but Maverick’s training corpus size is only half of Scout’s, and the consumed GPU time is also only half. Considering that the activated parameters of these two models are the same, the GPU time can be understood, but the corpus size being only half is very strange. It feels like either this is a trial of the new MoE architecture without intending to do a full training, or the training broke down later, and they had to release from an intermediate snapshot.</p><h2 id="Strange-Context-Length"><a href="#Strange-Context-Length" class="headerlink" title="Strange Context Length"></a>Strange Context Length</h2><p>Generally, larger models have stronger capabilities. However, in this generation of Llama, the most shocking 10M context is given to the smallest Scout, while the larger Maverick has only 1M context. Considering the mainstream method for expanding context is still post-training fine-tuning, the larger Maverick’s post-training investment is less than the smaller Scout.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>It feels like Llama 4 was initially intended to follow the traditional MoE path but was influenced by DeepSeek and halfway started to look at DeepSeek MoE. However, training might have already begun, and stopping it would have been difficult, so a middle-sized Maverick was inserted. Judging from the parameter selection, the goal seems to be to achieve similar performance to DeepSeek V3 with a smaller parameter count. However, with 17B activated parameters, it seems challenging to match DeepSeek V3’s 39B activated parameters. Nevertheless, the fact that this generation’s models were released in such a chaotic form, along with a futures model, suggests that there are significant issues within the Llama project.</p><p><img src="/en/../images/llama4.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;A while ago, the departure of Meta AI’s head raised suspicions about issues with the progress of Llama 4. However, just two days later, M</summary>
      
    
    
    
    
    <category term="LLM" scheme="http://oilbeater.com/en/tags/LLM/"/>
    
    <category term="Llama" scheme="http://oilbeater.com/en/tags/Llama/"/>
    
  </entry>
  
  <entry>
    <title>DeepSeek MoE -- An Innovative MoE Architecture</title>
    <link href="http://oilbeater.com/en/2025/03/29/deepseek-moe/"/>
    <id>http://oilbeater.com/en/2025/03/29/deepseek-moe/</id>
    <published>2025-03-29T12:54:37.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>Those who have been following DeepSeek’s work since DeepSeek V3&#x2F;R1 might easily think that a significant portion of DeepSeek’s efforts are focused on engineering optimizations for efficiency. However, reviewing DeepSeek’s papers over the past year reveals that they have actually been continuously innovating in model architecture and training methods, with V3 and R1 being scaled versions based on previous architectural innovations. The DeepSeek MoE paper introduces the main innovations in MoE architecture by DeepSeek, which currently appears promising and could potentially become the standard for future MoE architectures.</p><h2 id="MoE-vs-Dense"><a href="#MoE-vs-Dense" class="headerlink" title="MoE vs Dense"></a>MoE vs Dense</h2><p>First, let’s discuss the differences between MoE and traditional Dense architectures. Early LLMs were mostly based on Dense architectures, where every token generation requires activating all neurons for computation. This approach is quite different from how the human brain operates, as the brain doesn’t need to engage all brain cells for every problem. If it did, humans would be exhausted. Therefore, a natural idea is to no longer activate all neurons when generating tokens, but instead activate only those most relevant to the current task. This led to the MoE (Mixture of Experts) architecture, which divides neurons in each layer of the LLM into N Experts, with a Router selecting the K most relevant Experts to activate.</p><p><img src="/en/../images/20250329161016.png"></p><p>The benefit of this architecture is that during inference, not all neurons need to be activated, significantly reducing computational effort. In the most common 8-choose-2 mode before DeepSeek MoE, the computational load can be reduced to nearly one-third of a Dense model.</p><p>While the MoE architecture seems ideal, it essentially uses a few Experts to mimic the performance of a Dense model. The key lies in whether each Expert is specialized enough to truly mimic the Dense model’s performance. If we compare it to the human brain, when neurons are sufficiently specialized, specific tasks can be completed by activating only a few neurons.</p><p>The DeepSeek MoE paper introduces two innovations they made to push the specialization of each Expert to the extreme:</p><ul><li>More and smaller Experts</li><li>Knowledge-sharing Experts</li></ul><h2 id="More-and-Smaller-Experts"><a href="#More-and-Smaller-Experts" class="headerlink" title="More and Smaller Experts"></a>More and Smaller Experts</h2><p><img src="/en/../images/20250329165838.png"></p><p>Using more and smaller Experts to increase the specialization of each Expert seems to be a straightforward approach. However, previous mainstream MoE architectures typically used 8 or 16 Experts. Given the myriad types of problems LLMs need to handle, this number of Experts clearly cannot achieve high specialization, resulting in each Expert having a lot of irrelevant knowledge for the current task.</p><p>However, as the number of Experts increases, training difficulty also rises, and the Router can easily end up selecting only a few Experts, leading to extreme load imbalance. Ultimately, the theoretical MoE architecture might end up activating the same small group of Experts each time, effectively turning into a small model. Therefore, most previous MoE architectures did not have a large number of Experts.</p><p>DeepSeek designed a set of loss functions that penalize repeatedly selecting the same Expert, thereby forcing the Router to more evenly distribute the selection of Experts. This approach solved the training problem, allowing DeepSeek to gradually scale the number of Experts. From 64 choose 6 in this paper, to 128 choose 12, to 160 choose 6 in V2, and finally to 256 choose 8 in V3.</p><p>It’s evident that DeepSeek has progressively expanded the number of Experts, and the proportion of Experts needed to be selected has also decreased from 9% to 2%, proving that with sufficiently specialized Experts, fewer activations are needed to complete corresponding tasks.</p><h2 id="Knowledge-sharing-Experts"><a href="#Knowledge-sharing-Experts" class="headerlink" title="Knowledge-sharing Experts"></a>Knowledge-sharing Experts</h2><p><img src="/en/../images/20250329170955.png"></p><p>As Experts become smaller and their number increases, another issue arises: each Expert needs not only specific domain knowledge but also some general knowledge, such as general language understanding and logical analysis, which might be required by every Expert. If each Expert memorizes this knowledge, it leads to significant knowledge redundancy, which becomes more pronounced as the number of Experts increases. This can limit the specialization of each Expert and lead to resource waste during training and inference.</p><p>DeepSeek’s solution is to add a set of shared Experts, which are activated for every training sample. They aim for these shared Experts to learn general knowledge during training, so other Experts don’t need to learn this general knowledge and can focus on specialized knowledge. During inference, these shared Experts are also activated each time to provide general knowledge information.</p><p>This is another intuitive architectural innovation. However, since the Expert scale in previous MoE architectures was not large, the significance of this optimization was not apparent. Only when the scale increases does this problem become apparent. In this paper, DeepSeek scaled the number of shared Experts proportionally with the number of Experts. However, with more training and practice, they found that not so many shared Experts were needed, and by V3, only one shared Expert was used.</p><h2 id="Reflections"><a href="#Reflections" class="headerlink" title="Reflections"></a>Reflections</h2><p>After reading this paper, my biggest takeaway is that DeepSeek is not blindly stacking data with an already validated architecture but is genuinely innovating at the model level. This has resulted in many frameworks being unable to run DeepSeek’s models or having poor performance immediately after the V3&#x2F;R1 boom, as DeepSeek’s model architecture is significantly different from others.</p><p>Moreover, unlike the innovations like MLA, GRPO, and NSA mentioned in other DeepSeek papers, which require complex mathematical skills, these two model innovations are relatively intuitive. However, at that time, only DeepSeek dared to try this approach, while others were still following Llama’s Dense model. Having the courage to make unconventional attempts requires a lot of bravery, and here I can only once again express my respect to the DeepSeek team.</p><p><img src="/en/../images/20250329200735.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Those who have been following DeepSeek’s work since DeepSeek V3&amp;#x2F;R1 might easily think that a significant portion of DeepSeek’s effor</summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/en/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/en/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/en/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>From DeepSeek LLM to DeepSeek R1 — DeepSeek LLM</title>
    <link href="http://oilbeater.com/en/2025/03/14/from-deepseek-llm-to-r1-1/"/>
    <id>http://oilbeater.com/en/2025/03/14/from-deepseek-llm-to-r1-1/</id>
    <published>2025-03-14T10:48:50.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>Recently, I came across a collection of papers published by DeepSeek, documenting the evolution from the first version of DeepSeek’s LLM to the latest R1. From today’s perspective, we know that DeepSeek R1 has reached a level close to the industry’s leading models. However, what intrigues me more is how it evolved from a quant company in China that was initially overlooked to where it is now.</p><p>In this series of blog posts, I will attempt to trace their exploratory trajectory from a paper-reading perspective, following the path of DeepSeek LLM -&gt; DeepSeek MoE -&gt; DeepSeek V2 -&gt; DeepSeek V3 -&gt; DeepSeek R1. While organizing the papers, I discovered that DeepSeek’s first publicly released paper was in January 2024, when they had just released their first version of the model. Even within the AI industry, they were not considered a major competitor at that time. Yet, just a year later in January 2025, they had evolved to the leading-edge level of R1. It is often said that in AI, a day is like a year in the human world. But when you actually see the progress made in a year, it is truly awe-inspiring how fast DeepSeek has advanced.</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>DeepSeek LLM was the first LLM open-sourced by DeepSeek. At that time, the most popular open-source LLM was LLaMA-2, and many models were based on its architecture and foundation. From a retrospective perspective, we now know that DeepSeek eventually chose a MoE architecture different from the Dense architecture of LLaMA. However, in the first version, they essentially copied LLaMA-2’s architecture with some local adjustments. It can be inferred that the team was still in the exploratory phase regarding model architecture at that time.</p><p>Although the architecture was largely the same as LLaMA-2, with both trained on 2T tokens, DeepSeek LLM outperformed LLaMA-2 comprehensively in performance evaluations, as shown in the image below. The paper introduced some interesting findings about data, training, and fine-tuning methods.</p><p><img src="/en/../images/deepseekllm.png" alt="alt text"></p><p>It is noteworthy that DeepSeek could have used more data and larger models during training, which would undoubtedly improve model performance. However, the purpose of this paper was mainly to compare with LLaMA-2, so they intentionally kept the data scale and parameter size as similar as possible to focus on other areas for improvement.</p><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>There were significant differences in data selection between LLaMA-2 and DeepSeek LLM. Although both used 2T tokens, LLaMA-2 had close to 90% of its corpus in English, while DeepSeek LLM did not specify the language proportions but implied that the ratio of English to Chinese in the corpus was more balanced. Logically, it was not surprising that DeepSeek LLM significantly outperformed in Chinese evaluations. What was unexpected was that in English evaluation metrics, DeepSeek LLM achieved comparable performance despite significantly less training data.</p><p>I speculate that there are two reasons for this phenomenon. First, the quality of DeepSeek LLM’s corpus was higher, compensating for the quantity disadvantage. LLaMA-2 mentioned that it did not filter the dataset for sensitive information, while DeepSeek LLM introduced a model to assess data quality, specifically increasing the proportion of data from niche fields to enhance data diversity. Therefore, it can be inferred that the English corpus quality of DeepSeek LLM was higher. For reference, LLaMA-3 also introduced deduplication and quality filtering during data preparation to improve corpus quality.</p><p>The other reason I suspect is that the introduction of Chinese corpus also improved the model’s performance in English. OpenAI’s GPT 3.5 was primarily trained on English corpus but performed well in Chinese. A possible reason is that knowledge learned from English corpus was transferred to Chinese. Similarly, knowledge learned from Chinese corpus could be transferred to English. Additionally, since Chinese and English have significant differences in grammar and expression, this diverse data might enhance the model’s capabilities. Furthermore, different languages have distinct cultural backgrounds and content tendencies, further increasing data diversity. If this speculation holds, then preparing the corpus should intentionally increase the proportion of different languages to allow the model to learn richer linguistic expressions.</p><h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>In terms of model architecture, DeepSeek LLM was almost identical to LLaMA-2, using the same technologies such as Pre-Norm, FFN activation functions, and positional encoders. The biggest difference was the use of GQA (Group Query Attention). Compared to the original MHA (Multi Head Attention), GQA can be understood as a method to save training and inference memory usage by allowing multiple Query heads to share a set of Key and Value parameter matrices, significantly reducing memory consumption. However, this comes at the cost of reducing the number of Key and Value latent spaces, thereby diminishing the model’s expressive capabilities. LLaMA-2 addressed this by increasing the width of the FFN network to provide more nonlinear expressive capabilities, while DeepSeek LLM chose to increase the number of Attention layers. Roughly speaking, although the model parameters were the same, LLaMA-2 was a wider model, and DeepSeek LLM was a deeper model. From a retrospective perspective, DeepSeek’s subsequent MLA introduced in V2 made a radical update to the Attention layer, directly reducing the KV Cache required for inference by an order of magnitude.</p><p>Another difference was that LLaMA-2 used a cosine learning scheduler, while DeepSeek LLM used a Multi-Step learning scheduler. The reason given was that when increasing the amount of data, Multi-Step could better utilize the results from the previous stage, allowing for faster continuous training.</p><p>The paper also spent a significant amount of space explaining how to choose appropriate hyperparameters under different data scales, data qualities, and model sizes, and how to draw scaling law curves. This was presented as the biggest highlight of the paper, but it felt like alchemy to me, giving me a headache. Interested readers can check it out themselves.</p><h2 id="Post-Training"><a href="#Post-Training" class="headerlink" title="Post-Training"></a>Post-Training</h2><p>At the time of the paper’s publication, post-training mainly involved alignment, using SFT and RLHF to align with human preferences and increase model safety. The data used were mostly labeled conversational texts, with no special processing of the data distribution. DeepSeek LLM made a very different choice in data selection compared to LLaMA-2.</p><p>If you look at the model performance evaluation comparison chart at the top, you can see that DeepSeek LLM performed much better in non-Chinese metrics such as MATH, HumanEval, and MBPP. This is because nearly 70% of the samples in DeepSeek’s post-training SFT phase were related to Math and Code. It is evident that they did not prioritize alignment in post-training but focused on enhancing model capabilities, making this more of a sneaky optimization for ranking.</p><p>At that time, the mainstream approach was to SFT a code and math-focused model after the base model was trained, such as Code LLaMA and OpenAI Codex, which were SFTed from LLaMA-2 and OpenAI GPT3, respectively. Meta even SFTed a Python-specific LLM from Code LLaMA.</p><p>Now we know that performing RL on Math and Code samples during the post-training phase can stimulate the model’s CoT reasoning capabilities, and the idea for R1 might have been born at this time.</p><p>Additionally, DeepSeek LLM did not use the popular RLHF at that time but chose DPO (Direct Preference Optimization) for aligning with human preferences. This method directly optimizes the probability difference between two different generation results as the training objective, which is more intuitive and easier to design compared to RL. DPO was also used in the post-training process of LLaMA-3. It is evident that the DeepSeek team was not satisfied with the existing RL algorithms and was still exploring. This led to the later GRPO announced in DeepSeek Math.</p><h2 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h2><p>When I was in school, my teacher told me that real Future Work should not be written in the paper; instead, you should secretly work on it and publish the next paper. The Future Work section in DeepSeek LLM’s paper, from today’s perspective, is too sincere, almost pointing out the path to R1.</p><blockquote><p>DeepSeek LLM will be a long-term project, focusing on promoting the progress of open-source models.</p></blockquote><p>This is hard to say, as it has only been a year or so.</p><blockquote><p>Soon, we will release our technique reports in code intelligence and Mixture-of-Experts (MoE),<br>respectively. They show how we create high-quality code data for pre-training, and design<br>a sparse model to achieve dense model performance.</p></blockquote><p>This “Soon” meant releasing MoE in a week and DeepSeek Code in half a month. We now know that MoE became the foundational architecture for V2, V3, and R1, with parameters rising to 671B.</p><blockquote><p>At present, we are constructing a larger and improved dataset for the upcoming version of<br>DeepSeek LLM. We hope the reasoning, Chinese knowledge, math, and code capabilities<br>will be significantly improved in the next version</p></blockquote><p>The data volume increased from 2T to 8T in half a year, although LLaMA-3 increased to 15T at the same time. DeepSeek V2’s metrics were slightly behind LLaMA-3 in English, and by V2, their focus had shifted to wildly reducing costs and lead a crazy price war has in China.</p><blockquote><p>Our alignment team is dedicated to studying ways to deliver a model that is helpful,<br>honest, and safe to the public. Our initial experiments prove that reinforcement learning<br>could boost model complex reasoning capability.</p></blockquote><p>Looking back from today, this is precisely the most important method of R1.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>From today’s perspective, DeepSeek was likely still in the exploration phase, aligning with the open-source models in the industry and conducting theoretical research. However, from the details in the paper, the conditions for the groundbreaking R1 to be born a year later were almost in place.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Recently, I came across a collection of papers published by DeepSeek, documenting the evolution from the first version of DeepSeek’s LLM </summary>
      
    
    
    
    
    <category term="DeepSeek" scheme="http://oilbeater.com/en/tags/DeepSeek/"/>
    
    <category term="LLM" scheme="http://oilbeater.com/en/tags/LLM/"/>
    
    <category term="Paper" scheme="http://oilbeater.com/en/tags/Paper/"/>
    
  </entry>
  
  <entry>
    <title>Extending KubeVirt Through the Network Binding Plugin</title>
    <link href="http://oilbeater.com/en/2025/01/12/kubevirt-networking/"/>
    <id>http://oilbeater.com/en/2025/01/12/kubevirt-networking/</id>
    <published>2025-01-12T08:16:07.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>In the new KubeVirt v1.4 release, the <a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network Binding Plugin</a> has been elevated to Beta, offering a new way to extend KubeVirt’s networking capabilities. Although it is nominally designed to enhance networking, in practice, this mechanism can do much more. Any changes related to the libvirt domain XML can be implemented through this mechanism.</p><h2 id="KubeVirt-Network-Overview"><a href="#KubeVirt-Network-Overview" class="headerlink" title="KubeVirt Network Overview"></a>KubeVirt Network Overview</h2><p>First, let’s examine the existing networking mechanisms in KubeVirt and identify their shortcomings, as well as how the new mechanism facilitates extensions.</p><p>Since KubeVirt uses an architecture where VMs run inside Pods, it reuses the CNI networking mechanism. This approach divides networking into two parts: one is the Pod network provided by various CNIs, and the other is how the CNI-provided network is connected to the VM. In libvirt, this part is referred to as the Domain network.</p><p>KubeVirt’s previous networking mechanisms (Bridge, Masquerade, Passt, Slirp) essentially connect the Pod’s <code>eth0</code> interface to the VM’s <code>tap0</code> network interface using different technical solutions. For example, Bridge connects <code>tap0</code> and <code>eth0</code> to the same bridge, Masquerade routes <code>tap0</code> traffic through iptables NAT rules into <code>eth0</code>, and Passt and Slirp perform traffic redirection via a user-space network stack.</p><p><img src="/en/../images/kubevirt-networking-tradition.png" alt="alt text"></p><p>These methods are similar in implementation: they perform some network-related configurations inside the Pod and then modify libvirt’s startup parameters to connect to the respective network. However, the existing mechanisms are hardcoded into KubeVirt Core and lack an extension mechanism. Adding a new mechanism or modifying an existing one requires altering KubeVirt’s code, which is inflexible. For instance, the default bridge plugin hijacks DHCP requests but does not support IPv6, making dual-stack configurations under bridge mode difficult to achieve. Additionally, the DHCP implemented in Kube-OVN is bypassed by this mechanism. Previously, enabling dual-stack bridge required modifying KubeVirt’s code to disable the default DHCP, which was cumbersome. Therefore, the new version abstracts this set of mechanisms to provide a universal extension framework.</p><h2 id="Hook-Sidecar"><a href="#Hook-Sidecar" class="headerlink" title="Hook Sidecar"></a>Hook Sidecar</h2><p>Let’s first look at an existing extension mechanism in KubeVirt: the <a href="https://kubevirt.io/user-guide/user_workloads/hook-sidecar/">Hook Sidecar</a>.</p><p>This mechanism allows loading a user-defined image or a Shell&#x2F;Python script stored in a ConfigMap before the VM is officially created. It modifies libvirt’s startup parameters and cloud-init parameters before the VM starts.</p><p>Its execution mechanism is somewhat similar to CNI. Before starting a VM, <code>virt-handler</code> searches the corresponding directory for two binaries: <code>/usr/bin/onDefineDomain</code> and <code>/usr/bin/preCloudInitIso</code>. The former takes the libvirt XML configuration generated by <code>virt-handler</code> as input and returns the modified configuration; the latter takes the cloud-init configuration and returns the modified cloud-init configuration. This way, any libvirt and cloud-init parameters not natively supported by KubeVirt can be injected and modified through this mechanism. Moreover, since the Sidecar can execute arbitrary code, it can do much more than just modify these two configurations. Any capabilities not implemented by KubeVirt during the initialization phase can be realized here.</p><h2 id="Network-Binding-Plugin"><a href="#Network-Binding-Plugin" class="headerlink" title="Network Binding Plugin"></a>Network Binding Plugin</h2><p>Now let’s discuss the Network Binding Plugin mechanism, which is fundamentally similar to the Hook Sidecar. The main difference is that it replaces binary calls with gRPC calls. The methods registered in gRPC remain <code>onDefineDomain</code> and <code>preCloudInitIso</code>, but the parameters are passed through gRPC requests instead of command-line arguments. Everything else remains the same.</p><p>A specific example can be found in the <a href="https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding">Slirp Binding</a> implementation, which is still part of the KubeVirt codebase. Although the Network Binding Plugin specification has added a <code>networkAttachmentDefinition</code> field to select a CNI, this can still be achieved using the previous network interface selection mechanism. Furthermore, since the Sidecar can execute arbitrary code, it is also possible to implement a CNI within the Sidecar to override the Pod’s original network.</p><p>The resulting network architecture is depicted in the diagram below:</p><p><img src="/en/../images/networking-binding.png" alt="alt text"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Although the Network Binding Plugin mechanism is designed for network extensions, it can actually extend almost all processing logic on the <code>virt-handler</code> side of KubeVirt. It even allows treating KubeVirt as merely a framework, with all logic handled through Sidecars. This flexibility is expected to enable numerous innovative uses in the future.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://kubevirt.io/user-guide/user_workloads/hook-sidecar/">Hook Sidecar</a></li><li><a href="https://kubevirt.io/user-guide/network/network_binding_plugins/">Network Binding Plugins</a></li><li><a href="https://github.com/kubevirt/kubevirt/blob/main/docs/network/network-binding-plugin.md">Network Binding Plugin Documentation</a></li><li><a href="https://github.com/kubevirt/kubevirt/tree/main/cmd/sidecars/network-slirp-binding">Slirp Binding Implementation</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In the new KubeVirt v1.4 release, the &lt;a href=&quot;https://kubevirt.io/user-guide/network/network_binding_plugins/&quot;&gt;Network Binding Plugin&lt;/a</summary>
      
    
    
    
    
    <category term="kubevirt" scheme="http://oilbeater.com/en/tags/kubevirt/"/>
    
    <category term="networking" scheme="http://oilbeater.com/en/tags/networking/"/>
    
  </entry>
  
  <entry>
    <title>AI Gateway Survey: Cloudflare AI Gateway</title>
    <link href="http://oilbeater.com/en/2024/08/26/ai-gateway-cloudflare/"/>
    <id>http://oilbeater.com/en/2024/08/26/ai-gateway-cloudflare/</id>
    <published>2024-08-26T03:03:34.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>With the rising popularity of AI, I noticed that many API gateway products rebrand themselves as AI Gateways. This prompted me to explore what these so-called AI Gateways actually offer. My research includes those that started with API management or cloud-native Ingress Controllers and later integrated AI features, such as: <a href="https://konghq.com/products/kong-ai-gateway">Kong</a>, <a href="https://www.solo.io/products/gloo-ai-gateway/">Gloo</a>, and <a href="https://higress.io/en/">Higress</a>. It also covers gateways that were AI-native from the start, like <a href="https://portkey.ai/features/ai-gateway">Portkey</a> and <a href="https://github.com/songquanpeng/one-api">OneAPI</a>, as well as the public cloud Serverless-based <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a> discussed in this blog.</p><p>Generally, current AI Gateways excel in three main areas:</p><p><strong>Standard API gateway features applied to AI APIs</strong> such as monitoring, logging, rate limiting, reverse proxy, request or response rewriting, and user system integration. These features, although crucial, are not specifically AI-related; they treat LLM APIs like any standard API Service.</p><p><strong>API gateway features optimized for AI</strong> include enhancements like token-based rate limiting, prompt-based caching, firewall filtering based on prompts and LLM responses, load balancing across multiple LLM API keys, and API translation among different LLM providers. These functionalities extend existing API gateway concepts for AI scenarios.</p><p><strong>New features added for AI applications</strong> include embedding and RAG capabilities, exposing vector and text database functionalities via APIs. There are also cost optimizations related to token usage, such as prompt simplification and semantic caching. Additionally, some gateways offer application-layer features, like scoring the outputs of LLMs.</p><p>This blog post highlights the features of the <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a>.</p><h1 id="Basic-Principle"><a href="#Basic-Principle" class="headerlink" title="Basic Principle"></a>Basic Principle</h1><p>Cloudflare’s AI Gateway mainly functions as a reverse proxy. After reviewing it, I realized that I could potentially replicate similar functionalities using Cloudflare Worker. If you’re currently using OpenAI’s API, you simply need to change the SDK’s baseURL to <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/openai</code>. Through this setup, Cloudflare can offer monitoring, logging, and caching as traffic goes through their platform.</p><p>This approach has several advantages:</p><ul><li>Easy integration by changing the baseURL, with no change in API format. It is entirely Serverless and free, effectively giving away monitoring capabilities.</li><li>Leveraging Cloudflare’s global network can accelerate user access, though this is minimal compared to the latency of LLMs themselves. The most noticeable speed improvement may be in the latency of the first token response.</li><li>It can obscure the source IP, useful for bypassing regional restrictions on certain OpenAI API accesses.</li></ul><p>However, there are also drawbacks:</p><ul><li>All request data, including API keys, pass through Cloudflare, posing potential security risks.</li><li>The gateway lacks a plugin mechanism, making it challenging to extend functionalities without additional external layers.</li><li>Constantly changing IP addresses through Cloudflare’s network might trigger security measures from OpenAI.</li></ul><h1 id="Key-Features"><a href="#Key-Features" class="headerlink" title="Key Features"></a>Key Features</h1><h2 id="Multi-provider-Support"><a href="#Multi-provider-Support" class="headerlink" title="Multi-provider Support"></a>Multi-provider Support</h2><p>Since Cloudflare AI Gateway simply acts as a reverse proxy without altering the LLM API, it can support almost any mainstream LLM API by changing the baseURL to the format: <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/&#123;provider&#125;</code>.</p><p>It also offers a <a href="https://developers.cloudflare.com/ai-gateway/providers/universal/">Universal Endpoint</a> for simple fallbacks, allowing multiple provider queries in one API request to automatically call the next provider if the previous one fails.</p><h2 id="Observability"><a href="#Observability" class="headerlink" title="Observability"></a>Observability</h2><p>In addition to basic monitoring like QPS and error rates, Cloudflare provides specific dashboards for tokens, costs, and cache hit rates for LLM scenarios.</p><p>The logging is similar to that of Workers, focusing only on real-time logs without historical data access. This limitation makes it difficult for AI applications that rely on analyzing request and response logs for optimizations or fine-tuning.</p><h2 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h2><p>Cloudflare’s caching is still based on exact text matches, likely implemented using <a href="https://developers.cloudflare.com/kv/">Cloudflare Workers KV</a>. Custom cache keys and settings, including TTL and cache bypass, are possible, but semantic caching is not yet available, although promised for the future.</p><h2 id="Rate-Limiting"><a href="#Rate-Limiting" class="headerlink" title="Rate Limiting"></a>Rate Limiting</h2><p>Cloudflare’s rate limiting still follows traditional QPS-based methods without any AI-specific enhancements, like token-based limiting, which could be improved in the future.</p><h2 id="Custom-Metadata"><a href="#Custom-Metadata" class="headerlink" title="Custom Metadata"></a>Custom Metadata</h2><p>Custom headers can be added to requests, such as user information, retrievable through logging.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Overall, Cloudflare AI Gateway excels in simplicity and ease of use. New users can integrate within minutes, providing essential monitoring and caching capabilities. Despite its straightforward implementation, it lacks depth in more advanced features, and extending functionalities is cumbersome, requiring additional setups with Workers. A potential improvement could be open-sourcing the AI Gateway as a template, allowing users to modify code or create plugins to build a new ecosystem, considering it likely operates similarly to a Worker template.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;With the rising popularity of AI, I noticed that many API gateway products rebrand themselves as AI Gateways. This prompted me to explore</summary>
      
    
    
    
    
    <category term="AI" scheme="http://oilbeater.com/en/tags/AI/"/>
    
    <category term="Gateway" scheme="http://oilbeater.com/en/tags/Gateway/"/>
    
    <category term="Cloudflare" scheme="http://oilbeater.com/en/tags/Cloudflare/"/>
    
  </entry>
  
  <entry>
    <title>5 Solutions for Multi-Cluster Communication in Kubernetes</title>
    <link href="http://oilbeater.com/en/2024/05/24/five-kubernetes-multicluster-network/"/>
    <id>http://oilbeater.com/en/2024/05/24/five-kubernetes-multicluster-network/</id>
    <published>2024-05-24T08:25:30.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>As enterprises scale their operations, the use of Kubernetes extends from single-cluster to multi-cluster deployments. In a multi-cluster environment, communication between clusters becomes a critical area of research. This article introduces the basic principles, advantages, and limitations of five solutions for cross-Kubernetes cluster communication.</p><h2 id="1-Underlay-Network"><a href="#1-Underlay-Network" class="headerlink" title="1. Underlay Network"></a>1. Underlay Network</h2><p>This type of network plugin includes <a href="https://www.cni.dev/plugins/current/main/macvlan/">macvlan</a>, <a href="https://www.cni.dev/plugins/current/main/ipvlan/">ipvlan</a>, <a href="https://kubeovn.github.io/docs/stable/start/underlay/">Kube-OVN underlay</a>, and various cloud VPC CNIs.</p><p><strong>Basic Principle</strong>:</p><p>From the CNI (Container Network Interface) perspective, the underlay network is the simplest approach. This method relies on the underlying infrastructure to achieve connectivity at the network level, such as using VPC Peering on public clouds or configuring routing and large Layer 2 networks in physical networks. Once the underlying network is connected, cross-cluster container networks naturally connect.</p><p><strong>Advantages</strong>:</p><ul><li>Simplest from a CNI perspective, requiring no additional operations.</li><li>Architecturally clear, delegating the responsibility of cross-cluster communication to the underlying network.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Depends on specific CNI; underlay networks have limited use cases, and some scenarios can only use overlay networks.</li><li>Difficulties in heterogeneous environments, such as between multiple clouds or between public and private clouds.</li><li>Provides only basic container network communication without higher-level functionalities like service discovery, domain names, and network policies.</li><li>Lacks fine-grained control by connecting all cluster container networks at once.</li></ul><h2 id="2-Overlay-CNI-Providing-Cross-Cluster-Communication"><a href="#2-Overlay-CNI-Providing-Cross-Cluster-Communication" class="headerlink" title="2. Overlay CNI Providing Cross-Cluster Communication"></a>2. Overlay CNI Providing Cross-Cluster Communication</h2><p>When underlay networks cannot be used, some specific CNIs achieve cross-cluster communication at the overlay level, such as <a href="https://cilium.io/use-cases/cluster-mesh/">Cilium Cluster Mesh</a>, <a href="https://antrea.io/docs/v2.0.0/docs/multicluster/quick-start/">Antrea Multi-Cluster</a>, and <a href="https://kubeovn.github.io/docs/stable/en/advance/with-ovn-ic/">Kube-OVN with ovn-ic</a>.</p><p><strong>Basic Principle</strong>:</p><p>These CNIs typically select a set of nodes within the cluster as gateway nodes, which then establish tunnels between each other. Cross-cluster traffic is forwarded through these gateway nodes.</p><p><strong>Advantages</strong>:</p><ul><li>CNIs include cross-cluster functionality without needing additional components.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Dependent on specific CNIs, cannot achieve communication between different CNIs.</li><li>Cannot handle CIDR overlap; network segments need to be pre-planned.</li><li>Except for Cilium, which implements a complete solution including cross-cluster service discovery and network policies, others only provide basic container network communication.</li><li>Lacks fine-grained control by connecting all cluster container networks at once.</li></ul><h2 id="3-Submariner"><a href="#3-Submariner" class="headerlink" title="3. Submariner"></a>3. Submariner</h2><p>Given the common need for cross-cluster network interconnectivity, similar implementations lead to duplicated efforts among various CNIs. <a href="https://submariner.io/">Submariner</a>, an independent, cross-cluster network plugin, offers a generic solution capable of connecting clusters with different CNIs into one network. Initially created by engineers at Rancher and now a CNCF Sandbox project with active participation from Red Hat engineers.</p><p><strong>Basic Principle</strong>:</p><p>Submariner selects gateway nodes within the cluster that communicate via VXLAN. Cross-cluster traffic is transmitted through VXLAN. Submariner relies on the CNI to send egress traffic to the host network, which then forwards it. Additionally, Submariner deploys a set of CoreDNS for cross-cluster service discovery and uses a Globalnet Controller to address CIDR overlap issues.</p><p><strong>Advantages</strong>:</p><ul><li>CNI-agnostic to some extent, allowing the connection of clusters with different CNIs.</li><li>Implements cross-cluster service discovery, supporting service and domain name resolution.</li><li>Supports communication between clusters with overlapping CIDRs, avoiding IP conflicts post-deployment.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Not compatible with all CNIs, especially those like macvlan or short-circuiting Cilium where the host cannot see the traffic.</li><li>Gateway operates in active-passive mode, lacking horizontal load balancing, potentially causing performance bottlenecks in high-traffic scenarios.</li><li>Lacks fine-grained control by connecting all cluster container networks at once.</li></ul><h2 id="4-Skupper"><a href="#4-Skupper" class="headerlink" title="4. Skupper"></a>4. Skupper</h2><p><a href="https://skupper.io/index.html">Skupper</a> is considered the most interesting among the solutions, enabling service-layer network connectivity on demand, avoiding the control issues of complete connectivity. It innovatively uses a layer 7 message queue to achieve this, being entirely independent of the underlying network and CNI, making it very easy to get started. Currently, most contributors are engineers from Red Hat.</p><p><strong>Basic Principle</strong>:</p><p>Unlike the previous solutions that use tunnels to connect container IPs directly, Skupper introduces the concept of VAN (Virtual Application Networks) to connect networks at layer 7. Simply put, instead of directly connecting IPs, it connects services. Conceptually similar to ServiceExporter and ServiceImporter, but predating these community concepts, it was quite innovative at its inception.</p><p>Skupper uses a message queue implementation, forming a large message queue between multiple clusters. Cross-cluster communication packets are sent to this message queue and consumed on the other end. This approach is similar to reverse proxy but implemented with a message queue, making it a very novel idea. Services become message subscription points for consumption, allowing the establishment of a message queue between the server and client as needed, managing and controlling this message path through the message queue concept.</p><p><strong>Advantages</strong>:</p><ul><li>Excellent CNI compatibility, completely independent of CNI behavior at the application layer for packet connectivity.</li><li>Easy to get started, requiring no complex upfront network planning and no CIDR non-overlap requirements. Provides CLI for temporary testing and quick demonstrations.</li><li>Connects services on demand instead of entire container networks, allowing fine-grained control with lower infrastructure requirements.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Currently supports only TCP protocol; UDP and lower-level protocols like ICMP have issues.</li><li>IP information is lost as messages are forwarded through the message queue.</li><li>Using a message queue for forwarding might lead to performance issues like latency and throughput loss.</li><li>Given TCP’s stateful nature, converting it entirely to a message queue form and ensuring compatibility is questionable.</li></ul><h2 id="5-KubeSlice"><a href="#5-KubeSlice" class="headerlink" title="5. KubeSlice"></a>5. KubeSlice</h2><p><a href="https://kubeslice.io/documentation/open-source/1.3.0">KubeSlice</a> is a project that recently entered the CNCF Sandbox. Solutions based on tunnels cannot achieve fine-grained control and full CNI compatibility, while those based on the application layer cannot fully support network protocols. KubeSlice offers a new approach, attempting to solve both issues simultaneously.</p><p><strong>Basic Principle</strong>:</p><p>KubeSlice’s basic idea is simple and straightforward: dynamically insert a network card into a pod as needed, creating an overlay network for cross-cluster communication on this card. This overlay network then implements service discovery and network policies. Users can dynamically create and join networks across namespaces or pods, achieving flexible fine-grained control. Since it operates on a second network card, it is highly compatible with network protocols.</p><p><strong>Advantages</strong>:</p><ul><li>High CNI compatibility, as it involves adding an extra network card, regardless of the original CNI and avoiding original network address conflicts.</li><li>High protocol compatibility, as it uses an additional network card for traffic forwarding, compatible with all network protocols.</li><li>High flexibility, providing CLI tools for dynamically creating and joining networks, allowing users to create multiple cross-cluster virtual networks as needed.</li><li>Comprehensive functionality, implementing service discovery, DNS, QoS, NetworkPolicy, and monitoring on the additional overlay network.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Applications need to be aware of the additional network card and choose the appropriate network, potentially requiring application modifications.</li><li>Since cross-cluster traffic uses a different network card, not the pod’s primary IP, external systems like monitoring and tracing might require adjustments.</li><li>Documentation appears to be an internal project made open source, with many usage methods and API descriptions not well explained, requiring users to guess parameter meanings from references. Despite seeming comprehensive, the documentation needs significant improvement for external users to use it effectively.</li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In a multi-cluster Kubernetes environment, there are multiple solutions for efficient communication. Each solution has its advantages and limitations, allowing users to choose the most suitable one based on specific needs and environments.</p><blockquote><p>This post was originally written in Chinese and I translated it to English with the help of GPT4. If you find any errors, please feel free to let me know.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;As enterprises scale their operations, the use of Kubernetes extends from single-cluster to multi-cluster deployments. In a multi-cluster</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/en/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/en/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>k8gb: The Best Open Source GSLB Solution for Cloud Native</title>
    <link href="http://oilbeater.com/en/2024/04/18/k8gb-best-cloudnative-gslb/"/>
    <id>http://oilbeater.com/en/2024/04/18/k8gb-best-cloudnative-gslb/</id>
    <published>2024-04-18T10:01:10.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>Balancing traffic across multiple Kubernetes clusters and achieving automatic disaster recovery switching has always been a headache. We have explored public clouds and <a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">Karmada Ingress</a>, and have also tried manual DNS solutions, but these approaches often fell short in terms of cost, universality, flexibility, and automation. It was not until we discovered <a href="https://www.k8gb.io/">k8gb</a>, a project initiated by South Africa’s Absa Group to provide banking-level multi-availability, that we realized the ingenuity of using various DNS protocols to deliver a universal and highly automated GSLB solution. This blog will briefly discuss the problems with other approaches and how k8gb cleverly uses DNS to implement GSLB.</p><h2 id="What-is-GSLB"><a href="#What-is-GSLB" class="headerlink" title="What is GSLB"></a>What is GSLB</h2><p>GSLB (Global Service Load Balancer) is a concept in contrast to single-cluster load balancing, which mainly serves as an entrance to a cluster to distribute traffic within the cluster, whereas GSLB typically acts as an entrance for traffic across multiple clusters, handling load balancing and fault management. On one hand, GSLB can set geographic affinity rules to route traffic closer to users, enhancing overall performance; on the other hand, it can automatically redirect traffic to a functioning cluster when one fails, minimizing the impact on users.</p><h2 id="Problems-with-Other-Solutions"><a href="#Problems-with-Other-Solutions" class="headerlink" title="Problems with Other Solutions"></a>Problems with Other Solutions</h2><h3 id="Commercial-Load-Balancers"><a href="#Commercial-Load-Balancers" class="headerlink" title="Commercial Load Balancers"></a>Commercial Load Balancers</h3><p>GSLB is not a new concept, so many commercial companies have mature products, such as <a href="https://www.f5.com/solutions/use-cases/global-server-load-balancing-gslb">F5 GSLB</a>. These products generally have the following drawbacks:</p><ol><li>They do not integrate well with cloud-native systems, often requiring deployment outside Kubernetes clusters, which complicates unified management.</li><li>They are costly and pose risks of vendor lock-in.</li></ol><h3 id="Public-Cloud-Global-Load-Balancers"><a href="#Public-Cloud-Global-Load-Balancers" class="headerlink" title="Public Cloud Global Load Balancers"></a>Public Cloud Global Load Balancers</h3><p>Public clouds offer multi-cluster load balancing products to solve the issue of traffic distribution across multiple regions, such as AWS’s <a href="https://aws.amazon.com/global-accelerator/">Global Accelerator</a> and GCP’s <a href="https://cloud.google.com/load-balancing/docs/https">External Application Load Balancer</a>. GCP even offers custom <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress">Multi Cluster Ingress</a> resources that integrate well with Kubernetes Ingress. However, they also have several problems:</p><ol><li>Although they support multi-cluster load balancing, all clusters must be within the same public cloud, preventing inter-cloud traffic scheduling.</li><li>They are not usable in private clouds.</li></ol><h3 id="Karmada-Multi-Cluster-Ingress"><a href="#Karmada-Multi-Cluster-Ingress" class="headerlink" title="Karmada Multi Cluster Ingress"></a>Karmada Multi Cluster Ingress</h3><p>Karmada is a multi-cluster orchestration tool that also provides its own multi-cluster traffic routing solution through <a href="https://karmada.io/docs/userguide/service/multi-cluster-ingress/">Karmada Multi Cluster Ingress</a>. This solution involves deploying an ingress-nginx provided by the Karmada community and defining <code>MultiClusterIngress</code> within a cluster, but it has several issues:</p><ol><li>It relies on inter-cluster container network connectivity and CRDs like ServiceImporter and ServiceExporter, which have high requirements.</li><li>It requires additional management of the ingress-nginx instance that provides GSLB services, including its deployment location, quantity, and distribution, which are operational considerations.</li><li>The <a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">multi-cluster-ingress-nginx</a> modified by the community has had little code submission in recent years, raising concerns about its reliability.</li></ol><h3 id="Simple-DNS-Approach"><a href="#Simple-DNS-Approach" class="headerlink" title="Simple DNS Approach"></a>Simple DNS Approach</h3><p>A simple, manually configured DNS-based solution can work initially. Most DNS providers offer health check features, allowing us to add multiple cluster exit IP addresses to DNS records and configure health checks for fault switching. However, this simple approach has clear limitations as it scales:</p><ol><li>It lacks good automation; a single cluster might have multiple domain names and Ingress IP combinations, and manually managing their mappings becomes increasingly challenging with scale.</li><li>DNS providers’ health checks are typically based on TCP and ICMP, so they can detect and switch during complete cluster outages. However, partial failures are undetectable, e.g., when multiple Ingresses share an ingress-controller and route traffic by domain name. If the backend instances of one service fail, but other services on the ingress-controller operate normally, the health check will still pass, preventing traffic from being redirected to another cluster.</li><li>DNS inherently involves caching at various levels, which can delay updates.</li><li>DNS health checks are a capability provided by the vendor and are not guaranteed to be available from all vendors, especially in private cloud scenarios.</li></ol><h2 id="k8gb’s-Solution"><a href="#k8gb’s-Solution" class="headerlink" title="k8gb’s Solution"></a>k8gb’s Solution</h2><p>k8gb’s solution also utilizes DNS but addresses the deficiencies of the simple DNS approach through a series</p><p> of clever designs.</p><p>The essence of the simple DNS approach’s problem is that it does not integrate well with Kubernetes. Dynamic information within Kubernetes, such as new Ingress additions, new domain names, and service health status, cannot be effectively synced to upstream DNS servers. Upstream DNS servers’ simple health checks are also inadequate for handling the complex changes within Kubernetes. Thus, a core change in k8gb is that the upstream DNS records are no longer A or AAAA records pointing to a cluster’s exit address but are forwarded to a CoreDNS configured within the cluster for DNS resolution, pushing the complex DNS logic down into the cluster for internal control. In this way, the upstream DNS only needs to serve as a simple proxy without needing to configure health checks or dynamically adjust multiple address mappings.</p><p>The user’s DNS request process is illustrated below:</p><ul><li><img src="https://www.k8gb.io/docs/images/gslb-basic-multi-cluster.svg" alt="K8GB multi-cluster interoperability"></li></ul><ol><li>The user requests a domain name’s IP record from an external DNS provider.</li><li>The external DNS forwards this request to a CoreDNS managed by k8gb within the cluster.</li><li>k8gb analyzes available Ingress IPs based on the Ingress Spec’s domain name, the Ingress Status’s Ingress IP, and the health status of backend pods within the cluster, as well as load balancing policies, and returns a usable Ingress IP to the user.</li><li>The user can then directly access the corresponding Ingress Controller via this IP.</li><li>If a k8gb-managed CoreDNS in one cluster fails, since the upstream DNS concurrently proxies DNS requests to multiple clusters, another cluster can also return its Ingress IP, allowing the user to choose from multiple available IPs.</li></ol><p>This method requires administrators to register several domain name suffixes with the upstream DNS and proxy them to each cluster’s CoreDNS. k8gb also provides automation capabilities; once certificates are properly configured, it can automatically analyze the domain names used in Ingress and automatically register them with the upstream DNS, significantly simplifying administrators’ tasks.</p><p>Another important aspect to note is that each cluster’s CoreDNS must not only record its Ingress IPs but also those of other clusters for the same Ingress. If a cluster’s corresponding service’s pods are Not Ready, CoreDNS will return NXDomain. If the client receives this response, it will treat it as if the domain cannot be resolved, even though another cluster’s service might still be operational. Therefore, k8gb must also sync all clusters’ Ingress IPs for the same domain name and their health status.</p><p>As is well known, syncing data across multiple clusters is a global challenge, but k8gb cleverly achieves data syncing through DNS.</p><p>The syncing process is illustrated below:</p><p><img src="https://www.k8gb.io/docs/images/k8gb-multi-cluster-interoperabililty.svg" alt="k8gb multi-cluster interoperability"></p><p>The general idea is that each cluster’s k8gb registers its CoreDNS’s Ingress IP with the upstream CoreDNS, allowing each cluster to directly access another cluster’s CoreDNS. Then, each cluster’s CoreDNS maintains its own <code>localtargets-app.cloud.example.com</code> Ingress IP and health status for <code>app.cloud.example.com</code>, allowing each cluster to obtain other clusters’ Ingress IPs for the same domain name and include them in their return results, achieving multi-cluster domain name resolution syncing.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>k8gb, as an open-source GSLB, achieves seamless integration with Kubernetes, enabling effective cross-cluster domain name and traffic management with minimal external dependencies—only requiring an API to add DNS records. Although this project is not yet widely popular, it is already the best solution in this field in my view.</p><blockquote><p>This post was originally written in Chinese and I translated it to English with the help of GPT4. If you find any errors, please feel free to let me know.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Balancing traffic across multiple Kubernetes clusters and achieving automatic disaster recovery switching has always been a headache. We </summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/en/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/en/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>How to Easily Integrate AI-Generated Content into Logseq Notes</title>
    <link href="http://oilbeater.com/en/2024/03/21/ai-content-to-logseq/"/>
    <id>http://oilbeater.com/en/2024/03/21/ai-content-to-logseq/</id>
    <published>2024-03-21T10:39:37.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>My daily life has already incorporated a lot of assistance from AI, such as translating text, explaining a term, answering a question, etc. However, after frequent use, I encountered another problem: these AI outputs are beneficial supplements to my knowledge, but the use is usually one-off. I don’t settle these contents down. It is inconvenient when I need to collect them into notes because I have to organize them again. Imagine if this temporary query information could be automatically saved to my note-taking system, there would no longer be a worry about losing important knowledge snippets. Thus, I came up with the idea of creating a workflow to automatically store these AI-generated contents in my note system, and then review them periodically. Here, I will discuss the limitations of some tools I researched and how I used a simple framework to automatically import AI-generated content into logseq and make it into flashcards.</p><h1 id="Problem-Decomposition-and-Preliminary-Research"><a href="#Problem-Decomposition-and-Preliminary-Research" class="headerlink" title="Problem Decomposition and Preliminary Research"></a>Problem Decomposition and Preliminary Research</h1><p>Here, I take a daily translation scenario as an example: When encountering a sentence that I don’t understand, I wish to highlight and translate it, then have AI analyze the difficult words in the sentence, and finally save these contents as flashcards format in logseq. This way, with the synchronization on the mobile side, I can review this sentence during fragmented time. So, the problem essentially consists of three parts: 1. Extracting the highlighted text 2. Writing a prompt to generate the content I want 3. Saving it to logseq.</p><p>The first two steps have been integrated into many tools, including Raycast AI and OpenAI-translator, but none of them offer an extension interface to export the results of the first two steps to a third party. Raycast could theoretically accomplish this with its plugin system, but I’m not very familiar with JS and also don’t want to rely too much on Raycast AI, as I plan to discontinue the subscription. So, I decided to write a python script to do these things and then directly call the script with Raycast’s script command and shortcut keys, theoretically completing the entire workflow with one shortcut key.</p><h1 id="How-to-Obtain-Selected-Text"><a href="#How-to-Obtain-Selected-Text" class="headerlink" title="How to Obtain Selected Text"></a>How to Obtain Selected Text</h1><p>The first challenge is obtaining the selected text, which seems to be an operation at the system level and is not easy to implement. It was not until I saw the implementation of a dictionary software that I had an epiphany:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/raycast/script-commands/blob/master/commands/apps/dictionary/look-up-in-dictionary.applescript</span></span><br><span class="line">tell application <span class="string">&quot;System Events&quot;</span></span><br><span class="line">    keystroke <span class="string">&quot;c&quot;</span> using &#123;<span class="built_in">command</span> down&#125;</span><br><span class="line">    delay 0.1</span><br><span class="line">    <span class="keyword">do</span> shell script <span class="string">&quot;open dict://&quot;</span> &amp; the clipboard</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>This script essentially simulates pressing <code>cmd + v</code> on the keyboard, then directly reads from the clipboard, bypassing the issue of obtaining selected text. Considering that I often encounter recognition errors of selected text when using Raycast, I’ve developed the habit of using <code>cmd + v</code>, so there’s no need to find other methods for obtaining selected text; directly reading the clipboard will suffice.</p><h1 id="Calling-AI"><a href="#Calling-AI" class="headerlink" title="Calling AI"></a>Calling AI</h1><p>This part is pretty straightforward. The key is setting an appropriate prompt, translating while generating a teaching guide for a sentence, and outputting as much as possible in the format of logseq:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance, choose only the top 3, and output them with an explanation of their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h1 id="Output-to-Logseq"><a href="#Output-to-Logseq" class="headerlink" title="Output to Logseq"></a>Output to Logseq</h1><p>When it came to outputting to logseq, I initially planned to call Logseq’s API, which provides a local HTTP service that can be called through HTTP in developer mode. However, when I looked at their API documentation, I was almost overwhelmed. All operations required an ID, and there was no direct way to index page IDs; it was necessary to getAll and then filter. appendBlock did not allow inserting blocks with hierarchy, requiring several API calls to complete a simple operation.</p><p>When I was about to give up, I realized that Logseq is essentially a renderer for markdown files. Since the API was difficult to use, I decided to directly write files, thus making a series of complicated API calls into enjoyable file append operations. This not only circumvented the limitations of the Logseq API but also potentially allowed integration with other markdown-based note systems.</p><p>The final code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyperclip</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> AzureOpenAI</span><br><span class="line"></span><br><span class="line">content = pyperclip.paste()</span><br><span class="line"></span><br><span class="line">azure_endpoint = <span class="string">&quot;YOUR AZURE ENDPOINT&quot;</span></span><br><span class="line">api_key = <span class="string">&quot;YOUR API KEY&quot;</span></span><br><span class="line">model = <span class="string">&quot;YOUR MODEL NAME&quot;</span></span><br><span class="line">logseq_path = <span class="string">&quot;YOUR LOGSEQ PAGE FILE PATH&quot;</span></span><br><span class="line"></span><br><span class="line">client = AzureOpenAI(</span><br><span class="line">    azure_endpoint=azure_endpoint,</span><br><span class="line">    api_key=api_key,</span><br><span class="line">    api_version=<span class="string">&quot;2024-02-15-preview&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance, choose only the top 3, and explain their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">    model=model,  <span class="comment"># model = &quot;deployment_name&quot;</span></span><br><span class="line">    messages=message_text,</span><br><span class="line">    temperature=<span class="number">0.7</span>,</span><br><span class="line">    max_tokens=<span class="number">500</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    frequency_penalty=<span class="number">0</span>,</span><br><span class="line">    presence_penalty=<span class="number">0</span>,</span><br><span class="line">    stop=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> completion.choices[<span class="number">0</span>].message.content <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;No response&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">response = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(logseq_path, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&quot;\n- &quot;</span> + content.rstrip() + <span class="string">&quot; #card #English&quot;</span> + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> response.splitlines():</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">&quot;```&quot;</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> line.rstrip() == <span class="string">&quot;&quot;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line.startswith(<span class="string">&quot;- &quot;</span>):</span><br><span class="line">            line = <span class="string">&quot;- &quot;</span> + line.rstrip()</span><br><span class="line">        f.write(<span class="string">&quot;  &quot;</span> + line + <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p>The remaining task is to set a shortcut for this script in Raycast, so the next time I encounter a sentence I don’t understand, I can select, copy, and complete the translation, difficulty extraction, and flashcard generation workflow with a shortcut key.</p><p>The final flashcard effect is as follows:</p><p><img src="/en/../../images/logseq-card.png" alt="alt text"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>By integrating AI-generated content into Logseq notes automatically, we not only enhance the efficiency of information management but also optimize the learning and workflow process. With a simple script, anyone can easily integrate this powerful technology into their daily life, enabling the accumulation and review of knowledge.</p><blockquote><p>This post was originally written in Chinese and I translated it to English with the help of GPT4. If you find any errors, please feel free to let me know.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;My daily life has already incorporated a lot of assistance from AI, such as translating text, explaining a term, answering a question, et</summary>
      
    
    
    
    
    <category term="logseq" scheme="http://oilbeater.com/en/tags/logseq/"/>
    
    <category term="ai" scheme="http://oilbeater.com/en/tags/ai/"/>
    
    <category term="learning" scheme="http://oilbeater.com/en/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>Automatic Shutdown Server under VS Code Remote SSH</title>
    <link href="http://oilbeater.com/en/2024/03/13/shutdown-remote-ssh-server/"/>
    <id>http://oilbeater.com/en/2024/03/13/shutdown-remote-ssh-server/</id>
    <published>2024-03-13T10:09:26.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>I have migrated all my development environments to GCP’s Spot Instance and connect using the VS Code Remote SSH plugin. The advantage is that GCP charges by the second, making it cost-effective as long as you shut down promptly, even with high-spec machines. The downside is that forgetting to shut down during holidays can lead to high costs. After losing more than 10 dollars, I decided to find a way to automatically shut down the machine when VS Code is idle. This involved some tricky maneuvers to execute the shutdown command within a Docker container.</p><h1 id="How-to-Determine-Idle-State"><a href="#How-to-Determine-Idle-State" class="headerlink" title="How to Determine Idle State"></a>How to Determine Idle State</h1><p>This feature is similar to the idle timeout in CodeSpace, but the Remote SSH plugin doesn’t expose this functionality, so I had to implement it myself. The main challenge was determining idle state on the server side. I couldn’t find an exposed interface in VS Code, so I wondered if there was a simple way to determine idle state outside of VS Code.</p><p>The most straightforward idea was to monitor SSH connections, as Remote SSH connects via SSH. If there are no active SSH connections on the machine, it can be considered idle and shut down. However, the connection timeout for Remote SSH is quite long, reportedly four hours, and even closing the VS Code client didn’t terminate the server-side SSH connection. Setting a timeout on the server side would lead to quick client reconnection, so the number of connections wouldn’t decrease.</p><p>Since directly monitoring the number of SSH connections wasn’t feasible, I looked into whether it was possible to determine if no traffic under existing SSH connections. Using <code>tcpdump</code>, I found that there were TCP heartbeat packets every second, even without client interaction, so no traffic wasn’t a valid indicator. However, the size of these heartbeat packets was consistently 44 bytes, which could be used to identify idle state—if there were no packets larger than 44 bytes on the SSH port for a certain period, it was considered idle.</p><p>Here’s the first version of the script:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> [ -f /root/dump ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">timeout</span> 1m tcpdump -nn -i ens4 tcp and port 22 and greater 50 -w /root/dump</span><br><span class="line"></span><br><span class="line">  line_count=$(<span class="built_in">wc</span> -l &lt; /root/dump)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$line_count</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">    shutdown -h now</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>This script achieves the functionality of shutting down the machine if there are no non-heartbeat packets on the SSH port within one minute. The next step was to automate this script’s execution.</p><h1 id="Docker-Dark-Magic"><a href="#Docker-Dark-Magic" class="headerlink" title="Docker Dark Magic"></a>Docker Dark Magic</h1><p>When packaging the script into a Docker image, I encountered an interesting issue: none of the base images contained the <code>shutdown</code> command, nor could it be easily installed. To execute the <code>shutdown</code> command on the host, it was necessary to switch to the host’s namespace from within the Docker container:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsenter -a -t 1 shutdown -h now</span><br></pre></td></tr></table></figure><p>The <code>nsenter</code> command targets the process with Pid 1 and enters all its namespaces (pid, mount, network), making it appear as if operating directly on the host when Docker runs in the shared host Pid mode. To run the container with this capability, use the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name=close --pid=host --network=host --privileged --restart=always -d close:v0.0.1</span><br></pre></td></tr></table></figure><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>By monitoring SSH traffic, we can infer if VS Code is idle and then use some Docker “dark magic” to achieve automatic shutdown. However, this method is quite complex; is there a simpler way?</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;I have migrated all my development environments to GCP’s Spot Instance and connect using the VS Code Remote SSH plugin. The advantage is </summary>
      
    
    
    
    
    <category term="vscode" scheme="http://oilbeater.com/en/tags/vscode/"/>
    
    <category term="tools" scheme="http://oilbeater.com/en/tags/tools/"/>
    
    <category term="docker" scheme="http://oilbeater.com/en/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>The Impact of Preallocating Slice Memory in Golang (Continued)</title>
    <link href="http://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/"/>
    <id>http://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/</id>
    <published>2024-03-04T18:28:09.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#basic-performance-tests">Basic Performance Tests</a></li><li><a href="#appending-an-entire-slice">Appending an Entire Slice</a></li><li><a href="#reusing-slices">Reusing Slices</a></li><li><a href="#syncpool">sync.Pool</a></li><li><a href="#bytebufferpool">bytebufferpool</a></li><li><a href="#conclusion">Conclusion</a></li></ul><p>I previously wrote about <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance/">The Impact of Preallocating Slice Memory in Golang</a>, discussing the performance effects of preallocating memory in Slices. The scenarios considered were relatively simple, and recently, I conducted further tests to provide more information, including the impact of appending an entire Slice and the use of sync.Pool and bytebufferpool on performance.</p><h1 id="Basic-Performance-Tests"><a href="#Basic-Performance-Tests" class="headerlink" title="Basic Performance Tests"></a>Basic Performance Tests</h1><p>The initial Benchmark code only considered whether the Slice was preallocated with space. The specific code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1024</span></span><br><span class="line"><span class="keyword">var</span> testtext = <span class="built_in">make</span>([]<span class="type">byte</span>, length, length)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>It’s clear that not preallocating resulted in 8 additional memory allocations, and roughly, 40% of the time was spent on these extra 8 memory allocations.</p><p>These two test cases use a loop to append elements one by one, but the performance gap is not evident when appending an entire Slice. Moreover, in these two test cases, we can’t determine the proportion of time consumed by memory allocation.</p><h1 id="Appending-an-Entire-Slice"><a href="#Appending-an-Entire-Slice" class="headerlink" title="Appending an Entire Slice"></a>Appending an Entire Slice</h1><p>Therefore, two test cases for appending an entire Slice were added to observe whether preallocating memory still has a significant impact on performance. The new case code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3829890               311.5 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3968048               306.7 ns/op          1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>Both cases only involved one memory allocation, with almost identical time consumption, significantly lower than appending elements one by one. On the one hand, appending an entire Slice knows the final size at the time of Slice expansion, eliminating the need for dynamic memory allocation and reducing overhead. On the other hand, appending an entire Slice involves block copying, reducing loop overhead and significantly improving performance.</p><p>However, there is still one memory allocation in each case, and we cannot determine the proportion of time it consumes.</p><h1 id="Reusing-Slices"><a href="#Reusing-Slices" class="headerlink" title="Reusing Slices"></a>Reusing Slices</h1><p>To calculate the cost of a single memory allocation, we designed a new test case, placing the Slice’s creation outside the loop and setting the Slice’s length to 0 at the end of</p><p> each loop iteration for reuse. This way, only one memory allocation is made over many tests, which can be considered negligible. The specific code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate2</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">        init = init[:<span class="number">0</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        514904              2171 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          761772              1333 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                4041459               320.9 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3854649               320.1 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                63147178                18.63 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>This time, the test showed no memory allocation, and the overall time consumed dropped to 5% of the previous cases. Thus, we can roughly calculate that each memory allocation consumed 95% of the time in the previous test cases, a staggering proportion. Therefore, for performance-sensitive scenarios, it’s essential to reuse objects as much as possible to avoid the overhead of repeated object creation.</p><h1 id="sync-Pool"><a href="#sync-Pool" class="headerlink" title="sync.Pool"></a>sync.Pool</h1><p>In simple scenarios, one can manually clear the Slice and reuse it within the loop, as in the previous test case. However, in real scenarios, object creation often occurs in various parts of the code, necessitating unified management and reuse. Golang’s <code>sync.Pool</code> serves this purpose, and its use is quite simple. However, its internal implementation is complex, with a lot of lock-free design for performance. For more details on its implementation, please see <a href="https://unskilled.blog/posts/lets-dive-a-tour-of-sync.pool-internals/">Let’s dive: a tour of sync.Pool internals</a>.</p><p>The redesigned test case using <code>sync.Pool</code> is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sPool = &amp;sync.Pool&#123; </span><br><span class="line">        New: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> any &#123;</span><br><span class="line">                b := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">                <span class="keyword">return</span> &amp;b</span><br><span class="line">        &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPoolByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                b := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := *b</span><br><span class="line">                <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">                        buf = <span class="built_in">append</span>(buf, testtext[j])</span><br><span class="line">                &#125;</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(b)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                bufPtr := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := * bufPtr</span><br><span class="line">                buf = <span class="built_in">append</span>(buf, testtext...)</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(bufPtr)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, <code>New</code> provides a constructor function for <code>sync.Pool</code> to create an object when none is available. To use it, the <code>Get</code> method retrieves an object from the Pool, and after use, the <code>Put</code> method returns the object to <code>sync.Pool</code>. It’s important to manage the object’s lifecycle and clear it before returning it to <code>sync.Pool</code> to avoid dirty data. The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        469431              2313 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          802392              1339 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPoolByElement-12                1212828               961.5 ns/op             0 B/op          0 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3249004               370.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3268851               368.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                62596077                18.63 ns/op            0 B/op          0 allocs/op</span><br><span class="line">BenchmarkPool-12                        32707296                35.59 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>As can be seen, using <code>sync.Pool</code> can also avoid memory allocation. Since <code>sync.Pool</code> also has some additional processing performance overhead compared to manually reusing Slice, it is slightly higher. However, considering the convenience of use and the significant performance improvement compared to not using it, it is still a good solution.</p><p>However, directly using <code>sync.Pool</code> also has two issues:</p><ol><li>For Slices, the initial memory allocated by <code>New</code> is fixed. If the runtime usage exceeds this, there may still be a lot of dynamic memory allocation adjustments.</li><li>At the other extreme, if a Slice is dynamically expanded to a large size and then returned to <code>sync.Pool</code>, it may lead memory leaks and waste.</li></ol><h1 id="bytebufferpool"><a href="#bytebufferpool" class="headerlink" title="bytebufferpool"></a>bytebufferpool</h1><p>To achieve better runtime performance, <a href="https://github.com/valyala/bytebufferpool">bytebufferpool</a> builds on <code>sync.Pool</code> with some simple statistical rules to minimize the impact of the two issues mentioned above during runtime. (The project’s author has other projects like fasthttp, quicktemplate, and VictoriaMetrics under his belt, all of which are excellent examples of performance optimization.</p><p>The main structure in the code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bytebufferpool/pool.go</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  minBitSize = <span class="number">6</span> <span class="comment">// 2**6=64 is a CPU cache line size</span></span><br><span class="line">  steps      = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">  minSize = <span class="number">1</span> &lt;&lt; minBitSize</span><br><span class="line">  maxSize = <span class="number">1</span> &lt;&lt; (minBitSize + steps - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  calibrateCallsThreshold = <span class="number">42000</span></span><br><span class="line">  maxPercentile           = <span class="number">0.95</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Pool <span class="keyword">struct</span> &#123;</span><br><span class="line">  calls       [steps]<span class="type">uint64</span></span><br><span class="line">  calibrating <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">  defaultSize <span class="type">uint64</span></span><br><span class="line">  maxSize     <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">  pool sync.Pool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>defaultSize</code> is used for the initial size allocation for Slices, and <code>maxSize</code> is for rejecting Slices exceeding this size when returned to <code>sync.Pool</code>. The core algorithm dynamically adjusts <code>defaultSize</code> and <code>maxSize</code> based on statistical size information of Slices used at runtime, avoiding extra memory allocation while preventing memory leaks.</p><p>This dynamic statistical process is relatively simple, dividing the size of Slices returned to the Pool into 20 intervals for statistics. After <code>calibrating</code> number calls, it sorts by size, choosing the most frequently used interval size as <code>defaultSize</code>. This statistical method avoids many extra memory allocations. Then, by sorting by size and setting the 95% percentile size as <code>maxSize</code>, it prevents large objects from entering the Pool statistically. This dynamic adjustment of these two values achieves better performance at runtime.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul><li>Always specify the capacity when initializing Slices.</li><li>Avoid initializing Slices in loops.</li><li>Consider using <code>sync.Pool</code> for performance-sensitive paths.</li><li>The cost of memory allocation can far exceed that of business logic.</li><li>For reusing byte buffers, consider <code>bytebufferpool</code>.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#basic-performance-tests&quot;&gt;Basic Performance Tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#appending-an-entire-slice&quot;&gt;Appending an Entire Sl</summary>
      
    
    
    
    
    <category term="performance" scheme="http://oilbeater.com/en/tags/performance/"/>
    
    <category term="golang" scheme="http://oilbeater.com/en/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>The Impact of Pre-allocating Slice Memory on Performance in Golang</title>
    <link href="http://oilbeater.com/en/2024/03/04/golang-slice-performance/"/>
    <id>http://oilbeater.com/en/2024/03/04/golang-slice-performance/</id>
    <published>2024-03-04T10:48:09.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#theoretical-basis-of-slice-memory-allocation">Theoretical Basis of Slice Memory Allocation</a></li><li><a href="#quantitative-measurement">Quantitative Measurement</a></li><li><a href="#lint-tool-prealloc">Lint Tool prealloc</a></li><li><a href="#summary">Summary</a></li></ul><p>During my code reviews, I often focus on whether the slice initialization in the code has allocated the expected memory space, that is, I always request to change from <code>var init []int64</code> to <code>init := make([]int64, 0, length)</code> format whenever possible. However, I had no quantitative concept of how much this improvement affects performance, and it was more of a dogmatic requirement. This blog will introduce the theoretical basis of how pre-allocating memory improves performance, quantitative measurements, and tools for automated detection.</p><h1 id="Theoretical-Basis-of-Slice-Memory-Allocation"><a href="#Theoretical-Basis-of-Slice-Memory-Allocation" class="headerlink" title="Theoretical Basis of Slice Memory Allocation"></a>Theoretical Basis of Slice Memory Allocation</h1><p>The implementation for Golang Slice expansion can be found in <a href="https://github.com/golang/go/blob/go1.20.6/src/runtime/slice.go#L157">slice.go under growslice</a>. The general idea is that when the Slice capacity is less than 256, each expansion will create a new slice with double the capacity; when the capacity exceeds 256, each expansion will create a new slice with 1.25 times the original capacity. Afterwards, the old slice’s data is copied to the new slice, ultimately returning the new slice.</p><p>The expansion code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">newcap := oldCap</span><br><span class="line">doublecap := newcap + newcap</span><br><span class="line"><span class="keyword">if</span> newLen &gt; doublecap &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">const</span> threshold = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> oldCap &lt; threshold &#123;</span><br><span class="line">newcap = doublecap</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Check 0 &lt; newcap to detect overflow</span></span><br><span class="line"><span class="comment">// and prevent an infinite loop.</span></span><br><span class="line"><span class="keyword">for</span> <span class="number">0</span> &lt; newcap &amp;&amp; newcap &lt; newLen &#123;</span><br><span class="line"><span class="comment">// Transition from growing 2x for small slices</span></span><br><span class="line"><span class="comment">// to growing 1.25x for large slices. This formula</span></span><br><span class="line"><span class="comment">// gives a smooth-ish transition between the two.</span></span><br><span class="line">newcap += (newcap + <span class="number">3</span>*threshold) / <span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Set newcap to the requested cap when</span></span><br><span class="line"><span class="comment">// the newcap calculation overflowed.</span></span><br><span class="line"><span class="keyword">if</span> newcap &lt;= <span class="number">0</span> &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Theoretically, if the slice’s capacity is pre-allocated, eliminating the need for dynamic expansion, we can see performance improvements in several areas:</p><ol><li>Memory needs to be allocated only once, avoiding repeated allocations.</li><li>Data copying is not required repeatedly.</li><li>There’s no need for repeated garbage collection of the old slice.</li><li>Memory is allocated accurately, avoiding the capacity waste caused by dynamic allocation.</li></ol><p>In theory, pre-allocating slice capacity should lead to performance improvements compared to dynamic allocation, but the exact amount of improvement requires quantitative measurement.</p><h1 id="Quantitative-Measurement"><a href="#Quantitative-Measurement" class="headerlink" title="Quantitative Measurement"></a>Quantitative Measurement</h1><p>We refer to the code from <a href="https://github.com/alexkohler/prealloc/blob/master/prealloc_test.go">prealloc</a> and make simple modifications to measure the impact of pre-allocating vs. dynamically allocating slices of different capacities on performance.</p><p>The test code is as follows, and by changing <code>length</code>, we can observe performance data under different scenarios:</p><figure class="highlight go"><figcaption><span>title</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line"><span class="keyword">var</span> init []<span class="type">int64</span></span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Preallocate our initial slice</span></span><br><span class="line">init := <span class="built_in">make</span>([]<span class="type">int64</span>, <span class="number">0</span>, length)</span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The first function tests the performance of dynamic allocation, and the second tests the performance of pre-allocation. The tests can be executed with the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> -bench=. -benchmem prealloc_test.go</span><br></pre></td></tr></table></figure><p>Results with <code>length = 1</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12       40228154                27.36 ns/op            8 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         55662463                19.97 ns/op           </span><br><span class="line"></span><br><span class="line"> 8 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>With <code>length = 1</code>, theoretically, both dynamic and static allocation should perform a single initial memory allocation, so there should be no difference in performance. However, pre-allocation takes 70% of the time compared to dynamic allocation, showing a 1.4x performance advantage even when the number of memory allocations remains the same. This performance improvement seems related to the continuity of variable allocation.</p><p>Results with <code>length = 10</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12        5402014               228.3 ns/op           248 B/op          5 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         21908133                50.46 ns/op           80 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>With <code>length = 10</code>, pre-allocation still only performs one memory allocation, while dynamic allocation performs 5, making pre-allocation’s performance 4 times better. This indicates that even for smaller slice sizes, pre-allocation can significantly improve performance.</p><p>Results with <code>length</code> at 129, 1025, and 10000:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># length = 129</span></span><br><span class="line">BenchmarkNoPreallocate-12         743293              1393 ns/op            4088 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocate-12          3124831               386.1 ns/op          1152 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 1025</span></span><br><span class="line">BenchmarkNoPreallocate-12         169700              6571 ns/op           25208 B/op         12 allocs/op</span><br><span class="line">BenchmarkPreallocate-12           468880              2495 ns/op            9472 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 10000</span></span><br><span class="line">BenchmarkNoPreallocate-12          14430             86427 ns/op          357625 B/op         19 allocs/op</span><br><span class="line">BenchmarkPreallocate-12            56220             20693 ns/op           81920 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>At larger capacities, static allocation still only requires one memory allocation, but the performance improvement does not scale proportionally, typically 2 to 4 times better. This may be due to other overheads or special optimizations in Golang for large capacity copying, so the performance gap does not widen as much.</p><p>When changing the slice’s contents to a more complex struct, it was expected that copying would incur greater performance overhead, but in practice, the performance gap between pre-allocation and dynamic allocation for complex structs was even smaller. It appears there are many internal optimizations, and the behavior does not always align with intuition.</p><h1 id="Lint-Tool-prealloc"><a href="#Lint-Tool-prealloc" class="headerlink" title="Lint Tool prealloc"></a>Lint Tool prealloc</h1><p>Although pre-allocating memory can bring certain performance improvements, relying solely on manual review for this issue in larger projects is prone to oversights. This is where lint tools for automatic code scanning become necessary. <a href="https://github.com/alexkohler/prealloc">prealloc</a> is such a tool that can scan for potential slices that could be pre-allocated but were not, and it can be integrated into <a href="https://golangci-lint.run/usage/linters/#prealloc">golangci-lint</a>.</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Overall, pre-allocating slice memory is a relatively simple yet effective optimization method. Even when slice capacities are small, pre-allocation can still significantly improve performance. Using static code scanning tools like prealloc, these potential optimizations can be easily detected and integrated into CI, simplifying future operations.</p><blockquote><p>Update: I previously wrote about <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance/">The Impact of Preallocating Slice Memory in Golang</a>, discussing the performance effects of preallocating memory in Slices. The scenarios considered were relatively simple, and recently, I conducted further tests to provide more information, including the impact of appending an entire Slice and the use of sync.Pool and bytebufferpool on performance here <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/">The Impact of Preallocating Slice Memory in Golang (Continued)</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#theoretical-basis-of-slice-memory-allocation&quot;&gt;Theoretical Basis of Slice Memory Allocation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#quantit</summary>
      
    
    
    
    
    <category term="performance" scheme="http://oilbeater.com/en/tags/performance/"/>
    
    <category term="golang" scheme="http://oilbeater.com/en/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>The Single-Node Kubernetes Showdown: minikube vs. kind vs. k3d</title>
    <link href="http://oilbeater.com/en/2024/02/22/minikube-vs-kind-vs-k3d/"/>
    <id>http://oilbeater.com/en/2024/02/22/minikube-vs-kind-vs-k3d/</id>
    <published>2024-02-22T00:00:00.000Z</published>
    <updated>2025-04-23T11:05:07.138Z</updated>
    
    <content type="html"><![CDATA[<p>As a developer in the cloud-native ecosystem, a common challenge is the need to frequently test applications within a Kubernetes environment. In CI, this often extends to quickly trialing configurations across various Kubernetes clusters, including single-node, high-availability, dual-stack, multi-cluster setups, and more. Thus, the ability to swiftly create and manage Kubernetes clusters on a local machine, without breaking the bank, has become a must-have. This post will dive into three popular single-node Kubernetes management tools: minikube, kind, and k3d.Highlighting their unique features, use cases, and potential pitfalls.</p><blockquote><p>TL;DR</p><p>If speed is your only concern, k3d is your best bet. If you’re after compatibility and a simulation close to reality, minikube is your safest bet. kind sits comfortably in the middle, offering a balance between the two.</p></blockquote><ul><li><a href="#technical-comparison">Technical Comparison</a><ul><li><a href="#minikube">minikube</a></li><li><a href="#kind">kind</a></li><li><a href="#k3d">k3d</a></li></ul></li><li><a href="#performance-showdown">Performance Showdown</a><ul><li><a href="#methodology">Methodology</a></li><li><a href="#results">Results</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul><h1 id="Technical-Comparison"><a href="#Technical-Comparison" class="headerlink" title="Technical Comparison"></a>Technical Comparison</h1><p>At their core, these three tools serve a similar function: managing Kubernetes on a single machine. However, their differing historical backgrounds and technical choices have led to unique nuances and use cases.</p><h2 id="minikube"><a href="#minikube" class="headerlink" title="minikube"></a>minikube</h2><p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a> is the Kubernetes community’s OG tool for quickly setting up Kubernetes locally, a first love for many Kubernetes novices. Initially, it simulated multi-node clusters via VMs on your local machine, offering a high-fidelity emulation of real-world scenarios, down to the OS and kernel module level. The downside? It’s a resource hog, and if your virtualization environment doesn’t support nested virtualization, you’re out of luck, not to mention it’s slow to start. Recently, the community introduced a Docker Driver to mitigate these issues, though at the cost of losing some VM-level emulation capabilities. On the bright side, minikube comes with a plethora of add-ons, like dashboards and nginx-ingress, for easy community component installation.</p><h2 id="kind"><a href="#kind" class="headerlink" title="kind"></a>kind</h2><p><a href="https://kind.sigs.k8s.io/">kind</a> is a more recent favorite for local Kubernetes deployment, using Docker containers to simulate nodes and focusing purely on Kubernetes standard deployments, with community components requiring manual installation. It’s the go-to for Kubernetes’ own CI processes. The upside? Quick starts and a familiar environment for Docker veterans. The downside? Container simulation lacks OS-level isolation, sharing the host’s kernel, which can complicate OS-specific testing. I once had a kernel module test fail because the host’s netfilter tweaks caused havoc in a kind-managed cluster.</p><h2 id="k3d"><a href="#k3d" class="headerlink" title="k3d"></a>k3d</h2><p><a href="https://k3d.io/stable/">k3d</a>, a featherweight in local Kubernetes deployment, shares a similar approach to kind but opts for deploying a lightweight <a href="https://k3s.io/">k3s</a> instead of standard Kubernetes. This means it inherits k3s’s pros and cons, boasting incredibly fast setup times—don’t worry about correctness; just marvel at the speed. The trade-offs include a super-slimmed-down OS (sans glibc), complicating certain OS-level operations, and a unique installation approach that might puzzle those accustomed to kubeadm’s standard deployment features.</p><h1 id="Performance-Showdown"><a href="#Performance-Showdown" class="headerlink" title="Performance Showdown"></a>Performance Showdown</h1><p>While the minikube community provides some <a href="https://minikube.sigs.k8s.io/docs/benchmarks/timetok8s/v1.32.0/">performance benchmarks</a>, comparing the startup times of our three contenders, I was curious about other aspects like image size, memory footprint, and bare-minimum setup times, prompting another round of tests.</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>Testing was straightforward, given each tool’s one-liner setup, with a few caveats:</p><ol><li>minikube used the Docker Driver to keep the speed test fair.</li><li>All tests assumed pre-downloaded images to exclude network delays.</li><li>The latest versions were tested, though Kubernetes versions varied, making this more of a qualitative than quantitative analysis.</li><li>Tests focused on basic component starts without additional plugins, ensuring essentials like CNI, CoreDNS, and CSI were included.</li><li><code>docker image</code> and <code>docker stat</code> commands were used to measure image sizes and memory usage, respectively.</li></ol><p>Commands used:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#minikube</span></span><br><span class="line">time minikube start --driver=docker --force</span><br><span class="line"></span><br><span class="line"><span class="comment">#kind</span></span><br><span class="line">time kind create cluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#k3d</span></span><br><span class="line">time k3d cluster create mycluster --k3s-arg <span class="string">&#x27;--disable=traefik,metrics-server</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@server:*&#x27;</span> --no-lb</span><br></pre></td></tr></table></figure><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><table><thead><tr><th>Name</th><th>Software Version</th><th>Kubernetes Version</th><th>Image Size</th><th>Start Time</th><th>Memory Usage</th></tr></thead><tbody><tr><td>minikube</td><td>v1.32.0</td><td>v1.28.3</td><td>1.2GB</td><td>29s</td><td>536MiB</td></tr><tr><td>kind</td><td>v0.22</td><td>v1.29.2</td><td>956MB</td><td>20s</td><td>463MiB</td></tr><tr><td>k3d</td><td>v5.6.0</td><td>v1.27.4</td><td>263MB</td><td>7s</td><td>423MiB</td></tr></tbody></table><p>Evidently, k3d takes the crown in startup performance, boasting significant advantages in image size, startup time, and memory usage. It’s a godsend for those running CI on a shoestring budget.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>If speed and resource efficiency are your top priorities, k3d is a no-brainer. For OS-level isolation tests, minikube’s VM Driver is unbeatable. For everything in between, kind offers a balanced compromise between compatibility and performance.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;As a developer in the cloud-native ecosystem, a common challenge is the need to frequently test applications within a Kubernetes environm</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/en/tags/kubernetes/"/>
    
    <category term="ci" scheme="http://oilbeater.com/en/tags/ci/"/>
    
  </entry>
  
</feed>
