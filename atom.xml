<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Oilbeater&#39;s Study Room</title>
  <icon>http://oilbeater.com/en/icon.png</icon>
  
  <link href="http://oilbeater.com/en/atom.xml" rel="self"/>
  
  <link href="http://oilbeater.com/en/"/>
  <updated>2024-03-13T11:02:43.798Z</updated>
  <id>http://oilbeater.com/en/</id>
  
  <author>
    <name>Oilbeater</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Automatic Shutdown Server under VS Code Remote SSH</title>
    <link href="http://oilbeater.com/en/2024/03/13/shutdown-remote-ssh-server/"/>
    <id>http://oilbeater.com/en/2024/03/13/shutdown-remote-ssh-server/</id>
    <published>2024-03-13T10:09:26.000Z</published>
    <updated>2024-03-13T11:02:43.798Z</updated>
    
    <content type="html"><![CDATA[<p>I have migrated all my development environments to GCP’s Spot Instance and connect using the VS Code Remote SSH plugin. The advantage is that GCP charges by the second, making it cost-effective as long as you shut down promptly, even with high-spec machines. The downside is that forgetting to shut down during holidays can lead to high costs. After losing more than 10 dollars, I decided to find a way to automatically shut down the machine when VS Code is idle. This involved some tricky maneuvers to execute the shutdown command within a Docker container.</p><h1 id="How-to-Determine-Idle-State"><a href="#How-to-Determine-Idle-State" class="headerlink" title="How to Determine Idle State"></a>How to Determine Idle State</h1><p>This feature is similar to the idle timeout in CodeSpace, but the Remote SSH plugin doesn’t expose this functionality, so I had to implement it myself. The main challenge was determining idle state on the server side. I couldn’t find an exposed interface in VS Code, so I wondered if there was a simple way to determine idle state outside of VS Code.</p><p>The most straightforward idea was to monitor SSH connections, as Remote SSH connects via SSH. If there are no active SSH connections on the machine, it can be considered idle and shut down. However, the connection timeout for Remote SSH is quite long, reportedly four hours, and even closing the VS Code client didn’t terminate the server-side SSH connection. Setting a timeout on the server side would lead to quick client reconnection, so the number of connections wouldn’t decrease.</p><p>Since directly monitoring the number of SSH connections wasn’t feasible, I looked into whether it was possible to determine if no traffic under existing SSH connections. Using <code>tcpdump</code>, I found that there were TCP heartbeat packets every second, even without client interaction, so no traffic wasn’t a valid indicator. However, the size of these heartbeat packets was consistently 44 bytes, which could be used to identify idle state—if there were no packets larger than 44 bytes on the SSH port for a certain period, it was considered idle.</p><p>Here’s the first version of the script:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> [ -f /root/dump ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">timeout</span> 1m tcpdump -nn -i ens4 tcp and port 22 and greater 50 -w /root/dump</span><br><span class="line"></span><br><span class="line">  line_count=$(<span class="built_in">wc</span> -l &lt; /root/dump)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$line_count</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">    shutdown -h now</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>This script achieves the functionality of shutting down the machine if there are no non-heartbeat packets on the SSH port within one minute. The next step was to automate this script’s execution.</p><h1 id="Docker-Dark-Magic"><a href="#Docker-Dark-Magic" class="headerlink" title="Docker Dark Magic"></a>Docker Dark Magic</h1><p>When packaging the script into a Docker image, I encountered an interesting issue: none of the base images contained the <code>shutdown</code> command, nor could it be easily installed. To execute the <code>shutdown</code> command on the host, it was necessary to switch to the host’s namespace from within the Docker container:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsenter -a -t 1 shutdown -h now</span><br></pre></td></tr></table></figure><p>The <code>nsenter</code> command targets the process with Pid 1 and enters all its namespaces (pid, mount, network), making it appear as if operating directly on the host when Docker runs in the shared host Pid mode. To run the container with this capability, use the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name=close --pid=host --network=host --privileged --restart=always -d close:v0.0.1</span><br></pre></td></tr></table></figure><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>By monitoring SSH traffic, we can infer if VS Code is idle and then use some Docker “dark magic” to achieve automatic shutdown. However, this method is quite complex; is there a simpler way?</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;I have migrated all my development environments to GCP’s Spot Instance and connect using the VS Code Remote SSH plugin. The advantage is </summary>
      
    
    
    
    
    <category term="vscode" scheme="http://oilbeater.com/en/tags/vscode/"/>
    
    <category term="tools" scheme="http://oilbeater.com/en/tags/tools/"/>
    
    <category term="docker" scheme="http://oilbeater.com/en/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>The Impact of Preallocating Slice Memory in Golang (Continued)</title>
    <link href="http://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/"/>
    <id>http://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/</id>
    <published>2024-03-04T18:28:09.000Z</published>
    <updated>2024-03-06T08:18:57.328Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#basic-performance-tests">Basic Performance Tests</a></li><li><a href="#appending-an-entire-slice">Appending an Entire Slice</a></li><li><a href="#reusing-slices">Reusing Slices</a></li><li><a href="#syncpool">sync.Pool</a></li><li><a href="#bytebufferpool">bytebufferpool</a></li><li><a href="#conclusion">Conclusion</a></li></ul><p>I previously wrote about <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance/">The Impact of Preallocating Slice Memory in Golang</a>, discussing the performance effects of preallocating memory in Slices. The scenarios considered were relatively simple, and recently, I conducted further tests to provide more information, including the impact of appending an entire Slice and the use of sync.Pool and bytebufferpool on performance.</p><h1 id="Basic-Performance-Tests"><a href="#Basic-Performance-Tests" class="headerlink" title="Basic Performance Tests"></a>Basic Performance Tests</h1><p>The initial Benchmark code only considered whether the Slice was preallocated with space. The specific code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1024</span></span><br><span class="line"><span class="keyword">var</span> testtext = <span class="built_in">make</span>([]<span class="type">byte</span>, length, length)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>It’s clear that not preallocating resulted in 8 additional memory allocations, and roughly, 40% of the time was spent on these extra 8 memory allocations.</p><p>These two test cases use a loop to append elements one by one, but the performance gap is not evident when appending an entire Slice. Moreover, in these two test cases, we can’t determine the proportion of time consumed by memory allocation.</p><h1 id="Appending-an-Entire-Slice"><a href="#Appending-an-Entire-Slice" class="headerlink" title="Appending an Entire Slice"></a>Appending an Entire Slice</h1><p>Therefore, two test cases for appending an entire Slice were added to observe whether preallocating memory still has a significant impact on performance. The new case code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3829890               311.5 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3968048               306.7 ns/op          1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>Both cases only involved one memory allocation, with almost identical time consumption, significantly lower than appending elements one by one. On the one hand, appending an entire Slice knows the final size at the time of Slice expansion, eliminating the need for dynamic memory allocation and reducing overhead. On the other hand, appending an entire Slice involves block copying, reducing loop overhead and significantly improving performance.</p><p>However, there is still one memory allocation in each case, and we cannot determine the proportion of time it consumes.</p><h1 id="Reusing-Slices"><a href="#Reusing-Slices" class="headerlink" title="Reusing Slices"></a>Reusing Slices</h1><p>To calculate the cost of a single memory allocation, we designed a new test case, placing the Slice’s creation outside the loop and setting the Slice’s length to 0 at the end of</p><p> each loop iteration for reuse. This way, only one memory allocation is made over many tests, which can be considered negligible. The specific code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate2</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">        init = init[:<span class="number">0</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        514904              2171 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          761772              1333 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                4041459               320.9 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3854649               320.1 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                63147178                18.63 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>This time, the test showed no memory allocation, and the overall time consumed dropped to 5% of the previous cases. Thus, we can roughly calculate that each memory allocation consumed 95% of the time in the previous test cases, a staggering proportion. Therefore, for performance-sensitive scenarios, it’s essential to reuse objects as much as possible to avoid the overhead of repeated object creation.</p><h1 id="sync-Pool"><a href="#sync-Pool" class="headerlink" title="sync.Pool"></a>sync.Pool</h1><p>In simple scenarios, one can manually clear the Slice and reuse it within the loop, as in the previous test case. However, in real scenarios, object creation often occurs in various parts of the code, necessitating unified management and reuse. Golang’s <code>sync.Pool</code> serves this purpose, and its use is quite simple. However, its internal implementation is complex, with a lot of lock-free design for performance. For more details on its implementation, please see <a href="https://unskilled.blog/posts/lets-dive-a-tour-of-sync.pool-internals/">Let’s dive: a tour of sync.Pool internals</a>.</p><p>The redesigned test case using <code>sync.Pool</code> is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sPool = &amp;sync.Pool&#123; </span><br><span class="line">        New: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> any &#123;</span><br><span class="line">                b := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">                <span class="keyword">return</span> &amp;b</span><br><span class="line">        &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPoolByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                b := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := *b</span><br><span class="line">                <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">                        buf = <span class="built_in">append</span>(buf, testtext[j])</span><br><span class="line">                &#125;</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(b)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                bufPtr := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := * bufPtr</span><br><span class="line">                buf = <span class="built_in">append</span>(buf, testtext...)</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(bufPtr)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, <code>New</code> provides a constructor function for <code>sync.Pool</code> to create an object when none is available. To use it, the <code>Get</code> method retrieves an object from the Pool, and after use, the <code>Put</code> method returns the object to <code>sync.Pool</code>. It’s important to manage the object’s lifecycle and clear it before returning it to <code>sync.Pool</code> to avoid dirty data. The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        469431              2313 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          802392              1339 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPoolByElement-12                1212828               961.5 ns/op             0 B/op          0 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3249004               370.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3268851               368.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                62596077                18.63 ns/op            0 B/op          0 allocs/op</span><br><span class="line">BenchmarkPool-12                        32707296                35.59 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>As can be seen, using <code>sync.Pool</code> can also avoid memory allocation. Since <code>sync.Pool</code> also has some additional processing performance overhead compared to manually reusing Slice, it is slightly higher. However, considering the convenience of use and the significant performance improvement compared to not using it, it is still a good solution.</p><p>However, directly using <code>sync.Pool</code> also has two issues:</p><ol><li>For Slices, the initial memory allocated by <code>New</code> is fixed. If the runtime usage exceeds this, there may still be a lot of dynamic memory allocation adjustments.</li><li>At the other extreme, if a Slice is dynamically expanded to a large size and then returned to <code>sync.Pool</code>, it may lead memory leaks and waste.</li></ol><h1 id="bytebufferpool"><a href="#bytebufferpool" class="headerlink" title="bytebufferpool"></a>bytebufferpool</h1><p>To achieve better runtime performance, <a href="https://github.com/valyala/bytebufferpool">bytebufferpool</a> builds on <code>sync.Pool</code> with some simple statistical rules to minimize the impact of the two issues mentioned above during runtime. (The project’s author has other projects like fasthttp, quicktemplate, and VictoriaMetrics under his belt, all of which are excellent examples of performance optimization.</p><p>The main structure in the code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bytebufferpool/pool.go</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  minBitSize = <span class="number">6</span> <span class="comment">// 2**6=64 is a CPU cache line size</span></span><br><span class="line">  steps      = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">  minSize = <span class="number">1</span> &lt;&lt; minBitSize</span><br><span class="line">  maxSize = <span class="number">1</span> &lt;&lt; (minBitSize + steps - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  calibrateCallsThreshold = <span class="number">42000</span></span><br><span class="line">  maxPercentile           = <span class="number">0.95</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Pool <span class="keyword">struct</span> &#123;</span><br><span class="line">  calls       [steps]<span class="type">uint64</span></span><br><span class="line">  calibrating <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">  defaultSize <span class="type">uint64</span></span><br><span class="line">  maxSize     <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">  pool sync.Pool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>defaultSize</code> is used for the initial size allocation for Slices, and <code>maxSize</code> is for rejecting Slices exceeding this size when returned to <code>sync.Pool</code>. The core algorithm dynamically adjusts <code>defaultSize</code> and <code>maxSize</code> based on statistical size information of Slices used at runtime, avoiding extra memory allocation while preventing memory leaks.</p><p>This dynamic statistical process is relatively simple, dividing the size of Slices returned to the Pool into 20 intervals for statistics. After <code>calibrating</code> number calls, it sorts by size, choosing the most frequently used interval size as <code>defaultSize</code>. This statistical method avoids many extra memory allocations. Then, by sorting by size and setting the 95% percentile size as <code>maxSize</code>, it prevents large objects from entering the Pool statistically. This dynamic adjustment of these two values achieves better performance at runtime.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul><li>Always specify the capacity when initializing Slices.</li><li>Avoid initializing Slices in loops.</li><li>Consider using <code>sync.Pool</code> for performance-sensitive paths.</li><li>The cost of memory allocation can far exceed that of business logic.</li><li>For reusing byte buffers, consider <code>bytebufferpool</code>.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#basic-performance-tests&quot;&gt;Basic Performance Tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#appending-an-entire-slice&quot;&gt;Appending an Entire Sl</summary>
      
    
    
    
    
    <category term="performance" scheme="http://oilbeater.com/en/tags/performance/"/>
    
    <category term="golang" scheme="http://oilbeater.com/en/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>The Impact of Pre-allocating Slice Memory on Performance in Golang</title>
    <link href="http://oilbeater.com/en/2024/03/04/golang-slice-performance/"/>
    <id>http://oilbeater.com/en/2024/03/04/golang-slice-performance/</id>
    <published>2024-03-04T10:48:09.000Z</published>
    <updated>2024-03-12T08:03:27.853Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#theoretical-basis-of-slice-memory-allocation">Theoretical Basis of Slice Memory Allocation</a></li><li><a href="#quantitative-measurement">Quantitative Measurement</a></li><li><a href="#lint-tool-prealloc">Lint Tool prealloc</a></li><li><a href="#summary">Summary</a></li></ul><p>During my code reviews, I often focus on whether the slice initialization in the code has allocated the expected memory space, that is, I always request to change from <code>var init []int64</code> to <code>init := make([]int64, 0, length)</code> format whenever possible. However, I had no quantitative concept of how much this improvement affects performance, and it was more of a dogmatic requirement. This blog will introduce the theoretical basis of how pre-allocating memory improves performance, quantitative measurements, and tools for automated detection.</p><h1 id="Theoretical-Basis-of-Slice-Memory-Allocation"><a href="#Theoretical-Basis-of-Slice-Memory-Allocation" class="headerlink" title="Theoretical Basis of Slice Memory Allocation"></a>Theoretical Basis of Slice Memory Allocation</h1><p>The implementation for Golang Slice expansion can be found in <a href="https://github.com/golang/go/blob/go1.20.6/src/runtime/slice.go#L157">slice.go under growslice</a>. The general idea is that when the Slice capacity is less than 256, each expansion will create a new slice with double the capacity; when the capacity exceeds 256, each expansion will create a new slice with 1.25 times the original capacity. Afterwards, the old slice’s data is copied to the new slice, ultimately returning the new slice.</p><p>The expansion code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">newcap := oldCap</span><br><span class="line">doublecap := newcap + newcap</span><br><span class="line"><span class="keyword">if</span> newLen &gt; doublecap &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">const</span> threshold = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> oldCap &lt; threshold &#123;</span><br><span class="line">newcap = doublecap</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Check 0 &lt; newcap to detect overflow</span></span><br><span class="line"><span class="comment">// and prevent an infinite loop.</span></span><br><span class="line"><span class="keyword">for</span> <span class="number">0</span> &lt; newcap &amp;&amp; newcap &lt; newLen &#123;</span><br><span class="line"><span class="comment">// Transition from growing 2x for small slices</span></span><br><span class="line"><span class="comment">// to growing 1.25x for large slices. This formula</span></span><br><span class="line"><span class="comment">// gives a smooth-ish transition between the two.</span></span><br><span class="line">newcap += (newcap + <span class="number">3</span>*threshold) / <span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Set newcap to the requested cap when</span></span><br><span class="line"><span class="comment">// the newcap calculation overflowed.</span></span><br><span class="line"><span class="keyword">if</span> newcap &lt;= <span class="number">0</span> &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Theoretically, if the slice’s capacity is pre-allocated, eliminating the need for dynamic expansion, we can see performance improvements in several areas:</p><ol><li>Memory needs to be allocated only once, avoiding repeated allocations.</li><li>Data copying is not required repeatedly.</li><li>There’s no need for repeated garbage collection of the old slice.</li><li>Memory is allocated accurately, avoiding the capacity waste caused by dynamic allocation.</li></ol><p>In theory, pre-allocating slice capacity should lead to performance improvements compared to dynamic allocation, but the exact amount of improvement requires quantitative measurement.</p><h1 id="Quantitative-Measurement"><a href="#Quantitative-Measurement" class="headerlink" title="Quantitative Measurement"></a>Quantitative Measurement</h1><p>We refer to the code from <a href="https://github.com/alexkohler/prealloc/blob/master/prealloc_test.go">prealloc</a> and make simple modifications to measure the impact of pre-allocating vs. dynamically allocating slices of different capacities on performance.</p><p>The test code is as follows, and by changing <code>length</code>, we can observe performance data under different scenarios:</p><figure class="highlight go"><figcaption><span>title</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line"><span class="keyword">var</span> init []<span class="type">int64</span></span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Preallocate our initial slice</span></span><br><span class="line">init := <span class="built_in">make</span>([]<span class="type">int64</span>, <span class="number">0</span>, length)</span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The first function tests the performance of dynamic allocation, and the second tests the performance of pre-allocation. The tests can be executed with the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> -bench=. -benchmem prealloc_test.go</span><br></pre></td></tr></table></figure><p>Results with <code>length = 1</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12       40228154                27.36 ns/op            8 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         55662463                19.97 ns/op           </span><br><span class="line"></span><br><span class="line"> 8 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>With <code>length = 1</code>, theoretically, both dynamic and static allocation should perform a single initial memory allocation, so there should be no difference in performance. However, pre-allocation takes 70% of the time compared to dynamic allocation, showing a 1.4x performance advantage even when the number of memory allocations remains the same. This performance improvement seems related to the continuity of variable allocation.</p><p>Results with <code>length = 10</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12        5402014               228.3 ns/op           248 B/op          5 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         21908133                50.46 ns/op           80 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>With <code>length = 10</code>, pre-allocation still only performs one memory allocation, while dynamic allocation performs 5, making pre-allocation’s performance 4 times better. This indicates that even for smaller slice sizes, pre-allocation can significantly improve performance.</p><p>Results with <code>length</code> at 129, 1025, and 10000:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># length = 129</span></span><br><span class="line">BenchmarkNoPreallocate-12         743293              1393 ns/op            4088 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocate-12          3124831               386.1 ns/op          1152 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 1025</span></span><br><span class="line">BenchmarkNoPreallocate-12         169700              6571 ns/op           25208 B/op         12 allocs/op</span><br><span class="line">BenchmarkPreallocate-12           468880              2495 ns/op            9472 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 10000</span></span><br><span class="line">BenchmarkNoPreallocate-12          14430             86427 ns/op          357625 B/op         19 allocs/op</span><br><span class="line">BenchmarkPreallocate-12            56220             20693 ns/op           81920 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>At larger capacities, static allocation still only requires one memory allocation, but the performance improvement does not scale proportionally, typically 2 to 4 times better. This may be due to other overheads or special optimizations in Golang for large capacity copying, so the performance gap does not widen as much.</p><p>When changing the slice’s contents to a more complex struct, it was expected that copying would incur greater performance overhead, but in practice, the performance gap between pre-allocation and dynamic allocation for complex structs was even smaller. It appears there are many internal optimizations, and the behavior does not always align with intuition.</p><h1 id="Lint-Tool-prealloc"><a href="#Lint-Tool-prealloc" class="headerlink" title="Lint Tool prealloc"></a>Lint Tool prealloc</h1><p>Although pre-allocating memory can bring certain performance improvements, relying solely on manual review for this issue in larger projects is prone to oversights. This is where lint tools for automatic code scanning become necessary. <a href="https://github.com/alexkohler/prealloc">prealloc</a> is such a tool that can scan for potential slices that could be pre-allocated but were not, and it can be integrated into <a href="https://golangci-lint.run/usage/linters/#prealloc">golangci-lint</a>.</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Overall, pre-allocating slice memory is a relatively simple yet effective optimization method. Even when slice capacities are small, pre-allocation can still significantly improve performance. Using static code scanning tools like prealloc, these potential optimizations can be easily detected and integrated into CI, simplifying future operations.</p><blockquote><p>Update: I previously wrote about <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance/">The Impact of Preallocating Slice Memory in Golang</a>, discussing the performance effects of preallocating memory in Slices. The scenarios considered were relatively simple, and recently, I conducted further tests to provide more information, including the impact of appending an entire Slice and the use of sync.Pool and bytebufferpool on performance here <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/">The Impact of Preallocating Slice Memory in Golang (Continued)</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#theoretical-basis-of-slice-memory-allocation&quot;&gt;Theoretical Basis of Slice Memory Allocation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#quantit</summary>
      
    
    
    
    
    <category term="performance" scheme="http://oilbeater.com/en/tags/performance/"/>
    
    <category term="golang" scheme="http://oilbeater.com/en/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>The Single-Node Kubernetes Showdown: minikube vs. kind vs. k3d</title>
    <link href="http://oilbeater.com/en/2024/02/22/minikube-vs-kind-vs-k3d/"/>
    <id>http://oilbeater.com/en/2024/02/22/minikube-vs-kind-vs-k3d/</id>
    <published>2024-02-22T00:00:00.000Z</published>
    <updated>2024-02-28T16:40:46.255Z</updated>
    
    <content type="html"><![CDATA[<p>As a developer in the cloud-native ecosystem, a common challenge is the need to frequently test applications within a Kubernetes environment. In CI, this often extends to quickly trialing configurations across various Kubernetes clusters, including single-node, high-availability, dual-stack, multi-cluster setups, and more. Thus, the ability to swiftly create and manage Kubernetes clusters on a local machine, without breaking the bank, has become a must-have. This post will dive into three popular single-node Kubernetes management tools: minikube, kind, and k3d.Highlighting their unique features, use cases, and potential pitfalls.</p><blockquote><p>TL;DR</p><p>If speed is your only concern, k3d is your best bet. If you’re after compatibility and a simulation close to reality, minikube is your safest bet. kind sits comfortably in the middle, offering a balance between the two.</p></blockquote><ul><li><a href="#technical-comparison">Technical Comparison</a><ul><li><a href="#minikube">minikube</a></li><li><a href="#kind">kind</a></li><li><a href="#k3d">k3d</a></li></ul></li><li><a href="#performance-showdown">Performance Showdown</a><ul><li><a href="#methodology">Methodology</a></li><li><a href="#results">Results</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul><h1 id="Technical-Comparison"><a href="#Technical-Comparison" class="headerlink" title="Technical Comparison"></a>Technical Comparison</h1><p>At their core, these three tools serve a similar function: managing Kubernetes on a single machine. However, their differing historical backgrounds and technical choices have led to unique nuances and use cases.</p><h2 id="minikube"><a href="#minikube" class="headerlink" title="minikube"></a>minikube</h2><p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a> is the Kubernetes community’s OG tool for quickly setting up Kubernetes locally, a first love for many Kubernetes novices. Initially, it simulated multi-node clusters via VMs on your local machine, offering a high-fidelity emulation of real-world scenarios, down to the OS and kernel module level. The downside? It’s a resource hog, and if your virtualization environment doesn’t support nested virtualization, you’re out of luck, not to mention it’s slow to start. Recently, the community introduced a Docker Driver to mitigate these issues, though at the cost of losing some VM-level emulation capabilities. On the bright side, minikube comes with a plethora of add-ons, like dashboards and nginx-ingress, for easy community component installation.</p><h2 id="kind"><a href="#kind" class="headerlink" title="kind"></a>kind</h2><p><a href="https://kind.sigs.k8s.io/">kind</a> is a more recent favorite for local Kubernetes deployment, using Docker containers to simulate nodes and focusing purely on Kubernetes standard deployments, with community components requiring manual installation. It’s the go-to for Kubernetes’ own CI processes. The upside? Quick starts and a familiar environment for Docker veterans. The downside? Container simulation lacks OS-level isolation, sharing the host’s kernel, which can complicate OS-specific testing. I once had a kernel module test fail because the host’s netfilter tweaks caused havoc in a kind-managed cluster.</p><h2 id="k3d"><a href="#k3d" class="headerlink" title="k3d"></a>k3d</h2><p><a href="https://k3d.io/stable/">k3d</a>, a featherweight in local Kubernetes deployment, shares a similar approach to kind but opts for deploying a lightweight <a href="https://k3s.io/">k3s</a> instead of standard Kubernetes. This means it inherits k3s’s pros and cons, boasting incredibly fast setup times—don’t worry about correctness; just marvel at the speed. The trade-offs include a super-slimmed-down OS (sans glibc), complicating certain OS-level operations, and a unique installation approach that might puzzle those accustomed to kubeadm’s standard deployment features.</p><h1 id="Performance-Showdown"><a href="#Performance-Showdown" class="headerlink" title="Performance Showdown"></a>Performance Showdown</h1><p>While the minikube community provides some <a href="https://minikube.sigs.k8s.io/docs/benchmarks/timetok8s/v1.32.0/">performance benchmarks</a>, comparing the startup times of our three contenders, I was curious about other aspects like image size, memory footprint, and bare-minimum setup times, prompting another round of tests.</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>Testing was straightforward, given each tool’s one-liner setup, with a few caveats:</p><ol><li>minikube used the Docker Driver to keep the speed test fair.</li><li>All tests assumed pre-downloaded images to exclude network delays.</li><li>The latest versions were tested, though Kubernetes versions varied, making this more of a qualitative than quantitative analysis.</li><li>Tests focused on basic component starts without additional plugins, ensuring essentials like CNI, CoreDNS, and CSI were included.</li><li><code>docker image</code> and <code>docker stat</code> commands were used to measure image sizes and memory usage, respectively.</li></ol><p>Commands used:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#minikube</span></span><br><span class="line">time minikube start --driver=docker --force</span><br><span class="line"></span><br><span class="line"><span class="comment">#kind</span></span><br><span class="line">time kind create cluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#k3d</span></span><br><span class="line">time k3d cluster create mycluster --k3s-arg <span class="string">&#x27;--disable=traefik,metrics-server</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@server:*&#x27;</span> --no-lb</span><br></pre></td></tr></table></figure><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><table><thead><tr><th>Name</th><th>Software Version</th><th>Kubernetes Version</th><th>Image Size</th><th>Start Time</th><th>Memory Usage</th></tr></thead><tbody><tr><td>minikube</td><td>v1.32.0</td><td>v1.28.3</td><td>1.2GB</td><td>29s</td><td>536MiB</td></tr><tr><td>kind</td><td>v0.22</td><td>v1.29.2</td><td>956MB</td><td>20s</td><td>463MiB</td></tr><tr><td>k3d</td><td>v5.6.0</td><td>v1.27.4</td><td>263MB</td><td>7s</td><td>423MiB</td></tr></tbody></table><p>Evidently, k3d takes the crown in startup performance, boasting significant advantages in image size, startup time, and memory usage. It’s a godsend for those running CI on a shoestring budget.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>If speed and resource efficiency are your top priorities, k3d is a no-brainer. For OS-level isolation tests, minikube’s VM Driver is unbeatable. For everything in between, kind offers a balanced compromise between compatibility and performance.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;As a developer in the cloud-native ecosystem, a common challenge is the need to frequently test applications within a Kubernetes environm</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/en/tags/kubernetes/"/>
    
    <category term="ci" scheme="http://oilbeater.com/en/tags/ci/"/>
    
  </entry>
  
</feed>
