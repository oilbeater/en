<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Oilbeater&#39;s Study Room</title>
  <icon>http://oilbeater.com/en/icon.png</icon>
  
  <link href="http://oilbeater.com/en/atom.xml" rel="self"/>
  
  <link href="http://oilbeater.com/en/"/>
  <updated>2024-08-26T03:18:57.219Z</updated>
  <id>http://oilbeater.com/en/</id>
  
  <author>
    <name>Oilbeater</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AI Gateway Survey: Cloudflare AI Gateway</title>
    <link href="http://oilbeater.com/en/2024/08/26/ai-gateway-cloudflare/"/>
    <id>http://oilbeater.com/en/2024/08/26/ai-gateway-cloudflare/</id>
    <published>2024-08-26T03:03:34.000Z</published>
    <updated>2024-08-26T03:18:57.219Z</updated>
    
    <content type="html"><![CDATA[<p>With the rising popularity of AI, I noticed that many API gateway products rebrand themselves as AI Gateways. This prompted me to explore what these so-called AI Gateways actually offer. My research includes those that started with API management or cloud-native Ingress Controllers and later integrated AI features, such as: <a href="https://konghq.com/products/kong-ai-gateway">Kong</a>, <a href="https://www.solo.io/products/gloo-ai-gateway/">Gloo</a>, and <a href="https://higress.io/en/">Higress</a>. It also covers gateways that were AI-native from the start, like <a href="https://portkey.ai/features/ai-gateway">Portkey</a> and <a href="https://github.com/songquanpeng/one-api">OneAPI</a>, as well as the public cloud Serverless-based <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a> discussed in this blog.</p><p>Generally, current AI Gateways excel in three main areas:</p><p><strong>Standard API gateway features applied to AI APIs</strong> such as monitoring, logging, rate limiting, reverse proxy, request or response rewriting, and user system integration. These features, although crucial, are not specifically AI-related; they treat LLM APIs like any standard API Service.</p><p><strong>API gateway features optimized for AI</strong> include enhancements like token-based rate limiting, prompt-based caching, firewall filtering based on prompts and LLM responses, load balancing across multiple LLM API keys, and API translation among different LLM providers. These functionalities extend existing API gateway concepts for AI scenarios.</p><p><strong>New features added for AI applications</strong> include embedding and RAG capabilities, exposing vector and text database functionalities via APIs. There are also cost optimizations related to token usage, such as prompt simplification and semantic caching. Additionally, some gateways offer application-layer features, like scoring the outputs of LLMs.</p><p>This blog post highlights the features of the <a href="https://developers.cloudflare.com/ai-gateway/">Cloudflare AI Gateway</a>.</p><h1 id="Basic-Principle"><a href="#Basic-Principle" class="headerlink" title="Basic Principle"></a>Basic Principle</h1><p>Cloudflare’s AI Gateway mainly functions as a reverse proxy. After reviewing it, I realized that I could potentially replicate similar functionalities using Cloudflare Worker. If you’re currently using OpenAI’s API, you simply need to change the SDK’s baseURL to <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/openai</code>. Through this setup, Cloudflare can offer monitoring, logging, and caching as traffic goes through their platform.</p><p>This approach has several advantages:</p><ul><li>Easy integration by changing the baseURL, with no change in API format. It is entirely Serverless and free, effectively giving away monitoring capabilities.</li><li>Leveraging Cloudflare’s global network can accelerate user access, though this is minimal compared to the latency of LLMs themselves. The most noticeable speed improvement may be in the latency of the first token response.</li><li>It can obscure the source IP, useful for bypassing regional restrictions on certain OpenAI API accesses.</li></ul><p>However, there are also drawbacks:</p><ul><li>All request data, including API keys, pass through Cloudflare, posing potential security risks.</li><li>The gateway lacks a plugin mechanism, making it challenging to extend functionalities without additional external layers.</li><li>Constantly changing IP addresses through Cloudflare’s network might trigger security measures from OpenAI.</li></ul><h1 id="Key-Features"><a href="#Key-Features" class="headerlink" title="Key Features"></a>Key Features</h1><h2 id="Multi-provider-Support"><a href="#Multi-provider-Support" class="headerlink" title="Multi-provider Support"></a>Multi-provider Support</h2><p>Since Cloudflare AI Gateway simply acts as a reverse proxy without altering the LLM API, it can support almost any mainstream LLM API by changing the baseURL to the format: <code>https://gateway.ai.cloudflare.com/v1/$&#123;accountId&#125;/$&#123;gatewayId&#125;/&#123;provider&#125;</code>.</p><p>It also offers a <a href="https://developers.cloudflare.com/ai-gateway/providers/universal/">Universal Endpoint</a> for simple fallbacks, allowing multiple provider queries in one API request to automatically call the next provider if the previous one fails.</p><h2 id="Observability"><a href="#Observability" class="headerlink" title="Observability"></a>Observability</h2><p>In addition to basic monitoring like QPS and error rates, Cloudflare provides specific dashboards for tokens, costs, and cache hit rates for LLM scenarios.</p><p>The logging is similar to that of Workers, focusing only on real-time logs without historical data access. This limitation makes it difficult for AI applications that rely on analyzing request and response logs for optimizations or fine-tuning.</p><h2 id="Caching"><a href="#Caching" class="headerlink" title="Caching"></a>Caching</h2><p>Cloudflare’s caching is still based on exact text matches, likely implemented using <a href="https://developers.cloudflare.com/kv/">Cloudflare Workers KV</a>. Custom cache keys and settings, including TTL and cache bypass, are possible, but semantic caching is not yet available, although promised for the future.</p><h2 id="Rate-Limiting"><a href="#Rate-Limiting" class="headerlink" title="Rate Limiting"></a>Rate Limiting</h2><p>Cloudflare’s rate limiting still follows traditional QPS-based methods without any AI-specific enhancements, like token-based limiting, which could be improved in the future.</p><h2 id="Custom-Metadata"><a href="#Custom-Metadata" class="headerlink" title="Custom Metadata"></a>Custom Metadata</h2><p>Custom headers can be added to requests, such as user information, retrievable through logging.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Overall, Cloudflare AI Gateway excels in simplicity and ease of use. New users can integrate within minutes, providing essential monitoring and caching capabilities. Despite its straightforward implementation, it lacks depth in more advanced features, and extending functionalities is cumbersome, requiring additional setups with Workers. A potential improvement could be open-sourcing the AI Gateway as a template, allowing users to modify code or create plugins to build a new ecosystem, considering it likely operates similarly to a Worker template.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;With the rising popularity of AI, I noticed that many API gateway products rebrand themselves as AI Gateways. This prompted me to explore</summary>
      
    
    
    
    
    <category term="AI" scheme="http://oilbeater.com/en/tags/AI/"/>
    
    <category term="Gateway" scheme="http://oilbeater.com/en/tags/Gateway/"/>
    
    <category term="Cloudflare" scheme="http://oilbeater.com/en/tags/Cloudflare/"/>
    
  </entry>
  
  <entry>
    <title>5 Solutions for Multi-Cluster Communication in Kubernetes</title>
    <link href="http://oilbeater.com/en/2024/05/24/five-kubernetes-multicluster-network/"/>
    <id>http://oilbeater.com/en/2024/05/24/five-kubernetes-multicluster-network/</id>
    <published>2024-05-24T08:25:30.000Z</published>
    <updated>2024-08-26T03:18:57.219Z</updated>
    
    <content type="html"><![CDATA[<p>As enterprises scale their operations, the use of Kubernetes extends from single-cluster to multi-cluster deployments. In a multi-cluster environment, communication between clusters becomes a critical area of research. This article introduces the basic principles, advantages, and limitations of five solutions for cross-Kubernetes cluster communication.</p><h2 id="1-Underlay-Network"><a href="#1-Underlay-Network" class="headerlink" title="1. Underlay Network"></a>1. Underlay Network</h2><p>This type of network plugin includes <a href="https://www.cni.dev/plugins/current/main/macvlan/">macvlan</a>, <a href="https://www.cni.dev/plugins/current/main/ipvlan/">ipvlan</a>, <a href="https://kubeovn.github.io/docs/stable/start/underlay/">Kube-OVN underlay</a>, and various cloud VPC CNIs.</p><p><strong>Basic Principle</strong>:</p><p>From the CNI (Container Network Interface) perspective, the underlay network is the simplest approach. This method relies on the underlying infrastructure to achieve connectivity at the network level, such as using VPC Peering on public clouds or configuring routing and large Layer 2 networks in physical networks. Once the underlying network is connected, cross-cluster container networks naturally connect.</p><p><strong>Advantages</strong>:</p><ul><li>Simplest from a CNI perspective, requiring no additional operations.</li><li>Architecturally clear, delegating the responsibility of cross-cluster communication to the underlying network.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Depends on specific CNI; underlay networks have limited use cases, and some scenarios can only use overlay networks.</li><li>Difficulties in heterogeneous environments, such as between multiple clouds or between public and private clouds.</li><li>Provides only basic container network communication without higher-level functionalities like service discovery, domain names, and network policies.</li><li>Lacks fine-grained control by connecting all cluster container networks at once.</li></ul><h2 id="2-Overlay-CNI-Providing-Cross-Cluster-Communication"><a href="#2-Overlay-CNI-Providing-Cross-Cluster-Communication" class="headerlink" title="2. Overlay CNI Providing Cross-Cluster Communication"></a>2. Overlay CNI Providing Cross-Cluster Communication</h2><p>When underlay networks cannot be used, some specific CNIs achieve cross-cluster communication at the overlay level, such as <a href="https://cilium.io/use-cases/cluster-mesh/">Cilium Cluster Mesh</a>, <a href="https://antrea.io/docs/v2.0.0/docs/multicluster/quick-start/">Antrea Multi-Cluster</a>, and <a href="https://kubeovn.github.io/docs/stable/en/advance/with-ovn-ic/">Kube-OVN with ovn-ic</a>.</p><p><strong>Basic Principle</strong>:</p><p>These CNIs typically select a set of nodes within the cluster as gateway nodes, which then establish tunnels between each other. Cross-cluster traffic is forwarded through these gateway nodes.</p><p><strong>Advantages</strong>:</p><ul><li>CNIs include cross-cluster functionality without needing additional components.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Dependent on specific CNIs, cannot achieve communication between different CNIs.</li><li>Cannot handle CIDR overlap; network segments need to be pre-planned.</li><li>Except for Cilium, which implements a complete solution including cross-cluster service discovery and network policies, others only provide basic container network communication.</li><li>Lacks fine-grained control by connecting all cluster container networks at once.</li></ul><h2 id="3-Submariner"><a href="#3-Submariner" class="headerlink" title="3. Submariner"></a>3. Submariner</h2><p>Given the common need for cross-cluster network interconnectivity, similar implementations lead to duplicated efforts among various CNIs. <a href="https://submariner.io/">Submariner</a>, an independent, cross-cluster network plugin, offers a generic solution capable of connecting clusters with different CNIs into one network. Initially created by engineers at Rancher and now a CNCF Sandbox project with active participation from Red Hat engineers.</p><p><strong>Basic Principle</strong>:</p><p>Submariner selects gateway nodes within the cluster that communicate via VXLAN. Cross-cluster traffic is transmitted through VXLAN. Submariner relies on the CNI to send egress traffic to the host network, which then forwards it. Additionally, Submariner deploys a set of CoreDNS for cross-cluster service discovery and uses a Globalnet Controller to address CIDR overlap issues.</p><p><strong>Advantages</strong>:</p><ul><li>CNI-agnostic to some extent, allowing the connection of clusters with different CNIs.</li><li>Implements cross-cluster service discovery, supporting service and domain name resolution.</li><li>Supports communication between clusters with overlapping CIDRs, avoiding IP conflicts post-deployment.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Not compatible with all CNIs, especially those like macvlan or short-circuiting Cilium where the host cannot see the traffic.</li><li>Gateway operates in active-passive mode, lacking horizontal load balancing, potentially causing performance bottlenecks in high-traffic scenarios.</li><li>Lacks fine-grained control by connecting all cluster container networks at once.</li></ul><h2 id="4-Skupper"><a href="#4-Skupper" class="headerlink" title="4. Skupper"></a>4. Skupper</h2><p><a href="https://skupper.io/index.html">Skupper</a> is considered the most interesting among the solutions, enabling service-layer network connectivity on demand, avoiding the control issues of complete connectivity. It innovatively uses a layer 7 message queue to achieve this, being entirely independent of the underlying network and CNI, making it very easy to get started. Currently, most contributors are engineers from Red Hat.</p><p><strong>Basic Principle</strong>:</p><p>Unlike the previous solutions that use tunnels to connect container IPs directly, Skupper introduces the concept of VAN (Virtual Application Networks) to connect networks at layer 7. Simply put, instead of directly connecting IPs, it connects services. Conceptually similar to ServiceExporter and ServiceImporter, but predating these community concepts, it was quite innovative at its inception.</p><p>Skupper uses a message queue implementation, forming a large message queue between multiple clusters. Cross-cluster communication packets are sent to this message queue and consumed on the other end. This approach is similar to reverse proxy but implemented with a message queue, making it a very novel idea. Services become message subscription points for consumption, allowing the establishment of a message queue between the server and client as needed, managing and controlling this message path through the message queue concept.</p><p><strong>Advantages</strong>:</p><ul><li>Excellent CNI compatibility, completely independent of CNI behavior at the application layer for packet connectivity.</li><li>Easy to get started, requiring no complex upfront network planning and no CIDR non-overlap requirements. Provides CLI for temporary testing and quick demonstrations.</li><li>Connects services on demand instead of entire container networks, allowing fine-grained control with lower infrastructure requirements.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Currently supports only TCP protocol; UDP and lower-level protocols like ICMP have issues.</li><li>IP information is lost as messages are forwarded through the message queue.</li><li>Using a message queue for forwarding might lead to performance issues like latency and throughput loss.</li><li>Given TCP’s stateful nature, converting it entirely to a message queue form and ensuring compatibility is questionable.</li></ul><h2 id="5-KubeSlice"><a href="#5-KubeSlice" class="headerlink" title="5. KubeSlice"></a>5. KubeSlice</h2><p><a href="https://kubeslice.io/documentation/open-source/1.3.0">KubeSlice</a> is a project that recently entered the CNCF Sandbox. Solutions based on tunnels cannot achieve fine-grained control and full CNI compatibility, while those based on the application layer cannot fully support network protocols. KubeSlice offers a new approach, attempting to solve both issues simultaneously.</p><p><strong>Basic Principle</strong>:</p><p>KubeSlice’s basic idea is simple and straightforward: dynamically insert a network card into a pod as needed, creating an overlay network for cross-cluster communication on this card. This overlay network then implements service discovery and network policies. Users can dynamically create and join networks across namespaces or pods, achieving flexible fine-grained control. Since it operates on a second network card, it is highly compatible with network protocols.</p><p><strong>Advantages</strong>:</p><ul><li>High CNI compatibility, as it involves adding an extra network card, regardless of the original CNI and avoiding original network address conflicts.</li><li>High protocol compatibility, as it uses an additional network card for traffic forwarding, compatible with all network protocols.</li><li>High flexibility, providing CLI tools for dynamically creating and joining networks, allowing users to create multiple cross-cluster virtual networks as needed.</li><li>Comprehensive functionality, implementing service discovery, DNS, QoS, NetworkPolicy, and monitoring on the additional overlay network.</li></ul><p><strong>Limitations</strong>:</p><ul><li>Applications need to be aware of the additional network card and choose the appropriate network, potentially requiring application modifications.</li><li>Since cross-cluster traffic uses a different network card, not the pod’s primary IP, external systems like monitoring and tracing might require adjustments.</li><li>Documentation appears to be an internal project made open source, with many usage methods and API descriptions not well explained, requiring users to guess parameter meanings from references. Despite seeming comprehensive, the documentation needs significant improvement for external users to use it effectively.</li></ul><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>In a multi-cluster Kubernetes environment, there are multiple solutions for efficient communication. Each solution has its advantages and limitations, allowing users to choose the most suitable one based on specific needs and environments.</p><blockquote><p>This post was originally written in Chinese and I translated it to English with the help of GPT4. If you find any errors, please feel free to let me know.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;As enterprises scale their operations, the use of Kubernetes extends from single-cluster to multi-cluster deployments. In a multi-cluster</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/en/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/en/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>k8gb: The Best Open Source GSLB Solution for Cloud Native</title>
    <link href="http://oilbeater.com/en/2024/04/18/k8gb-best-cloudnative-gslb/"/>
    <id>http://oilbeater.com/en/2024/04/18/k8gb-best-cloudnative-gslb/</id>
    <published>2024-04-18T10:01:10.000Z</published>
    <updated>2024-08-26T03:18:57.219Z</updated>
    
    <content type="html"><![CDATA[<p>Balancing traffic across multiple Kubernetes clusters and achieving automatic disaster recovery switching has always been a headache. We have explored public clouds and <a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">Karmada Ingress</a>, and have also tried manual DNS solutions, but these approaches often fell short in terms of cost, universality, flexibility, and automation. It was not until we discovered <a href="https://www.k8gb.io/">k8gb</a>, a project initiated by South Africa’s Absa Group to provide banking-level multi-availability, that we realized the ingenuity of using various DNS protocols to deliver a universal and highly automated GSLB solution. This blog will briefly discuss the problems with other approaches and how k8gb cleverly uses DNS to implement GSLB.</p><h2 id="What-is-GSLB"><a href="#What-is-GSLB" class="headerlink" title="What is GSLB"></a>What is GSLB</h2><p>GSLB (Global Service Load Balancer) is a concept in contrast to single-cluster load balancing, which mainly serves as an entrance to a cluster to distribute traffic within the cluster, whereas GSLB typically acts as an entrance for traffic across multiple clusters, handling load balancing and fault management. On one hand, GSLB can set geographic affinity rules to route traffic closer to users, enhancing overall performance; on the other hand, it can automatically redirect traffic to a functioning cluster when one fails, minimizing the impact on users.</p><h2 id="Problems-with-Other-Solutions"><a href="#Problems-with-Other-Solutions" class="headerlink" title="Problems with Other Solutions"></a>Problems with Other Solutions</h2><h3 id="Commercial-Load-Balancers"><a href="#Commercial-Load-Balancers" class="headerlink" title="Commercial Load Balancers"></a>Commercial Load Balancers</h3><p>GSLB is not a new concept, so many commercial companies have mature products, such as <a href="https://www.f5.com/solutions/use-cases/global-server-load-balancing-gslb">F5 GSLB</a>. These products generally have the following drawbacks:</p><ol><li>They do not integrate well with cloud-native systems, often requiring deployment outside Kubernetes clusters, which complicates unified management.</li><li>They are costly and pose risks of vendor lock-in.</li></ol><h3 id="Public-Cloud-Global-Load-Balancers"><a href="#Public-Cloud-Global-Load-Balancers" class="headerlink" title="Public Cloud Global Load Balancers"></a>Public Cloud Global Load Balancers</h3><p>Public clouds offer multi-cluster load balancing products to solve the issue of traffic distribution across multiple regions, such as AWS’s <a href="https://aws.amazon.com/global-accelerator/">Global Accelerator</a> and GCP’s <a href="https://cloud.google.com/load-balancing/docs/https">External Application Load Balancer</a>. GCP even offers custom <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/multi-cluster-ingress">Multi Cluster Ingress</a> resources that integrate well with Kubernetes Ingress. However, they also have several problems:</p><ol><li>Although they support multi-cluster load balancing, all clusters must be within the same public cloud, preventing inter-cloud traffic scheduling.</li><li>They are not usable in private clouds.</li></ol><h3 id="Karmada-Multi-Cluster-Ingress"><a href="#Karmada-Multi-Cluster-Ingress" class="headerlink" title="Karmada Multi Cluster Ingress"></a>Karmada Multi Cluster Ingress</h3><p>Karmada is a multi-cluster orchestration tool that also provides its own multi-cluster traffic routing solution through <a href="https://karmada.io/docs/userguide/service/multi-cluster-ingress/">Karmada Multi Cluster Ingress</a>. This solution involves deploying an ingress-nginx provided by the Karmada community and defining <code>MultiClusterIngress</code> within a cluster, but it has several issues:</p><ol><li>It relies on inter-cluster container network connectivity and CRDs like ServiceImporter and ServiceExporter, which have high requirements.</li><li>It requires additional management of the ingress-nginx instance that provides GSLB services, including its deployment location, quantity, and distribution, which are operational considerations.</li><li>The <a href="https://github.com/karmada-io/multi-cluster-ingress-nginx">multi-cluster-ingress-nginx</a> modified by the community has had little code submission in recent years, raising concerns about its reliability.</li></ol><h3 id="Simple-DNS-Approach"><a href="#Simple-DNS-Approach" class="headerlink" title="Simple DNS Approach"></a>Simple DNS Approach</h3><p>A simple, manually configured DNS-based solution can work initially. Most DNS providers offer health check features, allowing us to add multiple cluster exit IP addresses to DNS records and configure health checks for fault switching. However, this simple approach has clear limitations as it scales:</p><ol><li>It lacks good automation; a single cluster might have multiple domain names and Ingress IP combinations, and manually managing their mappings becomes increasingly challenging with scale.</li><li>DNS providers’ health checks are typically based on TCP and ICMP, so they can detect and switch during complete cluster outages. However, partial failures are undetectable, e.g., when multiple Ingresses share an ingress-controller and route traffic by domain name. If the backend instances of one service fail, but other services on the ingress-controller operate normally, the health check will still pass, preventing traffic from being redirected to another cluster.</li><li>DNS inherently involves caching at various levels, which can delay updates.</li><li>DNS health checks are a capability provided by the vendor and are not guaranteed to be available from all vendors, especially in private cloud scenarios.</li></ol><h2 id="k8gb’s-Solution"><a href="#k8gb’s-Solution" class="headerlink" title="k8gb’s Solution"></a>k8gb’s Solution</h2><p>k8gb’s solution also utilizes DNS but addresses the deficiencies of the simple DNS approach through a series</p><p> of clever designs.</p><p>The essence of the simple DNS approach’s problem is that it does not integrate well with Kubernetes. Dynamic information within Kubernetes, such as new Ingress additions, new domain names, and service health status, cannot be effectively synced to upstream DNS servers. Upstream DNS servers’ simple health checks are also inadequate for handling the complex changes within Kubernetes. Thus, a core change in k8gb is that the upstream DNS records are no longer A or AAAA records pointing to a cluster’s exit address but are forwarded to a CoreDNS configured within the cluster for DNS resolution, pushing the complex DNS logic down into the cluster for internal control. In this way, the upstream DNS only needs to serve as a simple proxy without needing to configure health checks or dynamically adjust multiple address mappings.</p><p>The user’s DNS request process is illustrated below:</p><ul><li><img src="https://www.k8gb.io/docs/images/gslb-basic-multi-cluster.svg" alt="K8GB multi-cluster interoperability"></li></ul><ol><li>The user requests a domain name’s IP record from an external DNS provider.</li><li>The external DNS forwards this request to a CoreDNS managed by k8gb within the cluster.</li><li>k8gb analyzes available Ingress IPs based on the Ingress Spec’s domain name, the Ingress Status’s Ingress IP, and the health status of backend pods within the cluster, as well as load balancing policies, and returns a usable Ingress IP to the user.</li><li>The user can then directly access the corresponding Ingress Controller via this IP.</li><li>If a k8gb-managed CoreDNS in one cluster fails, since the upstream DNS concurrently proxies DNS requests to multiple clusters, another cluster can also return its Ingress IP, allowing the user to choose from multiple available IPs.</li></ol><p>This method requires administrators to register several domain name suffixes with the upstream DNS and proxy them to each cluster’s CoreDNS. k8gb also provides automation capabilities; once certificates are properly configured, it can automatically analyze the domain names used in Ingress and automatically register them with the upstream DNS, significantly simplifying administrators’ tasks.</p><p>Another important aspect to note is that each cluster’s CoreDNS must not only record its Ingress IPs but also those of other clusters for the same Ingress. If a cluster’s corresponding service’s pods are Not Ready, CoreDNS will return NXDomain. If the client receives this response, it will treat it as if the domain cannot be resolved, even though another cluster’s service might still be operational. Therefore, k8gb must also sync all clusters’ Ingress IPs for the same domain name and their health status.</p><p>As is well known, syncing data across multiple clusters is a global challenge, but k8gb cleverly achieves data syncing through DNS.</p><p>The syncing process is illustrated below:</p><p><img src="https://www.k8gb.io/docs/images/k8gb-multi-cluster-interoperabililty.svg" alt="k8gb multi-cluster interoperability"></p><p>The general idea is that each cluster’s k8gb registers its CoreDNS’s Ingress IP with the upstream CoreDNS, allowing each cluster to directly access another cluster’s CoreDNS. Then, each cluster’s CoreDNS maintains its own <code>localtargets-app.cloud.example.com</code> Ingress IP and health status for <code>app.cloud.example.com</code>, allowing each cluster to obtain other clusters’ Ingress IPs for the same domain name and include them in their return results, achieving multi-cluster domain name resolution syncing.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>k8gb, as an open-source GSLB, achieves seamless integration with Kubernetes, enabling effective cross-cluster domain name and traffic management with minimal external dependencies—only requiring an API to add DNS records. Although this project is not yet widely popular, it is already the best solution in this field in my view.</p><blockquote><p>This post was originally written in Chinese and I translated it to English with the help of GPT4. If you find any errors, please feel free to let me know.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Balancing traffic across multiple Kubernetes clusters and achieving automatic disaster recovery switching has always been a headache. We </summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/en/tags/kubernetes/"/>
    
    <category term="network" scheme="http://oilbeater.com/en/tags/network/"/>
    
  </entry>
  
  <entry>
    <title>How to Easily Integrate AI-Generated Content into Logseq Notes</title>
    <link href="http://oilbeater.com/en/2024/03/21/ai-content-to-logseq/"/>
    <id>http://oilbeater.com/en/2024/03/21/ai-content-to-logseq/</id>
    <published>2024-03-21T10:39:37.000Z</published>
    <updated>2024-08-26T03:18:57.219Z</updated>
    
    <content type="html"><![CDATA[<p>My daily life has already incorporated a lot of assistance from AI, such as translating text, explaining a term, answering a question, etc. However, after frequent use, I encountered another problem: these AI outputs are beneficial supplements to my knowledge, but the use is usually one-off. I don’t settle these contents down. It is inconvenient when I need to collect them into notes because I have to organize them again. Imagine if this temporary query information could be automatically saved to my note-taking system, there would no longer be a worry about losing important knowledge snippets. Thus, I came up with the idea of creating a workflow to automatically store these AI-generated contents in my note system, and then review them periodically. Here, I will discuss the limitations of some tools I researched and how I used a simple framework to automatically import AI-generated content into logseq and make it into flashcards.</p><h1 id="Problem-Decomposition-and-Preliminary-Research"><a href="#Problem-Decomposition-and-Preliminary-Research" class="headerlink" title="Problem Decomposition and Preliminary Research"></a>Problem Decomposition and Preliminary Research</h1><p>Here, I take a daily translation scenario as an example: When encountering a sentence that I don’t understand, I wish to highlight and translate it, then have AI analyze the difficult words in the sentence, and finally save these contents as flashcards format in logseq. This way, with the synchronization on the mobile side, I can review this sentence during fragmented time. So, the problem essentially consists of three parts: 1. Extracting the highlighted text 2. Writing a prompt to generate the content I want 3. Saving it to logseq.</p><p>The first two steps have been integrated into many tools, including Raycast AI and OpenAI-translator, but none of them offer an extension interface to export the results of the first two steps to a third party. Raycast could theoretically accomplish this with its plugin system, but I’m not very familiar with JS and also don’t want to rely too much on Raycast AI, as I plan to discontinue the subscription. So, I decided to write a python script to do these things and then directly call the script with Raycast’s script command and shortcut keys, theoretically completing the entire workflow with one shortcut key.</p><h1 id="How-to-Obtain-Selected-Text"><a href="#How-to-Obtain-Selected-Text" class="headerlink" title="How to Obtain Selected Text"></a>How to Obtain Selected Text</h1><p>The first challenge is obtaining the selected text, which seems to be an operation at the system level and is not easy to implement. It was not until I saw the implementation of a dictionary software that I had an epiphany:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://github.com/raycast/script-commands/blob/master/commands/apps/dictionary/look-up-in-dictionary.applescript</span></span><br><span class="line">tell application <span class="string">&quot;System Events&quot;</span></span><br><span class="line">    keystroke <span class="string">&quot;c&quot;</span> using &#123;<span class="built_in">command</span> down&#125;</span><br><span class="line">    delay 0.1</span><br><span class="line">    <span class="keyword">do</span> shell script <span class="string">&quot;open dict://&quot;</span> &amp; the clipboard</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>This script essentially simulates pressing <code>cmd + v</code> on the keyboard, then directly reads from the clipboard, bypassing the issue of obtaining selected text. Considering that I often encounter recognition errors of selected text when using Raycast, I’ve developed the habit of using <code>cmd + v</code>, so there’s no need to find other methods for obtaining selected text; directly reading the clipboard will suffice.</p><h1 id="Calling-AI"><a href="#Calling-AI" class="headerlink" title="Calling AI"></a>Calling AI</h1><p>This part is pretty straightforward. The key is setting an appropriate prompt, translating while generating a teaching guide for a sentence, and outputting as much as possible in the format of logseq:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance, choose only the top 3, and output them with an explanation of their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h1 id="Output-to-Logseq"><a href="#Output-to-Logseq" class="headerlink" title="Output to Logseq"></a>Output to Logseq</h1><p>When it came to outputting to logseq, I initially planned to call Logseq’s API, which provides a local HTTP service that can be called through HTTP in developer mode. However, when I looked at their API documentation, I was almost overwhelmed. All operations required an ID, and there was no direct way to index page IDs; it was necessary to getAll and then filter. appendBlock did not allow inserting blocks with hierarchy, requiring several API calls to complete a simple operation.</p><p>When I was about to give up, I realized that Logseq is essentially a renderer for markdown files. Since the API was difficult to use, I decided to directly write files, thus making a series of complicated API calls into enjoyable file append operations. This not only circumvented the limitations of the Logseq API but also potentially allowed integration with other markdown-based note systems.</p><p>The final code is as follows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyperclip</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> AzureOpenAI</span><br><span class="line"></span><br><span class="line">content = pyperclip.paste()</span><br><span class="line"></span><br><span class="line">azure_endpoint = <span class="string">&quot;YOUR AZURE ENDPOINT&quot;</span></span><br><span class="line">api_key = <span class="string">&quot;YOUR API KEY&quot;</span></span><br><span class="line">model = <span class="string">&quot;YOUR MODEL NAME&quot;</span></span><br><span class="line">logseq_path = <span class="string">&quot;YOUR LOGSEQ PAGE FILE PATH&quot;</span></span><br><span class="line"></span><br><span class="line">client = AzureOpenAI(</span><br><span class="line">    azure_endpoint=azure_endpoint,</span><br><span class="line">    api_key=api_key,</span><br><span class="line">    api_version=<span class="string">&quot;2024-02-15-preview&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">message_text = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a university English teacher, below is a paragraph in English. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Please first translate it into Chinese. Then extract difficult words and phrases from the source paragraph, sort them in descending order of importance, choose only the top 3, and explain their usage to me in detail from a linguistic perspective.&quot;</span> +</span><br><span class="line">        <span class="string">&quot;The overall output should look like this: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- &#123;The Chinese Translation&#125; \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;- Explanation: \n&quot;</span> +</span><br><span class="line">        <span class="string">&quot;  - &#123;word or phrase&#125;: &#123;explanation&#125;\n&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;role&quot;</span>:<span class="string">&quot;user&quot;</span>,</span><br><span class="line">        <span class="string">&quot;content&quot;</span>: content</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">completion = client.chat.completions.create(</span><br><span class="line">    model=model,  <span class="comment"># model = &quot;deployment_name&quot;</span></span><br><span class="line">    messages=message_text,</span><br><span class="line">    temperature=<span class="number">0.7</span>,</span><br><span class="line">    max_tokens=<span class="number">500</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    frequency_penalty=<span class="number">0</span>,</span><br><span class="line">    presence_penalty=<span class="number">0</span>,</span><br><span class="line">    stop=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> completion.choices[<span class="number">0</span>].message.content <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;No response&quot;</span>)</span><br><span class="line">    exit()</span><br><span class="line"></span><br><span class="line">response = completion.choices[<span class="number">0</span>].message.content</span><br><span class="line"><span class="built_in">print</span>(response)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(logseq_path, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&quot;\n- &quot;</span> + content.rstrip() + <span class="string">&quot; #card #English&quot;</span> + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> response.splitlines():</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">&quot;```&quot;</span>):</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> line.rstrip() == <span class="string">&quot;&quot;</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line.startswith(<span class="string">&quot;- &quot;</span>):</span><br><span class="line">            line = <span class="string">&quot;- &quot;</span> + line.rstrip()</span><br><span class="line">        f.write(<span class="string">&quot;  &quot;</span> + line + <span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure><p>The remaining task is to set a shortcut for this script in Raycast, so the next time I encounter a sentence I don’t understand, I can select, copy, and complete the translation, difficulty extraction, and flashcard generation workflow with a shortcut key.</p><p>The final flashcard effect is as follows:</p><p><img src="/en/../../images/logseq-card.png" alt="alt text"></p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>By integrating AI-generated content into Logseq notes automatically, we not only enhance the efficiency of information management but also optimize the learning and workflow process. With a simple script, anyone can easily integrate this powerful technology into their daily life, enabling the accumulation and review of knowledge.</p><blockquote><p>This post was originally written in Chinese and I translated it to English with the help of GPT4. If you find any errors, please feel free to let me know.</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;My daily life has already incorporated a lot of assistance from AI, such as translating text, explaining a term, answering a question, et</summary>
      
    
    
    
    
    <category term="logseq" scheme="http://oilbeater.com/en/tags/logseq/"/>
    
    <category term="ai" scheme="http://oilbeater.com/en/tags/ai/"/>
    
    <category term="learning" scheme="http://oilbeater.com/en/tags/learning/"/>
    
  </entry>
  
  <entry>
    <title>Automatic Shutdown Server under VS Code Remote SSH</title>
    <link href="http://oilbeater.com/en/2024/03/13/shutdown-remote-ssh-server/"/>
    <id>http://oilbeater.com/en/2024/03/13/shutdown-remote-ssh-server/</id>
    <published>2024-03-13T10:09:26.000Z</published>
    <updated>2024-08-26T03:18:57.219Z</updated>
    
    <content type="html"><![CDATA[<p>I have migrated all my development environments to GCP’s Spot Instance and connect using the VS Code Remote SSH plugin. The advantage is that GCP charges by the second, making it cost-effective as long as you shut down promptly, even with high-spec machines. The downside is that forgetting to shut down during holidays can lead to high costs. After losing more than 10 dollars, I decided to find a way to automatically shut down the machine when VS Code is idle. This involved some tricky maneuvers to execute the shutdown command within a Docker container.</p><h1 id="How-to-Determine-Idle-State"><a href="#How-to-Determine-Idle-State" class="headerlink" title="How to Determine Idle State"></a>How to Determine Idle State</h1><p>This feature is similar to the idle timeout in CodeSpace, but the Remote SSH plugin doesn’t expose this functionality, so I had to implement it myself. The main challenge was determining idle state on the server side. I couldn’t find an exposed interface in VS Code, so I wondered if there was a simple way to determine idle state outside of VS Code.</p><p>The most straightforward idea was to monitor SSH connections, as Remote SSH connects via SSH. If there are no active SSH connections on the machine, it can be considered idle and shut down. However, the connection timeout for Remote SSH is quite long, reportedly four hours, and even closing the VS Code client didn’t terminate the server-side SSH connection. Setting a timeout on the server side would lead to quick client reconnection, so the number of connections wouldn’t decrease.</p><p>Since directly monitoring the number of SSH connections wasn’t feasible, I looked into whether it was possible to determine if no traffic under existing SSH connections. Using <code>tcpdump</code>, I found that there were TCP heartbeat packets every second, even without client interaction, so no traffic wasn’t a valid indicator. However, the size of these heartbeat packets was consistently 44 bytes, which could be used to identify idle state—if there were no packets larger than 44 bytes on the SSH port for a certain period, it was considered idle.</p><p>Here’s the first version of the script:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> [ -f /root/dump ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">timeout</span> 1m tcpdump -nn -i ens4 tcp and port 22 and greater 50 -w /root/dump</span><br><span class="line"></span><br><span class="line">  line_count=$(<span class="built_in">wc</span> -l &lt; /root/dump)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> [ <span class="variable">$line_count</span> -gt 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">rm</span> /root/dump</span><br><span class="line">    shutdown -h now</span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>This script achieves the functionality of shutting down the machine if there are no non-heartbeat packets on the SSH port within one minute. The next step was to automate this script’s execution.</p><h1 id="Docker-Dark-Magic"><a href="#Docker-Dark-Magic" class="headerlink" title="Docker Dark Magic"></a>Docker Dark Magic</h1><p>When packaging the script into a Docker image, I encountered an interesting issue: none of the base images contained the <code>shutdown</code> command, nor could it be easily installed. To execute the <code>shutdown</code> command on the host, it was necessary to switch to the host’s namespace from within the Docker container:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsenter -a -t 1 shutdown -h now</span><br></pre></td></tr></table></figure><p>The <code>nsenter</code> command targets the process with Pid 1 and enters all its namespaces (pid, mount, network), making it appear as if operating directly on the host when Docker runs in the shared host Pid mode. To run the container with this capability, use the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name=close --pid=host --network=host --privileged --restart=always -d close:v0.0.1</span><br></pre></td></tr></table></figure><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>By monitoring SSH traffic, we can infer if VS Code is idle and then use some Docker “dark magic” to achieve automatic shutdown. However, this method is quite complex; is there a simpler way?</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;I have migrated all my development environments to GCP’s Spot Instance and connect using the VS Code Remote SSH plugin. The advantage is </summary>
      
    
    
    
    
    <category term="vscode" scheme="http://oilbeater.com/en/tags/vscode/"/>
    
    <category term="tools" scheme="http://oilbeater.com/en/tags/tools/"/>
    
    <category term="docker" scheme="http://oilbeater.com/en/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>The Impact of Preallocating Slice Memory in Golang (Continued)</title>
    <link href="http://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/"/>
    <id>http://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/</id>
    <published>2024-03-04T18:28:09.000Z</published>
    <updated>2024-08-26T03:18:57.219Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#basic-performance-tests">Basic Performance Tests</a></li><li><a href="#appending-an-entire-slice">Appending an Entire Slice</a></li><li><a href="#reusing-slices">Reusing Slices</a></li><li><a href="#syncpool">sync.Pool</a></li><li><a href="#bytebufferpool">bytebufferpool</a></li><li><a href="#conclusion">Conclusion</a></li></ul><p>I previously wrote about <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance/">The Impact of Preallocating Slice Memory in Golang</a>, discussing the performance effects of preallocating memory in Slices. The scenarios considered were relatively simple, and recently, I conducted further tests to provide more information, including the impact of appending an entire Slice and the use of sync.Pool and bytebufferpool on performance.</p><h1 id="Basic-Performance-Tests"><a href="#Basic-Performance-Tests" class="headerlink" title="Basic Performance Tests"></a>Basic Performance Tests</h1><p>The initial Benchmark code only considered whether the Slice was preallocated with space. The specific code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">&quot;sync&quot;</span></span><br><span class="line">    <span class="string">&quot;testing&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1024</span></span><br><span class="line"><span class="keyword">var</span> testtext = <span class="built_in">make</span>([]<span class="type">byte</span>, length, length)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocateByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">            init = <span class="built_in">append</span>(init, testtext[j])</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>It’s clear that not preallocating resulted in 8 additional memory allocations, and roughly, 40% of the time was spent on these extra 8 memory allocations.</p><p>These two test cases use a loop to append elements one by one, but the performance gap is not evident when appending an entire Slice. Moreover, in these two test cases, we can’t determine the proportion of time consumed by memory allocation.</p><h1 id="Appending-an-Entire-Slice"><a href="#Appending-an-Entire-Slice" class="headerlink" title="Appending an Entire Slice"></a>Appending an Entire Slice</h1><p>Therefore, two test cases for appending an entire Slice were added to observe whether preallocating memory still has a significant impact on performance. The new case code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">        <span class="keyword">var</span> init []<span class="type">byte</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        569978              2151 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          804807              1304 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3829890               311.5 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3968048               306.7 ns/op          1024 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>Both cases only involved one memory allocation, with almost identical time consumption, significantly lower than appending elements one by one. On the one hand, appending an entire Slice knows the final size at the time of Slice expansion, eliminating the need for dynamic memory allocation and reducing overhead. On the other hand, appending an entire Slice involves block copying, reducing loop overhead and significantly improving performance.</p><p>However, there is still one memory allocation in each case, and we cannot determine the proportion of time it consumes.</p><h1 id="Reusing-Slices"><a href="#Reusing-Slices" class="headerlink" title="Reusing Slices"></a>Reusing Slices</h1><p>To calculate the cost of a single memory allocation, we designed a new test case, placing the Slice’s creation outside the loop and setting the Slice’s length to 0 at the end of</p><p> each loop iteration for reuse. This way, only one memory allocation is made over many tests, which can be considered negligible. The specific code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate2</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">    b.ResetTimer()</span><br><span class="line">    init := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">        <span class="comment">// Preallocate our initial slice</span></span><br><span class="line">        init = <span class="built_in">append</span>(init, testtext...)</span><br><span class="line">        init = init[:<span class="number">0</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        514904              2171 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          761772              1333 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                4041459               320.9 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3854649               320.1 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                63147178                18.63 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>This time, the test showed no memory allocation, and the overall time consumed dropped to 5% of the previous cases. Thus, we can roughly calculate that each memory allocation consumed 95% of the time in the previous test cases, a staggering proportion. Therefore, for performance-sensitive scenarios, it’s essential to reuse objects as much as possible to avoid the overhead of repeated object creation.</p><h1 id="sync-Pool"><a href="#sync-Pool" class="headerlink" title="sync.Pool"></a>sync.Pool</h1><p>In simple scenarios, one can manually clear the Slice and reuse it within the loop, as in the previous test case. However, in real scenarios, object creation often occurs in various parts of the code, necessitating unified management and reuse. Golang’s <code>sync.Pool</code> serves this purpose, and its use is quite simple. However, its internal implementation is complex, with a lot of lock-free design for performance. For more details on its implementation, please see <a href="https://unskilled.blog/posts/lets-dive-a-tour-of-sync.pool-internals/">Let’s dive: a tour of sync.Pool internals</a>.</p><p>The redesigned test case using <code>sync.Pool</code> is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sPool = &amp;sync.Pool&#123; </span><br><span class="line">        New: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> any &#123;</span><br><span class="line">                b := <span class="built_in">make</span>([]<span class="type">byte</span>, <span class="number">0</span>, length)</span><br><span class="line">                <span class="keyword">return</span> &amp;b</span><br><span class="line">        &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPoolByElement</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                b := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := *b</span><br><span class="line">                <span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">                        buf = <span class="built_in">append</span>(buf, testtext[j])</span><br><span class="line">                &#125;</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(b)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPool</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">        b.ResetTimer()</span><br><span class="line">        <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line">                <span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line">                bufPtr := sPool.Get().(*[]<span class="type">byte</span>)</span><br><span class="line">                buf := * bufPtr</span><br><span class="line">                buf = <span class="built_in">append</span>(buf, testtext...)</span><br><span class="line">                buf = buf[:<span class="number">0</span>]</span><br><span class="line">                sPool.Put(bufPtr)</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Here, <code>New</code> provides a constructor function for <code>sync.Pool</code> to create an object when none is available. To use it, the <code>Get</code> method retrieves an object from the Pool, and after use, the <code>Put</code> method returns the object to <code>sync.Pool</code>. It’s important to manage the object’s lifecycle and clear it before returning it to <code>sync.Pool</code> to avoid dirty data. The test results are as follows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocateByElement-12        469431              2313 ns/op            3320 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocateByElement-12          802392              1339 ns/op            1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPoolByElement-12                1212828               961.5 ns/op             0 B/op          0 allocs/op</span><br><span class="line">BenchmarkNoPreallocate-12                3249004               370.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12                  3268851               368.2 ns/op          1024 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate2-12                62596077                18.63 ns/op            0 B/op          0 allocs/op</span><br><span class="line">BenchmarkPool-12                        32707296                35.59 ns/op            0 B/op          0 allocs/op</span><br></pre></td></tr></table></figure><p>As can be seen, using <code>sync.Pool</code> can also avoid memory allocation. Since <code>sync.Pool</code> also has some additional processing performance overhead compared to manually reusing Slice, it is slightly higher. However, considering the convenience of use and the significant performance improvement compared to not using it, it is still a good solution.</p><p>However, directly using <code>sync.Pool</code> also has two issues:</p><ol><li>For Slices, the initial memory allocated by <code>New</code> is fixed. If the runtime usage exceeds this, there may still be a lot of dynamic memory allocation adjustments.</li><li>At the other extreme, if a Slice is dynamically expanded to a large size and then returned to <code>sync.Pool</code>, it may lead memory leaks and waste.</li></ol><h1 id="bytebufferpool"><a href="#bytebufferpool" class="headerlink" title="bytebufferpool"></a>bytebufferpool</h1><p>To achieve better runtime performance, <a href="https://github.com/valyala/bytebufferpool">bytebufferpool</a> builds on <code>sync.Pool</code> with some simple statistical rules to minimize the impact of the two issues mentioned above during runtime. (The project’s author has other projects like fasthttp, quicktemplate, and VictoriaMetrics under his belt, all of which are excellent examples of performance optimization.</p><p>The main structure in the code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// bytebufferpool/pool.go</span></span><br><span class="line"><span class="keyword">const</span> (</span><br><span class="line">  minBitSize = <span class="number">6</span> <span class="comment">// 2**6=64 is a CPU cache line size</span></span><br><span class="line">  steps      = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">  minSize = <span class="number">1</span> &lt;&lt; minBitSize</span><br><span class="line">  maxSize = <span class="number">1</span> &lt;&lt; (minBitSize + steps - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  calibrateCallsThreshold = <span class="number">42000</span></span><br><span class="line">  maxPercentile           = <span class="number">0.95</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Pool <span class="keyword">struct</span> &#123;</span><br><span class="line">  calls       [steps]<span class="type">uint64</span></span><br><span class="line">  calibrating <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">  defaultSize <span class="type">uint64</span></span><br><span class="line">  maxSize     <span class="type">uint64</span></span><br><span class="line"></span><br><span class="line">  pool sync.Pool</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>defaultSize</code> is used for the initial size allocation for Slices, and <code>maxSize</code> is for rejecting Slices exceeding this size when returned to <code>sync.Pool</code>. The core algorithm dynamically adjusts <code>defaultSize</code> and <code>maxSize</code> based on statistical size information of Slices used at runtime, avoiding extra memory allocation while preventing memory leaks.</p><p>This dynamic statistical process is relatively simple, dividing the size of Slices returned to the Pool into 20 intervals for statistics. After <code>calibrating</code> number calls, it sorts by size, choosing the most frequently used interval size as <code>defaultSize</code>. This statistical method avoids many extra memory allocations. Then, by sorting by size and setting the 95% percentile size as <code>maxSize</code>, it prevents large objects from entering the Pool statistically. This dynamic adjustment of these two values achieves better performance at runtime.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><ul><li>Always specify the capacity when initializing Slices.</li><li>Avoid initializing Slices in loops.</li><li>Consider using <code>sync.Pool</code> for performance-sensitive paths.</li><li>The cost of memory allocation can far exceed that of business logic.</li><li>For reusing byte buffers, consider <code>bytebufferpool</code>.</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#basic-performance-tests&quot;&gt;Basic Performance Tests&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#appending-an-entire-slice&quot;&gt;Appending an Entire Sl</summary>
      
    
    
    
    
    <category term="performance" scheme="http://oilbeater.com/en/tags/performance/"/>
    
    <category term="golang" scheme="http://oilbeater.com/en/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>The Impact of Pre-allocating Slice Memory on Performance in Golang</title>
    <link href="http://oilbeater.com/en/2024/03/04/golang-slice-performance/"/>
    <id>http://oilbeater.com/en/2024/03/04/golang-slice-performance/</id>
    <published>2024-03-04T10:48:09.000Z</published>
    <updated>2024-08-26T03:18:57.219Z</updated>
    
    <content type="html"><![CDATA[<ul><li><a href="#theoretical-basis-of-slice-memory-allocation">Theoretical Basis of Slice Memory Allocation</a></li><li><a href="#quantitative-measurement">Quantitative Measurement</a></li><li><a href="#lint-tool-prealloc">Lint Tool prealloc</a></li><li><a href="#summary">Summary</a></li></ul><p>During my code reviews, I often focus on whether the slice initialization in the code has allocated the expected memory space, that is, I always request to change from <code>var init []int64</code> to <code>init := make([]int64, 0, length)</code> format whenever possible. However, I had no quantitative concept of how much this improvement affects performance, and it was more of a dogmatic requirement. This blog will introduce the theoretical basis of how pre-allocating memory improves performance, quantitative measurements, and tools for automated detection.</p><h1 id="Theoretical-Basis-of-Slice-Memory-Allocation"><a href="#Theoretical-Basis-of-Slice-Memory-Allocation" class="headerlink" title="Theoretical Basis of Slice Memory Allocation"></a>Theoretical Basis of Slice Memory Allocation</h1><p>The implementation for Golang Slice expansion can be found in <a href="https://github.com/golang/go/blob/go1.20.6/src/runtime/slice.go#L157">slice.go under growslice</a>. The general idea is that when the Slice capacity is less than 256, each expansion will create a new slice with double the capacity; when the capacity exceeds 256, each expansion will create a new slice with 1.25 times the original capacity. Afterwards, the old slice’s data is copied to the new slice, ultimately returning the new slice.</p><p>The expansion code is as follows:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">newcap := oldCap</span><br><span class="line">doublecap := newcap + newcap</span><br><span class="line"><span class="keyword">if</span> newLen &gt; doublecap &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">const</span> threshold = <span class="number">256</span></span><br><span class="line"><span class="keyword">if</span> oldCap &lt; threshold &#123;</span><br><span class="line">newcap = doublecap</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// Check 0 &lt; newcap to detect overflow</span></span><br><span class="line"><span class="comment">// and prevent an infinite loop.</span></span><br><span class="line"><span class="keyword">for</span> <span class="number">0</span> &lt; newcap &amp;&amp; newcap &lt; newLen &#123;</span><br><span class="line"><span class="comment">// Transition from growing 2x for small slices</span></span><br><span class="line"><span class="comment">// to growing 1.25x for large slices. This formula</span></span><br><span class="line"><span class="comment">// gives a smooth-ish transition between the two.</span></span><br><span class="line">newcap += (newcap + <span class="number">3</span>*threshold) / <span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// Set newcap to the requested cap when</span></span><br><span class="line"><span class="comment">// the newcap calculation overflowed.</span></span><br><span class="line"><span class="keyword">if</span> newcap &lt;= <span class="number">0</span> &#123;</span><br><span class="line">newcap = newLen</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Theoretically, if the slice’s capacity is pre-allocated, eliminating the need for dynamic expansion, we can see performance improvements in several areas:</p><ol><li>Memory needs to be allocated only once, avoiding repeated allocations.</li><li>Data copying is not required repeatedly.</li><li>There’s no need for repeated garbage collection of the old slice.</li><li>Memory is allocated accurately, avoiding the capacity waste caused by dynamic allocation.</li></ol><p>In theory, pre-allocating slice capacity should lead to performance improvements compared to dynamic allocation, but the exact amount of improvement requires quantitative measurement.</p><h1 id="Quantitative-Measurement"><a href="#Quantitative-Measurement" class="headerlink" title="Quantitative Measurement"></a>Quantitative Measurement</h1><p>We refer to the code from <a href="https://github.com/alexkohler/prealloc/blob/master/prealloc_test.go">prealloc</a> and make simple modifications to measure the impact of pre-allocating vs. dynamically allocating slices of different capacities on performance.</p><p>The test code is as follows, and by changing <code>length</code>, we can observe performance data under different scenarios:</p><figure class="highlight go"><figcaption><span>title</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> prealloc_test</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;testing&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> length = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkNoPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Don&#x27;t preallocate our initial slice</span></span><br><span class="line"><span class="keyword">var</span> init []<span class="type">int64</span></span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">BenchmarkPreallocate</span><span class="params">(b *testing.B)</span></span> &#123;</span><br><span class="line">b.ResetTimer()</span><br><span class="line"><span class="keyword">for</span> i := <span class="number">0</span>; i &lt; b.N; i++ &#123;</span><br><span class="line"><span class="comment">// Preallocate our initial slice</span></span><br><span class="line">init := <span class="built_in">make</span>([]<span class="type">int64</span>, <span class="number">0</span>, length)</span><br><span class="line"><span class="keyword">for</span> j := <span class="number">0</span>; j &lt; length; j++ &#123;</span><br><span class="line">init = <span class="built_in">append</span>(init, <span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The first function tests the performance of dynamic allocation, and the second tests the performance of pre-allocation. The tests can be executed with the following command:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go <span class="built_in">test</span> -bench=. -benchmem prealloc_test.go</span><br></pre></td></tr></table></figure><p>Results with <code>length = 1</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12       40228154                27.36 ns/op            8 B/op          1 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         55662463                19.97 ns/op           </span><br><span class="line"></span><br><span class="line"> 8 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>With <code>length = 1</code>, theoretically, both dynamic and static allocation should perform a single initial memory allocation, so there should be no difference in performance. However, pre-allocation takes 70% of the time compared to dynamic allocation, showing a 1.4x performance advantage even when the number of memory allocations remains the same. This performance improvement seems related to the continuity of variable allocation.</p><p>Results with <code>length = 10</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BenchmarkNoPreallocate-12        5402014               228.3 ns/op           248 B/op          5 allocs/op</span><br><span class="line">BenchmarkPreallocate-12         21908133                50.46 ns/op           80 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>With <code>length = 10</code>, pre-allocation still only performs one memory allocation, while dynamic allocation performs 5, making pre-allocation’s performance 4 times better. This indicates that even for smaller slice sizes, pre-allocation can significantly improve performance.</p><p>Results with <code>length</code> at 129, 1025, and 10000:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># length = 129</span></span><br><span class="line">BenchmarkNoPreallocate-12         743293              1393 ns/op            4088 B/op          9 allocs/op</span><br><span class="line">BenchmarkPreallocate-12          3124831               386.1 ns/op          1152 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 1025</span></span><br><span class="line">BenchmarkNoPreallocate-12         169700              6571 ns/op           25208 B/op         12 allocs/op</span><br><span class="line">BenchmarkPreallocate-12           468880              2495 ns/op            9472 B/op          1 allocs/op</span><br><span class="line"></span><br><span class="line"><span class="comment"># length = 10000</span></span><br><span class="line">BenchmarkNoPreallocate-12          14430             86427 ns/op          357625 B/op         19 allocs/op</span><br><span class="line">BenchmarkPreallocate-12            56220             20693 ns/op           81920 B/op          1 allocs/op</span><br></pre></td></tr></table></figure><p>At larger capacities, static allocation still only requires one memory allocation, but the performance improvement does not scale proportionally, typically 2 to 4 times better. This may be due to other overheads or special optimizations in Golang for large capacity copying, so the performance gap does not widen as much.</p><p>When changing the slice’s contents to a more complex struct, it was expected that copying would incur greater performance overhead, but in practice, the performance gap between pre-allocation and dynamic allocation for complex structs was even smaller. It appears there are many internal optimizations, and the behavior does not always align with intuition.</p><h1 id="Lint-Tool-prealloc"><a href="#Lint-Tool-prealloc" class="headerlink" title="Lint Tool prealloc"></a>Lint Tool prealloc</h1><p>Although pre-allocating memory can bring certain performance improvements, relying solely on manual review for this issue in larger projects is prone to oversights. This is where lint tools for automatic code scanning become necessary. <a href="https://github.com/alexkohler/prealloc">prealloc</a> is such a tool that can scan for potential slices that could be pre-allocated but were not, and it can be integrated into <a href="https://golangci-lint.run/usage/linters/#prealloc">golangci-lint</a>.</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>Overall, pre-allocating slice memory is a relatively simple yet effective optimization method. Even when slice capacities are small, pre-allocation can still significantly improve performance. Using static code scanning tools like prealloc, these potential optimizations can be easily detected and integrated into CI, simplifying future operations.</p><blockquote><p>Update: I previously wrote about <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance/">The Impact of Preallocating Slice Memory in Golang</a>, discussing the performance effects of preallocating memory in Slices. The scenarios considered were relatively simple, and recently, I conducted further tests to provide more information, including the impact of appending an entire Slice and the use of sync.Pool and bytebufferpool on performance here <a href="https://oilbeater.com/en/2024/03/04/golang-slice-performance-cont/">The Impact of Preallocating Slice Memory in Golang (Continued)</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#theoretical-basis-of-slice-memory-allocation&quot;&gt;Theoretical Basis of Slice Memory Allocation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#quantit</summary>
      
    
    
    
    
    <category term="performance" scheme="http://oilbeater.com/en/tags/performance/"/>
    
    <category term="golang" scheme="http://oilbeater.com/en/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>The Single-Node Kubernetes Showdown: minikube vs. kind vs. k3d</title>
    <link href="http://oilbeater.com/en/2024/02/22/minikube-vs-kind-vs-k3d/"/>
    <id>http://oilbeater.com/en/2024/02/22/minikube-vs-kind-vs-k3d/</id>
    <published>2024-02-22T00:00:00.000Z</published>
    <updated>2024-08-26T03:18:57.219Z</updated>
    
    <content type="html"><![CDATA[<p>As a developer in the cloud-native ecosystem, a common challenge is the need to frequently test applications within a Kubernetes environment. In CI, this often extends to quickly trialing configurations across various Kubernetes clusters, including single-node, high-availability, dual-stack, multi-cluster setups, and more. Thus, the ability to swiftly create and manage Kubernetes clusters on a local machine, without breaking the bank, has become a must-have. This post will dive into three popular single-node Kubernetes management tools: minikube, kind, and k3d.Highlighting their unique features, use cases, and potential pitfalls.</p><blockquote><p>TL;DR</p><p>If speed is your only concern, k3d is your best bet. If you’re after compatibility and a simulation close to reality, minikube is your safest bet. kind sits comfortably in the middle, offering a balance between the two.</p></blockquote><ul><li><a href="#technical-comparison">Technical Comparison</a><ul><li><a href="#minikube">minikube</a></li><li><a href="#kind">kind</a></li><li><a href="#k3d">k3d</a></li></ul></li><li><a href="#performance-showdown">Performance Showdown</a><ul><li><a href="#methodology">Methodology</a></li><li><a href="#results">Results</a></li></ul></li><li><a href="#conclusion">Conclusion</a></li></ul><h1 id="Technical-Comparison"><a href="#Technical-Comparison" class="headerlink" title="Technical Comparison"></a>Technical Comparison</h1><p>At their core, these three tools serve a similar function: managing Kubernetes on a single machine. However, their differing historical backgrounds and technical choices have led to unique nuances and use cases.</p><h2 id="minikube"><a href="#minikube" class="headerlink" title="minikube"></a>minikube</h2><p><a href="https://minikube.sigs.k8s.io/docs/">minikube</a> is the Kubernetes community’s OG tool for quickly setting up Kubernetes locally, a first love for many Kubernetes novices. Initially, it simulated multi-node clusters via VMs on your local machine, offering a high-fidelity emulation of real-world scenarios, down to the OS and kernel module level. The downside? It’s a resource hog, and if your virtualization environment doesn’t support nested virtualization, you’re out of luck, not to mention it’s slow to start. Recently, the community introduced a Docker Driver to mitigate these issues, though at the cost of losing some VM-level emulation capabilities. On the bright side, minikube comes with a plethora of add-ons, like dashboards and nginx-ingress, for easy community component installation.</p><h2 id="kind"><a href="#kind" class="headerlink" title="kind"></a>kind</h2><p><a href="https://kind.sigs.k8s.io/">kind</a> is a more recent favorite for local Kubernetes deployment, using Docker containers to simulate nodes and focusing purely on Kubernetes standard deployments, with community components requiring manual installation. It’s the go-to for Kubernetes’ own CI processes. The upside? Quick starts and a familiar environment for Docker veterans. The downside? Container simulation lacks OS-level isolation, sharing the host’s kernel, which can complicate OS-specific testing. I once had a kernel module test fail because the host’s netfilter tweaks caused havoc in a kind-managed cluster.</p><h2 id="k3d"><a href="#k3d" class="headerlink" title="k3d"></a>k3d</h2><p><a href="https://k3d.io/stable/">k3d</a>, a featherweight in local Kubernetes deployment, shares a similar approach to kind but opts for deploying a lightweight <a href="https://k3s.io/">k3s</a> instead of standard Kubernetes. This means it inherits k3s’s pros and cons, boasting incredibly fast setup times—don’t worry about correctness; just marvel at the speed. The trade-offs include a super-slimmed-down OS (sans glibc), complicating certain OS-level operations, and a unique installation approach that might puzzle those accustomed to kubeadm’s standard deployment features.</p><h1 id="Performance-Showdown"><a href="#Performance-Showdown" class="headerlink" title="Performance Showdown"></a>Performance Showdown</h1><p>While the minikube community provides some <a href="https://minikube.sigs.k8s.io/docs/benchmarks/timetok8s/v1.32.0/">performance benchmarks</a>, comparing the startup times of our three contenders, I was curious about other aspects like image size, memory footprint, and bare-minimum setup times, prompting another round of tests.</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>Testing was straightforward, given each tool’s one-liner setup, with a few caveats:</p><ol><li>minikube used the Docker Driver to keep the speed test fair.</li><li>All tests assumed pre-downloaded images to exclude network delays.</li><li>The latest versions were tested, though Kubernetes versions varied, making this more of a qualitative than quantitative analysis.</li><li>Tests focused on basic component starts without additional plugins, ensuring essentials like CNI, CoreDNS, and CSI were included.</li><li><code>docker image</code> and <code>docker stat</code> commands were used to measure image sizes and memory usage, respectively.</li></ol><p>Commands used:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#minikube</span></span><br><span class="line">time minikube start --driver=docker --force</span><br><span class="line"></span><br><span class="line"><span class="comment">#kind</span></span><br><span class="line">time kind create cluster</span><br><span class="line"></span><br><span class="line"><span class="comment">#k3d</span></span><br><span class="line">time k3d cluster create mycluster --k3s-arg <span class="string">&#x27;--disable=traefik,metrics-server</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@server:*&#x27;</span> --no-lb</span><br></pre></td></tr></table></figure><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><table><thead><tr><th>Name</th><th>Software Version</th><th>Kubernetes Version</th><th>Image Size</th><th>Start Time</th><th>Memory Usage</th></tr></thead><tbody><tr><td>minikube</td><td>v1.32.0</td><td>v1.28.3</td><td>1.2GB</td><td>29s</td><td>536MiB</td></tr><tr><td>kind</td><td>v0.22</td><td>v1.29.2</td><td>956MB</td><td>20s</td><td>463MiB</td></tr><tr><td>k3d</td><td>v5.6.0</td><td>v1.27.4</td><td>263MB</td><td>7s</td><td>423MiB</td></tr></tbody></table><p>Evidently, k3d takes the crown in startup performance, boasting significant advantages in image size, startup time, and memory usage. It’s a godsend for those running CI on a shoestring budget.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>If speed and resource efficiency are your top priorities, k3d is a no-brainer. For OS-level isolation tests, minikube’s VM Driver is unbeatable. For everything in between, kind offers a balanced compromise between compatibility and performance.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;As a developer in the cloud-native ecosystem, a common challenge is the need to frequently test applications within a Kubernetes environm</summary>
      
    
    
    
    
    <category term="kubernetes" scheme="http://oilbeater.com/en/tags/kubernetes/"/>
    
    <category term="ci" scheme="http://oilbeater.com/en/tags/ci/"/>
    
  </entry>
  
</feed>
